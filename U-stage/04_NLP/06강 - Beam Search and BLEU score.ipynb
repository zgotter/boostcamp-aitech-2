{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06강 - Beam Search and BLEU score.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOS5n0OG6AwJjCDY21/oi59"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VEm0dJb_yrZz"},"source":["# 6. Beam Search and BLEU score"]},{"cell_type":"markdown","metadata":{"id":"uLqMLjEjywX9"},"source":["## 강의 소개\n","\n","문장을 decoding 하는 데에 사용하는 대표적인 알고리즘인 **Beam Search**와 번역 task에서 번역된 문장을 평가하는 대표적인 metric인 **BLEU score**를 소개합니다.\n","\n","**Beam Search**\n","\n","- 언어 모델이 문장을 generation할 때에는 확률값에 기반한 다양한 경우의 수가 존재합니다.\n","- 모든 경우의 수를 고려하는 것은 비효율적이며 너무 작은 확률값까지 고려한다면 생성된 문장의 quality가 떨어질 수 있습니다.\n","- 가장 높은 확률값을 고려하는 방법 역시 모델이 단순한 generation을 하도록 하게 만드는 단점이 있을 수 있습니다.\n","- 이러한 문제의 대안으로 제안된 Beam Search를 알아봅니다.\n","\n","**BLEU score**\n","\n","- 자연어는 컴퓨터가 이해할 수 있는 방식으로 변환되어 모델의 입력 및 출력으로 활용되기 때문에 적절한 metric을 이용해 모델을 평가해야 합니다.\n","- 다양한 자연어처리 관련 metric이 있지만, 그중에서도 번역 task에서 가장 대표적인 BLEU score를 소개합니다.\n","- 번역에 있어서 BLEU score가 precision을 고려하는 이유에 대해서 고민하면서 강의를 들어주시면 좋을 것 같습니다."]},{"cell_type":"markdown","metadata":{"id":"B60agKv7zBQs"},"source":["<br>\n","\n","## Further Reading\n","\n","- [Deep learning.ai-BeamSearch](https://www.youtube.com/watch?v=RLWuzLLSIgw&feature=youtu.be)\n","- [Deep learning.ai-RefiningBeamSearch](https://www.youtube.com/watch?v=gb__z7LlN_4&feature=youtu.be)\n","- [OpenNMT-beam search](https://opennmt.net/OpenNMT/translation/beam_search/)"]},{"cell_type":"markdown","metadata":{"id":"3zDyitAUzeH2"},"source":["<br>\n","\n","## Further Question\n","\n","- BLEU score가 번역 문장 평가에 있어서 갖는 단점은 무엇이 있을까요?\n","  - 참고 : [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://arxiv.org/abs/2006.06264?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)"]},{"cell_type":"markdown","metadata":{"id":"tLHveQZ5zoRc"},"source":["<br>\n","\n","## 6.1 Beam Search"]},{"cell_type":"markdown","metadata":{"id":"e2nPNCvV6piX"},"source":["### References\n","\n","- [Machine Translation, Sequence-to-sequence and Attention](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)\n","- [[Sooftware 머신러닝] Beam Search (빔서치)](https://blog.naver.com/PostView.nhn?blogId=sooftware&logNo=221809101199&from=search&redirect=Log&widgetTypeCall=true&directAccess=false)"]},{"cell_type":"markdown","metadata":{"id":"4W1hImFd12RZ"},"source":["<br>\n","\n","### 6.1.1 Greedy decoding\n","\n","- 디코더에서 하나의 단어만을 입력으로 사용하여 다음 단어를 예측하는 것을 Greedy decoding 이라고 한다.\n","- Greedy decoding has no way to undo decisions!\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=11W2qMRev0hbM_e-Qeps9AR-Wcf5jUAAz' width=500/>\n","\n","- How can we fix this?"]},{"cell_type":"markdown","metadata":{"id":"44kA4iCK33zl"},"source":["<br>\n","\n","### 6.1.2 Exhaustive search (철저한 탐색)\n","\n","- Ideally, we want to find a (length $T$) translation $y$ that maximizes\n","- $x$: 입력 문장\n","- $y$: 출력 문장\n","\n","<br>\n","\n","\\begin{align*}\n","P(y \\mid x)&=P\\left(y_{1} \\mid x\\right) P\\left(y_{2} \\mid y_{1}, x\\right) P\\left(y_{3} \\mid y_{2}, y_{1}, x\\right) \\ldots P\\left(y_{T} \\mid y_{1}, \\ldots, y_{T-1}, x\\right)\\\\\n","&=\\prod_{1}^{T} P\\left(y_{t} \\mid y_{1}, \\ldots, y_{t-1}, x\\right)\n","\\end{align*}\n","\n","<br>\n","\n","- We could try computing <font color='red'>all possible sequences</font> $y$\n","  - This means that on each step $t$ of the decoder, we are tracking $V^t$ possible partial translations, where $V$ is the vocabulary size\n","  - This $O(V^t)$ complexity is far too expensive!\n","- Exhaustive search 의 차선책으로 등장한 것이 Beam Search 이다."]},{"cell_type":"markdown","metadata":{"id":"dWisSBLN4RWN"},"source":["<br>\n","\n","### 6.1.3 Beam search\n","\n","- Greedy decoding 과 Exhaustive search 의 중간쯤에 있는 아이디어"]},{"cell_type":"markdown","metadata":{"id":"begVIBzk6ENe"},"source":["<br>\n","\n","#### 6.1.3.1 Core idea\n","\n","- on each time step of the decoder, we keep track of the <font color='red'>$k$</font> most probable partial translations (which we call hypothese)\n","- 매 time step 마다 미리 정해놓은 $k$개의 단어를 고려하여 예측을 수행한다.\n","- $k$개의 candidate 중에서 가장 확률이 높은 것을 택한다.\n","  - $k$ is the beam size (in practice around 5 to 10)\n","\n","<br>\n","\n","- A hypothesis $y_1, \\dots, y_t$ has a score of its log probability:\n","\n","$$\n","\\operatorname{score}\\left(y_{1}, \\ldots, y_{t}\\right)=\\log P_{L M}\\left(y_{1}, \\ldots, y_{t} \\mid x\\right)=\\sum_{i=1}^{t} \\log P_{L M}\\left(y_{i} \\mid y_{1}, \\ldots, y_{i-1}, x\\right)\n","$$\n","\n","- 최대화하고자 하는 값은 joint probability $P_{L M}\\left(y_{1}, \\ldots, y_{t} \\mid x\\right)$ 이다.\n","- 이 값에 로그를 씌우게 되면 확률값들의 덧셈으로 변환할 수 있다.\n","- 여기서 사용된 로그함수는 단조증가함수이기 때문에 적용하기 전에 가장 큰 확률값도 로그 적용 후 큰 값으로 유지될 수 있다.\n","\n","<br>\n","\n","- Scores are all negative, and a higher score is better\n","- We search for high-scoring hypotheses, tracking the top $k$ ones on each step"]},{"cell_type":"markdown","metadata":{"id":"YN8ZQFyB597L"},"source":["<br>\n","\n","#### 6.1.3.2 Caution\n","\n","- Beam search is <font color='red'>not guaranteed</font> to find a globally optimal solution. \n","- But it is <font color='red'>much more efficient</font> than exhaustive search!\n"]},{"cell_type":"markdown","metadata":{"id":"0tlCX0996deO"},"source":["<br>\n","\n","#### 6.1.3.3 Example\n","\n","- Beam size: $k=2$"]},{"cell_type":"markdown","metadata":{"id":"TK56eMUq-Nzp"},"source":["<br>\n","\n","- Calculate prob dist of next word\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=11mba4LZjqoThQP6VjlvLhePdhoxqGqCt' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"VqwuJW2D6kSe"},"source":["<br>\n","\n","- Take top k words and compute scores\n","- 예측값은 vocabulary 상에 정의된 단어의 확률분포로 output이 나타나게 된다.\n","- 가장 확률값이 높은 2개의 단어를 선택하게 된다.\n","- 확률값들은 0~1 사이의 값들을 갖게 되는데 로그함수의 x절편이 1이기 때문에 로그를 취한 확률값은 모두 음수로 나오게 된다.\n","- 확률값이 커질수록 로그를 씌운 확률값은 0에 가깝게 나타난다.\n","  - 즉, 결과값이 음수 중에 0에 가까운 값일수록 큰 확률을 갖는다고 볼 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=11otZ_NmjTHpZmwa6aPZuqlcxnCYQaLSV' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"UpDvMq1N9owl"},"source":["<br>\n","\n","- For each of the $k$ hypotheses, find top $k$ next words and calculate scores\n","- 일시적으로 $k^k$개(=4)의 hypothesis를 고려해서 각각의 자식 노드의 예측값을 생성한다.\n","- 해당 예측값과 부모노드의 예측값의 합이 가장 큰값을 갖는 beam size(=2) 만큼의 hypothesis를 선택한다.\n","\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=11wR43z_KvBkGjNWWmImfrUsRfCfd7g6W' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"UfbLhvdP9pol"},"source":["<br>\n","\n","- For each of the $k$ hypotheses, find top $k$ next words and calculate scores\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=121PeRYzdnvRvmdpddZbP4piMjRFZRjLc' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"JkcJ3Bva9qY1"},"source":["<br>\n","\n","- For each of the $k$ hypotheses, find top $k$ next words and calculate scores\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=126bgbAdY9p4c_b_b99wovkJHgQkO60S4' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"jesrksYY9rAs"},"source":["<br>\n","\n","- For each of the $k$ hypotheses, find top $k$ next words and calculate scores\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12EP4BSxVhE3BEcOzFvvpWT5HiMVUOGNe' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"BP7osZ689rfM"},"source":["<br>\n","\n","- For each of the $k$ hypotheses, find top $k$ next words and calculate scores\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12RkSWhURqPR7zoNLg7oVr-ezj7fn2XGz' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"UuK-ob0Q9sZk"},"source":["<br>\n","\n","#### 6.1.3.4 Stopping criterion\n","\n","- In <font color='skyblue'>greedy decoding</font>, usually we decode until the model produces a <font color='red'>\\<END\\> token</font>\n","  - For example: `<START> he hit me with a pie <END>`\n","\n","<br>\n","\n","- In <font color='skyblue'>beam search decoding</font>, different hypotheses may produce \\<END\\> tokens on <font color='red'>different timesteps</font>\n","- 각각의 hypothesis들은 서로 다른 time step에서 `<END>` 토큰을 만날 수가 있다.\n","  - When a hypothesis produces \\<END\\>, that hypothesis is <font color='red'>complete</font>\n","  - `<END>` 토큰을 만난 hypothesis에 대해서는 생성과정을 중단한다.\n","  - <font color='red'>Place ist aside</font> and continue exploring other hypotheses via beam search\n","  - 완료된 hypothesis는 임시 공간에 저장한 뒤 나머지 hypothesis들의 생성과정을 진행한다.\n","\n","<br>\n","\n","- Usually we continue beam search until:\n","  - We reach timestep $T$ (where $T$ is some pre-defined cutoff), or\n","    - 특정 time step에 도달했을 때 종료\n","  - We have at least $n$ completed hypotheses (where $n$ is the pre-defined cutoff)\n","    - 저장공간에 특정 갯수 이상의 hypothesis가 채워졌을 때 종료"]},{"cell_type":"markdown","metadata":{"id":"7NT2ZgTA_tNv"},"source":["<br>\n","\n","#### 6.1.3.5 Finishing up\n","\n","- We have our list of completed hypotheses\n","- How to select the top one with the highest score?\n","- Each hypothesis $y_1, \\dots, y_t$ on our list has a score\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12c0fPOs57TW4YjTMyssgwmFK9YA_ey38' width=600/>\n","\n","- Problem with this: <font color='red'>longer hypotheses have lower scores</font>\n","  - hypothesis들의 시퀀스의 길이가 서로 다를 수가 있다.\n","  - 상대적으로 짧은 길이를 갖는 시퀀스가 높은 joint probability 값을 갖게될 것이다.\n","  - 긴 길이의 시퀀스를 갖는 hypothesis는 낮은 joint probability 값을 갖게 된다.\n","- Fix: Normalize by length\n","  - joint probability 의 값을 시퀀스의 길이로 나눠줌으로서 normalize 할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12br5XIulIhHxxgVg-niNxMKKW2-CZOrc' width=450/>"]},{"cell_type":"markdown","metadata":{"id":"IszWqMAoBNtN"},"source":["<br>\n","\n","## 6.2 BLEU score"]},{"cell_type":"markdown","metadata":{"id":"ZfOgCMFiBbhQ"},"source":["### References\n","\n","- [딥러닝을 이용한 자연어 처리 입문 - BLEU Score](https://wikidocs.net/31695)"]},{"cell_type":"markdown","metadata":{"id":"0k5JWBOvBzb-"},"source":["<br>\n","\n","### 6.2.1 Precision and Recall"]},{"cell_type":"markdown","metadata":{"id":"m9VJSdMCAyh5"},"source":["#### 6.2.1.1 Example 1\n","\n","- Reference: <font color='green'>Half</font> of <font color='green'>my heart is in</font> Havana <font color='green'>ooh na</font> na\n","- Predicted: <font color='green'>Half</font> as <font color='green'>my heart is in</font> Obama <font color='green'>ooh na</font>"]},{"cell_type":"markdown","metadata":{"id":"d62euI3pYkI9"},"source":["- precision: 정밀도\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12dJ46YqlxM0DLPf2AjBU_BlTGPOxAUD0' width=500/>\n","\n","- 양성으로 예측한 것들 중 양성으로 예측을 성공한 비율\n","- 예측된 결과가 도출이 되었을 때 실질적으로 느끼는 정확도\n","  - `length_of_prediction` : predicted 의 단어 수\n","  - `#(correct words)` : predicted 기준 reference 와 겹치는 단어 수\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k0JHO7GOYnWj"},"source":["- recall: 재현율\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12gNGpdqvUHmfIoRzDcDCo30GZGJ201Hu' width=500/>\n","\n","- 실제 양성인 것들 중 양성으로 예측을 성공한 비율\n","- 실제값에 대한 정확도\n","  - `length_of_reference` : reference 의 단어 수\n","  - `#(correct words)` : predicted 기준 reference 와 겹치는 단어 수\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aqIr8hHLYqM0"},"source":["- F - measure\n","- precision 과 recall 의 조화 평균\n","  - 산술평균 > 기하평균 > 조화평균\n","  - 조화평균은 작은 값에 더 큰 가중치를 부여한 형태로 볼 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12h0pjN8OXakGcK8EhTiLijQDLuVMelLS' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"UUQ3gtbLCTbC"},"source":["<br>\n","\n","#### 6.2.1.2 Example 2\n","\n","- Predicted (from model 1): <font color='green'>Half</font> as <font color='green'>my heart is in</font> Obama <font color='green'>ooh na</font>\n","- Reference: $\\qquad\\qquad\\quad\\;\\;$Half of my heart is in Havana ooh na na\n","- Predicted (from model 2): <font color='green'>Havana na in heart my is Half ooh of na</font>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12icRh9VDJUyeJJVkH_N7xd9od2GUEGpD' width=600/>\n","\n","- <font color='red'>Flaw(결함): no penalty for reordering</font>\n","  - precision과 recall을 통한 모델 평가는 문장의 순서에 따른 패널티를 부과할 수 없다.\n"]},{"cell_type":"markdown","metadata":{"id":"AHTjbXG6Da7X"},"source":["<br>\n","\n","### 6.2.2 BiLingual Evaluation Understudy (BLEU) score\n","\n","- <font color='red'>N-gram overlap</font> between machine translation output and reference sentence\n","  - 개별 단어 레벨에서 봤을 때 얼마나 ground truth 문장과 겹치는 단어들이 나왔는가를 평가할 뿐만 아니라 **n-gram을 통해 연속된 단어 집합에 대한 평가**도 함께 실시한다."]},{"cell_type":"markdown","metadata":{"id":"O6aHwX4Ig3WK"},"source":["- Compute preecision for <font color='red'>n-grams of size one to four(1~4)</font>\n","  - BLEU는 precision 만을 고려한다. (recall은 무시)\n","  - 이는 번역 결과만을 보고 정확도를 판단한다고 볼 수 있다."]},{"cell_type":"markdown","metadata":{"id":"hkgumz1Lg4_9"},"source":["- Add brevity(간결성) penalty (for too short translations)\n","  - reference의 길이 보다 짧은 문장을 생성한 경우에 짧아진 비율만큼 precision의 기하평균값을 낮춰준다는 의미이다.\n","  - brevity penalty는 recall의 최대값을 의미한다.\n","  - Typically computed over the entire corpus, not on single sentences"]},{"cell_type":"markdown","metadata":{"id":"JQ9yXWD6g6me"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=130fc13BO53Z9OtSjoqxbpOpE_k3yCSyi' width=600/>\n","\n","- precision의 기하평균\n","  - 1-gram 부터 4-gram 까지의 precision을 구한 후 이들의 기하평균을 계산한다.\n"]},{"cell_type":"markdown","metadata":{"id":"Tjeb0N4-ECPB"},"source":["<br>\n","\n","- Predicted (from model 1): Half as <font color='green'>my heart is in</font> Obama ooh na\n","- Reference: $\\qquad\\qquad\\quad\\;\\;$Half of my heart is in Havana ooh na na\n","- Predicted (from model 2): Havana na in heart my is Half ooh of na\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=12oSkfoKqopuCk_1zuIn7Cs-cjHki4gG9' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"WUAgCK3BEW9V"},"source":["<br>\n","\n","## 6.3 실습: Seq2seq + Attention\n","\n","1. 여러 Attention 모듈을 구현한다.\n","2. 기존 Seq2seq 모델과의 차이를 이해한다."]},{"cell_type":"markdown","metadata":{"id":"I3HcXpAOmby1"},"source":["<br>\n","\n","### 6.3.1 필요 패키지 import"]},{"cell_type":"code","metadata":{"id":"BfNk2ZNamfwM","executionInfo":{"status":"ok","timestamp":1631191722020,"user_tz":-540,"elapsed":2690,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["from tqdm import tqdm\n","from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.nn import functional as F\n","\n","import torch\n","import random"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hh03WDMmho_"},"source":["<br>\n","\n","### 6.3.2 데이터 전처리\n","\n","- 데이터 전처리는 이전과 동일하다."]},{"cell_type":"code","metadata":{"id":"udVQKkLkmm48","executionInfo":{"status":"ok","timestamp":1631191735637,"user_tz":-540,"elapsed":1128,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["vocab_size = 100\n","pad_id = 0\n","sos_id = 1\n","eos_id = 2\n","\n","src_data = [\n","  [3, 77, 56, 26, 3, 55, 12, 36, 31],\n","  [58, 20, 65, 46, 26, 10, 76, 44],\n","  [58, 17, 8],\n","  [59],\n","  [29, 3, 52, 74, 73, 51, 39, 75, 19],\n","  [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93],\n","  [39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99, 5],\n","  [75, 34, 17, 3, 86, 88],\n","  [63, 39, 5, 35, 67, 56, 68, 89, 55, 66],\n","  [12, 40, 69, 39, 49]\n","]\n","\n","trg_data = [\n","  [75, 13, 22, 77, 89, 21, 13, 86, 95],\n","  [79, 14, 91, 41, 32, 79, 88, 34, 8, 68, 32, 77, 58, 7, 9, 87],\n","  [85, 8, 50, 30],\n","  [47, 30],\n","  [8, 85, 87, 77, 47, 21, 23, 98, 83, 4, 47, 97, 40, 43, 70, 8, 65, 71, 69, 88],\n","  [32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18],\n","  [37, 14, 49, 24, 93, 37, 54, 51, 39, 84],\n","  [16, 98, 68, 57, 55, 46, 66, 85, 18],\n","  [20, 70, 14, 6, 58, 90, 30, 17, 91, 18, 90],\n","  [37, 93, 98, 13, 45, 28, 89, 72, 70]\n","]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XViHYIAumo-F","executionInfo":{"status":"ok","timestamp":1631191767857,"user_tz":-540,"elapsed":340,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"21935d10-b6cb-485a-c905-b147fe62a7e6"},"source":["trg_data = [[sos_id] + seq + [eos_id] for seq in tqdm(trg_data)]"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 13374.69it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"zTTk1gyAmxfl","executionInfo":{"status":"ok","timestamp":1631191881981,"user_tz":-540,"elapsed":468,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["def padding(data, is_src=True):\n","    max_len = len(max(data, key=len))\n","    print(f\"Maximum sequence length: {max_len}\")\n","\n","    valid_lens = []\n","    for i, seq in enumerate(tqdm(data)):\n","        valid_lens.append(len(seq))\n","        if len(seq) < max_len:\n","            data[i] = seq + [pad_id] * (max_len - len(seq))\n","\n","    return data, valid_lens, max_len"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpC3DMmDnNVQ","executionInfo":{"status":"ok","timestamp":1631191890771,"user_tz":-540,"elapsed":678,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"fe353113-725c-4de6-c002-af496e8da1e2"},"source":["src_data, src_lens, src_max_len = padding(src_data)\n","trg_data, trg_lens, trg_max_len = padding(trg_data)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 65027.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 22\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 21720.89it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R09aA5genPaN","executionInfo":{"status":"ok","timestamp":1631191951599,"user_tz":-540,"elapsed":326,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"62258bfe-fa87-443a-9323-83ee048c5470"},"source":["# B: batch size, S_L: source maximum sequence length, T_L: target maximum sequence length\n","src_batch = torch.LongTensor(src_data) # (B, S_L)\n","src_batch_lens = torch.LongTensor(src_lens) # (B,)\n","trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\n","trg_batch_lens = torch.LongTensor(trg_lens)  # (B)\n","\n","print(src_batch.shape)\n","print(src_batch_lens.shape)\n","print(trg_batch.shape)\n","print(trg_batch_lens.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 15])\n","torch.Size([10])\n","torch.Size([10, 22])\n","torch.Size([10])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HICXi7lneXH","executionInfo":{"status":"ok","timestamp":1631192649607,"user_tz":-540,"elapsed":661,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"95058b35-307b-420f-e8c0-c34148a60882"},"source":["src_batch_lens, sorted_idx = src_batch_lens.sort(descending=True)\n","src_batch = src_batch[sorted_idx]\n","trg_batch = trg_batch[sorted_idx]\n","trg_batch_lens = trg_batch_lens[sorted_idx]\n","\n","print(src_batch)\n","print(src_batch_lens)\n","print(trg_batch)\n","print(trg_batch_lens)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99,  5],\n","        [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93,  0,  0,  0,  0],\n","        [63, 39,  5, 35, 67, 56, 68, 89, 55, 66,  0,  0,  0,  0,  0],\n","        [ 3, 77, 56, 26,  3, 55, 12, 36, 31,  0,  0,  0,  0,  0,  0],\n","        [29,  3, 52, 74, 73, 51, 39, 75, 19,  0,  0,  0,  0,  0,  0],\n","        [58, 20, 65, 46, 26, 10, 76, 44,  0,  0,  0,  0,  0,  0,  0],\n","        [75, 34, 17,  3, 86, 88,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [12, 40, 69, 39, 49,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [58, 17,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [59,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n","tensor([15, 11, 10,  9,  9,  8,  6,  5,  3,  1])\n","tensor([[ 1, 37, 14, 49, 24, 93, 37, 54, 51, 39, 84,  2,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1, 32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18,  2,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1, 20, 70, 14,  6, 58, 90, 30, 17, 91, 18, 90,  2,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1, 75, 13, 22, 77, 89, 21, 13, 86, 95,  2,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1,  8, 85, 87, 77, 47, 21, 23, 98, 83,  4, 47, 97, 40, 43, 70,  8, 65,\n","         71, 69, 88,  2],\n","        [ 1, 79, 14, 91, 41, 32, 79, 88, 34,  8, 68, 32, 77, 58,  7,  9, 87,  2,\n","          0,  0,  0,  0],\n","        [ 1, 16, 98, 68, 57, 55, 46, 66, 85, 18,  2,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1, 37, 93, 98, 13, 45, 28, 89, 72, 70,  2,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1, 85,  8, 50, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0],\n","        [ 1, 47, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0]])\n","tensor([12, 14, 13, 11, 22, 18, 11, 11,  6,  4])\n"]}]},{"cell_type":"markdown","metadata":{"id":"uzbBqTvFnygG"},"source":["<br>\n","\n","### 6.3.3 Encoder 구현\n","\n","- Encoder 역시 기존 Seq2seq 모델과 동일하다."]},{"cell_type":"code","metadata":{"id":"CHgFSdyQqTSl","executionInfo":{"status":"ok","timestamp":1631192702401,"user_tz":-540,"elapsed":718,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["embedding_size = 256\n","hidden_size = 512\n","num_layers = 2\n","num_dirs = 2\n","dropout = 0.1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"FILkYm_3qVit","executionInfo":{"status":"ok","timestamp":1631193123843,"user_tz":-540,"elapsed":336,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","        self.gru = nn.GRU(\n","            input_size=embedding_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            bidirectional=True if num_dirs > 1 else False,\n","            dropout=dropout\n","        )\n","        self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)\n","\n","    def forward(self, batch, batch_lens): # batch: (B, S_L), batch_lens: (B)\n","        # d_w: word embedding size\n","        batch_emb = self.embedding(batch)  # (B, S_L, d_w)\n","        batch_emb = batch_emb.transpose(0, 1)  # (S_L, B, d_w)\n","\n","        packed_input = pack_padded_sequence(batch_emb, batch_lens)\n","\n","        h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers*num_dirs, B, d_h) = (4, B, d_h)\n","        packed_outputs, h_n = self.gru(packed_input, h_0)  # h_n: (4, B, d_h)\n","        outputs = pad_packed_sequence(packed_outputs)[0]  # outputs: (S_L, B, 2d_h)\n","        outputs = torch.tanh(self.linear(outputs))  # (S_L, B, d_h)\n","\n","        forward_hidden = h_n[-2, :, :]\n","        backward_hidden = h_n[-1, :, :]\n","        hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0)  # (1, B, d_h)\n","\n","        return outputs, hidden"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Gls5wC5r8ft","executionInfo":{"status":"ok","timestamp":1631193133233,"user_tz":-540,"elapsed":351,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["encoder = Encoder()"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lt4oXi8pr-l8"},"source":["<br>\n","\n","### 6.3.4 Dot-product Attention 구현\n","\n","- 우선 대표적인 attention 형태 중 하나인 Dot-product Attention은 다음과 같이 구현할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"WsjW_BHhsJ5s","executionInfo":{"status":"ok","timestamp":1631193423097,"user_tz":-540,"elapsed":434,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["class DotAttention(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, decoder_hidden, encoder_outputs): # (1, B, d_h), (S_L, B, d_h)\n","        query = decoder_hidden.squeeze(0) # (B, d_h)\n","        key = encoder_outputs.transpose(0, 1) # (B, S_L, d_h)\n","\n","        energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1) # (B, S_L)\n","\n","        attn_scores = F.softmax(energy, dim=-1) # (B, S_L)\n","        attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0, 1), attn_scores.unsqueeze(2)), dim=1) # (B, d_h)\n","\n","        return attn_values, attn_scores"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ti5WW6oTtFkW","executionInfo":{"status":"ok","timestamp":1631193431125,"user_tz":-540,"elapsed":434,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["dot_attn = DotAttention()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-wgphgMtHgN"},"source":["<br>\n","\n","### 6.3.5 Decoder 구현 (Dot-product attention)\n","\n","- Dot-product attention 모듈을 가지는 Decoder 클래스를 구현한다."]},{"cell_type":"code","metadata":{"id":"AGGT769ctQO0","executionInfo":{"status":"ok","timestamp":1631197435514,"user_tz":-540,"elapsed":403,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self, attention):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","        self.attention = attention\n","        self.rnn = nn.GRU(\n","            embedding_size,\n","            hidden_size\n","        )\n","        self.output_linear = nn.Linear(2*hidden_size, vocab_size)\n","\n","    def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (L, B, d_h), hidden: (1, B, d_h)\n","        batch_emb = self.embedding(batch) # (B, d_w)\n","        batch_emb = batch_emb.unsqueeze(0) # (1, B, d_w)\n","\n","        outputs, hidden = self.rnn(batch_emb, hidden) # (1, B, d_h), (1, B, d_h)\n","\n","        attn_values, attn_scores = self.attention(hidden, encoder_outputs) # (B, d_h), (B, S_L)\n","        concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim=-1) # (1, B, 2d_h)\n","\n","        return self.output_linear(concat_outputs).squeeze(0), hidden # (B, V), (1, B, d_h)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"aS1WdX998ZJf","executionInfo":{"status":"ok","timestamp":1631197445611,"user_tz":-540,"elapsed":318,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["decoder = Decoder(dot_attn)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eBCZE5ab8bq2"},"source":["<br>\n","\n","### 6.3.6 Seq2seq 모델 구축\n","\n","- 최종적으로 seq2seq 모델을 다음과 같이 구성할 수 있다."]},{"cell_type":"code","metadata":{"id":"KtPLPQqF8i0d","executionInfo":{"status":"ok","timestamp":1631198012249,"user_tz":-540,"elapsed":513,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["class Seq2seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2seq, self).__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):\n","        # src_batch: (B, S_L)\n","        # src_batch_lens: (B)\n","        # trg_batch: (B, T_L)\n","\n","        encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens) # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\n","\n","        input_ids = trg_batch[:, 0] # (B)\n","        batch_size = src_batch.shape[0]\n","        outputs = torch.zeros(trg_max_len, batch_size, vocab_size) # (T_L, B, V)\n","\n","        for t in range(1, trg_max_len):\n","            decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden) # decoder_outputs: (B, V), hidden: (1, B, d_h)\n","\n","            outputs[t] = decoder_outputs\n","            _, top_ids = torch.max(decoder_outputs, dim=-1) # top_ids: (B)\n","\n","            input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids\n","\n","        return outputs"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxzZnuem-l9f","executionInfo":{"status":"ok","timestamp":1631198027304,"user_tz":-540,"elapsed":468,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["seq2seq = Seq2seq(encoder, decoder)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P8eUtP_l-poV"},"source":["<br>\n","\n","### 6.3.7 모델 사용해보기\n","\n","- 만든 모델로 결과 확인"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVczoq1j-2FP","executionInfo":{"status":"ok","timestamp":1631198133937,"user_tz":-540,"elapsed":320,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"78928690-56f2-4b8d-b274-b6bf6716a64d"},"source":["# V: vocab size\n","outputs = seq2seq(src_batch, src_batch_lens, trg_batch) # (T_L, B, V)\n","\n","print(outputs)\n","print(outputs.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         ...,\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00]],\n","\n","        [[-3.9002e-02, -6.0949e-02, -7.3044e-02,  ...,  1.2073e-01,\n","           1.9356e-02, -1.9067e-01],\n","         [-8.3504e-03, -8.9391e-02, -4.3635e-02,  ...,  1.7211e-01,\n","          -4.9343e-02, -1.8412e-01],\n","         [-1.5035e-02, -9.5775e-02, -6.9908e-02,  ...,  1.3096e-01,\n","          -2.9351e-03, -2.0875e-01],\n","         ...,\n","         [ 8.5860e-03, -8.8916e-02, -9.2692e-02,  ...,  1.4487e-01,\n","          -2.9824e-02, -1.8835e-01],\n","         [ 1.1904e-02, -6.2445e-02, -5.8169e-02,  ...,  1.0531e-01,\n","          -4.1200e-02, -1.5574e-01],\n","         [-5.6609e-03, -8.7103e-02, -8.9904e-02,  ...,  1.5076e-01,\n","          -3.6772e-02, -1.7562e-01]],\n","\n","        [[-1.4608e-01,  5.5328e-02,  3.9061e-02,  ...,  8.7099e-02,\n","          -3.9520e-02, -1.3873e-02],\n","         [-7.0133e-03,  6.0537e-02,  7.6074e-02,  ...,  4.7827e-02,\n","          -1.0944e-01, -2.6554e-01],\n","         [-9.2134e-02,  1.4021e-02, -2.1673e-02,  ..., -3.8568e-02,\n","          -1.6077e-02, -1.3603e-01],\n","         ...,\n","         [-1.1206e-01,  4.6277e-02,  2.8276e-02,  ...,  9.5861e-02,\n","          -7.6356e-02, -1.7282e-02],\n","         [-5.1696e-02, -5.1135e-02, -1.4116e-01,  ..., -5.8153e-02,\n","          -3.1383e-02, -3.0971e-02],\n","         [-3.4445e-02, -4.6216e-03,  1.1285e-01,  ...,  9.2915e-02,\n","          -1.0355e-01, -1.0672e-01]],\n","\n","        ...,\n","\n","        [[-1.0541e-01,  6.5386e-02,  5.5142e-02,  ...,  3.3263e-02,\n","           1.5101e-01, -1.1967e-01],\n","         [-6.3065e-02,  3.6282e-02,  6.6312e-02,  ...,  5.7078e-02,\n","           8.9830e-02, -1.2146e-01],\n","         [-8.2594e-02,  2.0271e-02,  5.5662e-02,  ...,  1.4472e-02,\n","           1.4245e-01, -1.2585e-01],\n","         ...,\n","         [-1.4196e-01, -5.7138e-02, -8.4065e-02,  ..., -3.4564e-02,\n","          -6.2951e-02,  1.2744e-01],\n","         [-2.7081e-02, -1.4207e-01,  1.7225e-01,  ..., -1.1506e-02,\n","          -4.0147e-02,  1.6826e-01],\n","         [-1.4501e-01, -4.6512e-02, -8.6249e-02,  ..., -2.7972e-02,\n","          -6.0713e-02,  1.2539e-01]],\n","\n","        [[-1.3391e-01, -4.6785e-02,  1.0242e-02,  ...,  5.2258e-02,\n","           5.1252e-02,  5.7399e-02],\n","         [-1.0245e-01, -7.7201e-02,  2.2408e-02,  ...,  7.6771e-02,\n","           5.0049e-05,  5.8190e-02],\n","         [-1.2067e-01, -8.0320e-02,  1.6312e-02,  ...,  3.7887e-02,\n","           3.9165e-02,  4.7683e-02],\n","         ...,\n","         [-1.4496e-01, -1.1523e-01, -5.8808e-02,  ..., -2.0143e-03,\n","          -6.2414e-02,  1.6791e-01],\n","         [-5.8931e-02, -1.2558e-01,  6.8652e-02,  ...,  4.2273e-04,\n","          -5.2560e-02,  2.2861e-01],\n","         [-1.4694e-01, -1.0686e-01, -5.9547e-02,  ...,  3.7804e-03,\n","          -6.0897e-02,  1.6615e-01]],\n","\n","        [[-1.5112e-01, -9.9824e-02, -1.7016e-02,  ...,  5.5352e-02,\n","           5.4632e-03,  1.3259e-01],\n","         [-1.2672e-01, -1.2918e-01, -2.6880e-03,  ...,  7.8439e-02,\n","          -3.6830e-02,  1.3338e-01],\n","         [-1.4361e-01, -1.2418e-01, -7.8781e-03,  ...,  4.3076e-02,\n","          -5.6960e-03,  1.2102e-01],\n","         ...,\n","         [-1.5137e-01, -1.3349e-01, -5.4453e-02,  ...,  1.6397e-02,\n","          -6.2077e-02,  1.8096e-01],\n","         [-9.3387e-02, -1.3108e-01,  7.2560e-03,  ...,  1.7283e-02,\n","          -5.5337e-02,  2.3170e-01],\n","         [-1.5272e-01, -1.2655e-01, -5.4097e-02,  ...,  2.1684e-02,\n","          -6.1187e-02,  1.7904e-01]]], grad_fn=<CopySlices>)\n","torch.Size([22, 10, 100])\n"]}]},{"cell_type":"code","metadata":{"id":"MKUcSsjl_Dgn","executionInfo":{"status":"ok","timestamp":1631198263591,"user_tz":-540,"elapsed":321,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["sample_sent = [4, 10, 88, 46, 72, 34, 14, 51]\n","sample_len = len(sample_sent)\n","\n","sample_batch = torch.LongTensor(sample_sent).unsqueeze(0) # (1, L)\n","sample_batch_len = torch.LongTensor([sample_len]) # (1)\n","\n","encoder_output, hidden = seq2seq.encoder(sample_batch, sample_batch_len) # hidden: (4, 1, d_h) <- (?) (1, 1, d_h)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMOv_HX8AExO","executionInfo":{"status":"ok","timestamp":1631198405490,"user_tz":-540,"elapsed":324,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"ceb4ce16-9428-455b-d68e-da1933a1e32a"},"source":["hidden.shape"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 512])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"quaC39f__jWq","executionInfo":{"status":"ok","timestamp":1631198485333,"user_tz":-540,"elapsed":819,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["input_id = torch.LongTensor([sos_id]) # (1)\n","output = []\n","\n","for t in range(1, trg_max_len):\n","    decoder_output, hidden = seq2seq.decoder(input_id, encoder_output, hidden)  # decoder_output: (1, V), hidden: (4, 1, d_h) <- (?) (1, 1, d_h)\n","\n","    _, top_id = torch.max(decoder_output, dim=-1)  # top_ids: (1)\n","\n","    if top_id == eos_id:\n","        break\n","    else:\n","        output += top_id.tolist()\n","        input_id = top_id    "],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRTcEZEdAZWh","executionInfo":{"status":"ok","timestamp":1631198509461,"user_tz":-540,"elapsed":349,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"a928b41e-b196-4e4d-de00-b239a44f1fcb"},"source":["print(output)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[80, 7, 38, 57, 27, 84, 84, 38, 63, 55, 38, 63, 48, 57, 42, 82, 18, 18, 60, 70, 18]\n"]}]},{"cell_type":"markdown","metadata":{"id":"GH0jK6a6AaWV"},"source":["<br>\n","\n","### 6.3.8 Concat Attention 구현 (Bahdanau Attention)\n","\n","- Bahdanau Attention이라고도 불리는 Concat Attention을 구현해보도록 하겠습니다.  \n","  - `self.w`: Concat한 query와 key 벡터를 1차적으로 linear transformation.\n","  - `self.v`: Attention logit 값을 계산."]},{"cell_type":"code","metadata":{"id":"0cc_LX8OEQ9x","executionInfo":{"status":"ok","timestamp":1631199658007,"user_tz":-540,"elapsed":4,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["class ConcatAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.w = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","    self.v = nn.Linear(hidden_size, 1, bias=False)\n","\n","  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\n","    src_max_len = encoder_outputs.shape[0]\n","\n","    decoder_hidden = decoder_hidden.transpose(0, 1).repeat(1, src_max_len, 1)  # (B, S_L, d_h)\n","    encoder_outputs = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\n","\n","    concat_hiddens = torch.cat((decoder_hidden, encoder_outputs), dim=2)  # (B, S_L, 2d_h)\n","    energy = torch.tanh(self.w(concat_hiddens))  # (B, S_L, d_h)\n","\n","    attn_scores = F.softmax(self.v(energy), dim=1)  # (B, S_L, 1)\n","    attn_values = torch.sum(torch.mul(encoder_outputs, attn_scores), dim=1)  # (B, d_h)\n","\n","    return attn_values, attn_scores"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoVuyB8TE3uQ","executionInfo":{"status":"ok","timestamp":1631199665575,"user_tz":-540,"elapsed":324,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["concat_attn = ConcatAttention()"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zERCRwk1E5qF"},"source":["<br>\n","\n","### 6.3.9 Decoder 구현 (Concat attention)"]},{"cell_type":"code","metadata":{"id":"ziZOrNwwE-ox","executionInfo":{"status":"ok","timestamp":1631199763478,"user_tz":-540,"elapsed":736,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["class Decoder(nn.Module):\n","  def __init__(self, attention):\n","    super().__init__()\n","\n","    self.embedding = nn.Embedding(vocab_size, embedding_size)\n","    self.attention = attention\n","    self.rnn = nn.GRU(\n","        embedding_size + hidden_size, # (?)\n","        hidden_size\n","    )\n","    self.output_linear = nn.Linear(hidden_size, vocab_size)\n","\n","  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)  \n","    batch_emb = self.embedding(batch)  # (B, d_w)\n","    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\n","\n","    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\n","\n","    concat_emb = torch.cat((batch_emb, attn_values.unsqueeze(0)), dim=-1)  # (1, B, d_w+d_h)\n","\n","    outputs, hidden = self.rnn(concat_emb, hidden)  # (1, B, d_h), (1, B, d_h)\n","\n","    return self.output_linear(outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjNxwCY_FRSq","executionInfo":{"status":"ok","timestamp":1631199770875,"user_tz":-540,"elapsed":435,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["decoder = Decoder(concat_attn)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIeC7CXPFTK-"},"source":["<br>\n","\n","### 6.3.10 Seq2seq 모델 사용 (concat attention)"]},{"cell_type":"code","metadata":{"id":"pIkwW61mFbQl","executionInfo":{"status":"ok","timestamp":1631199812940,"user_tz":-540,"elapsed":4,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}}},"source":["seq2seq = Seq2seq(encoder, decoder)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2d3BNOJLFdRd","executionInfo":{"status":"ok","timestamp":1631199818549,"user_tz":-540,"elapsed":595,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"44ae4f7f-8eaa-49e5-c32b-d4780b46adb0"},"source":["outputs = seq2seq(src_batch, src_batch_lens, trg_batch)\n","\n","print(outputs)\n","print(outputs.shape)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0633, -0.0522,  0.0354,  ..., -0.0638,  0.0769,  0.1471],\n","         [ 0.0322, -0.0379,  0.0097,  ..., -0.0071,  0.0730,  0.2010],\n","         [ 0.0110, -0.0533, -0.0111,  ..., -0.0246,  0.0761,  0.1379],\n","         ...,\n","         [ 0.0380, -0.0317,  0.0043,  ..., -0.0419,  0.0822,  0.1966],\n","         [ 0.0526, -0.0004, -0.0139,  ..., -0.0436,  0.0530,  0.1265],\n","         [ 0.0495, -0.0256,  0.0128,  ..., -0.0171,  0.0848,  0.1625]],\n","\n","        [[ 0.1842, -0.0340, -0.0502,  ...,  0.0617,  0.1970,  0.0072],\n","         [-0.0629, -0.0461, -0.2107,  ..., -0.0321,  0.0203,  0.0108],\n","         [-0.1264, -0.0411,  0.0316,  ..., -0.0728,  0.1783,  0.1074],\n","         ...,\n","         [ 0.1776, -0.0273, -0.0619,  ...,  0.0819,  0.2025,  0.0339],\n","         [ 0.0102,  0.1283,  0.0322,  ..., -0.0139,  0.0288, -0.0400],\n","         [-0.0917,  0.2026,  0.0283,  ..., -0.1421, -0.0129,  0.0136]],\n","\n","        ...,\n","\n","        [[-0.0727,  0.2002,  0.1263,  ..., -0.0363, -0.0167, -0.0327],\n","         [ 0.0444,  0.1739, -0.1997,  ..., -0.1686, -0.0960,  0.1136],\n","         [-0.1265,  0.0586,  0.2719,  ...,  0.3171, -0.1429,  0.0567],\n","         ...,\n","         [-0.0968,  0.1661,  0.2810,  ...,  0.1695, -0.1863, -0.0698],\n","         [-0.1527,  0.1534,  0.2644,  ...,  0.2998, -0.2690,  0.0054],\n","         [-0.1541,  0.1513,  0.2623,  ...,  0.2934, -0.2679,  0.0075]],\n","\n","        [[-0.1403,  0.1869,  0.2705,  ...,  0.1536, -0.1654, -0.0752],\n","         [ 0.0096, -0.0779, -0.1122,  ..., -0.1808, -0.0832,  0.1185],\n","         [-0.1771,  0.1314,  0.2767,  ...,  0.3503, -0.2769, -0.0236],\n","         ...,\n","         [-0.1537,  0.1811,  0.2875,  ...,  0.2551, -0.3046, -0.1055],\n","         [-0.1531,  0.1233,  0.0292,  ..., -0.0035, -0.1654,  0.1087],\n","         [-0.1546,  0.1220,  0.0261,  ..., -0.0085, -0.1635,  0.1118]],\n","\n","        [[-0.1890,  0.1996,  0.2806,  ...,  0.2391, -0.2818, -0.1046],\n","         [-0.0147,  0.0954, -0.2765,  ..., -0.0013,  0.0244,  0.2170],\n","         [-0.1662,  0.1164,  0.0354,  ...,  0.0228, -0.1828,  0.0913],\n","         ...,\n","         [-0.1783,  0.3164, -0.0120,  ...,  0.0606, -0.2260, -0.1165],\n","         [-0.1656,  0.3547,  0.0343,  ..., -0.0937, -0.1597,  0.1980],\n","         [-0.1673,  0.3538,  0.0315,  ..., -0.0990, -0.1582,  0.2010]]],\n","       grad_fn=<CopySlices>)\n","torch.Size([22, 10, 100])\n"]}]},{"cell_type":"code","metadata":{"id":"vlxye8eZFe6v"},"source":[""],"execution_count":null,"outputs":[]}]}