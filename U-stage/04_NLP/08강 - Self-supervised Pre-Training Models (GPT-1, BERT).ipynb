{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08강 - Self-supervised Pre-Training Models (GPT-1, BERT).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNgTK8+mPXlsAgHjG0bjD5P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LzGEXnlb2hLP"},"source":["# 8. Self-supervised Pre-Training Models\n","\n","- GPT-1\n","- BERT"]},{"cell_type":"markdown","metadata":{"id":"rNZJqQ4H2ygF"},"source":["<br>\n","\n","## 강의 소개\n","\n","- 자연어 처리 분야에 한 획을 그은 **GPT-1**과 **BERT**를 소개합니다.\n","- **GPT-1**과 **BERT**는 **Transfer Learning, Self-supervised Learning, Transformer**를 사용했다는 공통점이 있습니다. 세가지의 강력한 무기를 이용해 대용량의 text를 학습한 모델을 target task에 적용해 거의 모든 기존 자연어처리 task를 압도하는 성능을 보여주었습니다. 세 가지의 키워드를 통해 두 모델을 자세히 알아봅니다"]},{"cell_type":"markdown","metadata":{"id":"YOOeo9743BJM"},"source":["<br>\n","\n","## Further Reading\n","\n","- [GPT-1](https://openai.com/blog/language-unsupervised/)\n","- [BERT : Pre-training of deep bidirectional transformers for language understanding, NAACL’19](https://arxiv.org/abs/1810.04805)\n","- [SQuAD: Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)\n","- [SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference](https://leaderboard.allenai.org/swag/submissions/public)"]},{"cell_type":"markdown","metadata":{"id":"UvPMNCs23QG9"},"source":["<br>\n","\n","## Further Question\n","\n","- BERT의 Masked Language Model의 단점은 무엇이 있을까요? 사람이 실제로 언어를 배우는 방식과의 차이를 생각해보며 떠올려봅시다\n","  - 참고: [XLNet: Generalized Auto-regressive Pre-training for Language Understanding](https://arxiv.org/abs/1906.08237)"]},{"cell_type":"markdown","metadata":{"id":"uGZMgjobAGKU"},"source":["<br>\n","\n","## References\n","\n","- GPT-1\n","  - [https://blog.openai.com/language-unsupervised/](https://blog.openai.com/language-unsupervised/)\n","- BERT : Pre-training of deep bidirectional transformers for language understanding,\n","NAACL’19\n","  - [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n","- SQuAD: Stanford Question Answering Dataset\n","  - [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n","- SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference\n","  - [https://leaderboard.allenai.org/swag/submissions/public](https://leaderboard.allenai.org/swag/submissions/public)\n"]},{"cell_type":"markdown","metadata":{"id":"QtrdXcmL3hXs"},"source":["<br>\n","\n","## 8.1 Recent Trends\n","\n","- Transformer model and its self-attention block has become a general-purpose sequence (or set) encoder and decoder in recent NLP applications as well as in other areas. \n","  - Transformer 및 self-attention block은 범용적인 시퀀스 인코더 및 디코더로서 최근 자연어처리의 다양한 task에서 좋은 성능을 내고 있고 자연어처리 이외의 분야에서도 활발하게 사용되고 있다.\n","- Training deeply stacked Transformer models via a self-supervised learning framework has significantly advanced various NLP tasks through transfer learning, e.g., BERT, GPT-3, XLNet, ALBERT, RoBERTa, Reformer, T5, ELECTRA…\n","  - Transformer는 모델 구조에 대한 변경 없이 self-attention block을 점점 더 많이 쌓아서(12개, 24개, 등) 깊은 모델을 만들고 이를 대규모 학습 데이터를 통해서 학습할 수 있는 self-supervised learning framework으로 학습한 후 이를 다양한 task들에 transfer learning 형태로 fine-tuning 하는 것을 통해 좋은 성능을 내고 있다.\n","- Other applications are fast adopting the self-attention and Transformer architecture as well as self-supervised learning approach, e.g., recommender systems, drug discovery, computer vision, …\n","  - self-attention 모델은 추천 시스템, 신약 개발, 영상 처리 분야에도 확장해나가고 있다.\n","- As for natural language generation, self-attention models still requires a greedy decoding of words one at a time. \n","  - 그러나 self-attention에 기반한 모델들도 자연어 생성(natural language generation)이라는 task에서 `<SOS>` 부터 `<EOS>`까지 단어들을 하나씩 생성하는 greedy decoding 이라는 framework에서 벗어나지는 못하고 있는 한계점이 있다."]},{"cell_type":"markdown","metadata":{"id":"pP9z0r8M30V0"},"source":["<br>\n","\n","## 8.2 GPT-1: Improving Language Understanding by Generative Pre-training\n","\n","- pre-training model의 시초\n","- OpenAI에서 나온 모델\n","- 최근의 GPT-2, GPT-3까지 이어지는 모델을 통해 자연어 생성 task에서의 놀라운 결과를 보여주고 있다."]},{"cell_type":"markdown","metadata":{"id":"e046FbJ94G7s"},"source":["<br>\n","\n","### 8.2.1 Introduction\n","\n","- GPT-1 introduces special tokens, such as `<S>` / `<E>` / `$`, to achieve effective transfer learning during fine-tuning\n","- It does not need to use additional task-specific architectures on top of transferred\n","\n","<br>\n","\n","- GPT-1의 특징은 기본적으로 **여러가지의 special token들을 제안**하여 간단한 task 뿐만 아니라 **자연어처리 분야의 다양한 task들을 동시에 처리**할 수 있는 통합된 모델을 제안했다는 것이다."]},{"cell_type":"markdown","metadata":{"id":"KmWUxVEAjdns"},"source":["<br>\n","\n","### 8.2.2 Model Structure\n","\n","- GPT-1은 주어진 텍스트 시퀀스에 Position Embedding을 더하고 self-attention block을 총 12개를 쌓은 구조를 갖고 있다.\n","- Text Prediction task\n","  - 알고 있는 첫 단어부터 다음에 등장할 단어를 순차적으로 예측하는 Language Model task를 수행하는 것\n","  - 입력과 출력 시퀀스가 별도로 있는 것이 아닌 영어로 되어 있는 수많은 웹페이지를 다운받아서 그 데이터를 가지고 추출된 각 문장 자기자신을 예측하게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16bhuzpvkhHOP0FsYLwroiqLPJaaWFCZY' width=300/>"]},{"cell_type":"markdown","metadata":{"id":"R0X-VxnVjfo2"},"source":["<br>\n","\n","### 8.2.3 task별 학습 방식\n","\n","- GPT-1은 간단한 Language Model task뿐만 아니라 문장 레벨 또는 다수의 문장이 존재하는 경우에도 모델의 큰 변형없이 활용될 수 있도록 학습의 프레임워크를 제시한다.\n","- `Extract` 토큰은 처음엔 시퀀스의 마지막을 나타내는 토큰으로서 시퀀스에 추가된다.\n","- 하지만 특정 task를 수행하는 데 필요한 정보가 Self-Attention을 통한 인코딩 과정 중에 `Extract` 토큰에 포함되어 task를 수행하는데 사용된다."]},{"cell_type":"markdown","metadata":{"id":"hzUsO_DtHuz7"},"source":["<br>\n","\n","#### 8.2.3.1 Classification\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16hoFCDt-t-f2X-P6MJhz3CFXeJ5RMBLP' width=800/>\n","\n","- 문장 level의 감정 분류 task를 수행하기 위해 주어진 한 문장을 넣고 앞부분에 `Start` 토큰을 넣고 뒷부분에 EOS 토큰에 해당하는 `Extract` 를 넣는다.\n","- 이렇게 만들어진 시퀀스를 GPT-1 모델(Transformer)을 통해 인코딩한 후 최종적으로 나온 `Extract` 토큰에 해당하는 인코딩 벡터를 최종 output layer의 입력 벡터로 줌으로서 해당 문장이 긍정인지 부정인지를 분류할 수 있는 task로 학습을 하게 된다."]},{"cell_type":"markdown","metadata":{"id":"KsReAMMIHwRt"},"source":["\n","<br>\n","\n","#### 8.2.3.2 Entailment(함의)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16hxQUVMv-FDJtjnbWHq0EOrSPD_0HNoK' width=800/>\n","\n","- `Premise` 문장이 참이면 반드시 `Hypothesis` 문장도 참인 논리적인 내포 관계를 갖고 있다고 하자.\n","  - `Premise`: \"어제 John이 결혼을 했다.\"\n","  - `Hypothesis`: \"어제 최소한 1명은 결혼을 했다.\"\n","- 이러한 task에서는 다수의 문장으로 이루어진 입력을 받아서 예측을 수행해야 되고 이를 위해 모델에서는 2개의 문장을 단순하게 하나의 시퀀스로 만들되 앞부분에 `Start` 토큰을 넣고 문장 사이에는 특수 토큰에 해당하는 `Delim` 토큰을 넣는다.\n","- 시퀀스 마지막에는 `Extract` 토큰을 추가한다.\n","- 해당 시퀀스를 입력으로 하여 모델을 거쳐 얻어진 단어별 인코딩 벡터들 중 `Extract` 토큰에 해당하는 인코딩 벡터를 최종적인 output layer를 통과시켜 두 문장이 논리적으로 내포관계인지 아니면 논리적으로 모순관계인지를 분류할 수 있도록 학습한다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4B3JPVkIJdFP"},"source":["<br>\n","\n","#### 8.2.3.3 Similarity\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16lCO410WEhES6z8CULAh14DFybD1xyY0' width=800/>\n","\n","- 두 문장 간의 유사도를 측정하는 task에는 위 그림과 같은 형태로 학습된다."]},{"cell_type":"markdown","metadata":{"id":"jpLuMJTYJc6m"},"source":["<br>\n","\n","#### 8.2.3.4 Multiple Choice\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16nHEd2IIXTOtlpkPouNM8AbpQtMz01SZ' width=800/>\n","\n","- 여러 문장 중 하나의 문장을 선택하는 task에서는 위 그림과 같은 형태로 학습된다."]},{"cell_type":"markdown","metadata":{"id":"k2tvTNTsMCZt"},"source":["<br>\n","\n","### 8.2.4 Transfer Learning (Pre-training & Fine-tuning)\n","\n","- GPT-1 모델은 또 다른 task에 대해 transfer learning 형태로 활용할 때에는 다음과 같은 방식으로 동작하게 된다.\n","\n","<br>\n","\n","- 먼저 긍부정 분류 task를 수행하기 위해 학습된 모델이 있다고 하자.\n","  - 기존 task: 긍부정 분류\n","- 이번에는 이 모델을 통해서 주제 분류 task를 수행하고자 하는데 이 task는 downstream task로서 긍부정 분류 task와는 다르다.\n","  - 새로운 task: 주제 분류\n","- 그렇기 때문에 먼저 긍부정 예측으로서 갖고 있던 output layer와 다음 단어 예측을 위한 output layer를 제거한다.\n","- output layer 전 단계인 Transformer에서 나오는 단어별 인코딩 벡터를 사용해서 새로운 task를 위한 추가적인 output layer를 붙이고 해당 layer는 randomization을 한다.\n","- 새롭게 구축한 모델에 대해 새로운 task인 주제 분류와 관련된 학습 데이터를 통해 전체 네트워크를 학습하는 과정을 거친다.\n","- 새롭게 추가한 output layer는 randomization 되었기 때문에 충분히 학습되어야 한다.\n","- 이미 학습된 Transformer Encoder에는 learning rate을 상대적으로 작게 줌으로서 큰 변화가 일어나지 않도록 하여 기존의 task를 통해 학습했던 여러 지식들을 충분히 담아내어 그 정보를 새롭게 원하는 task에 활용할 수 있도록 하는 Pre-Training 및 Fine-Tuning 과정이 일어나게 된다."]},{"cell_type":"markdown","metadata":{"id":"ePsGQJtzQFha"},"source":["<br>\n","\n","### 8.2.5 Self-supervised Learning\n","\n","- pre-train된 다양한 task로부터 유용한 지식을 얻을 수 있는 요소는 어디서 오는가에 대해 살펴보자.\n","- pre-train 단계에서 사용된 Language Modeling은 별도의 label을 필요로 하지 않는 데이터이기 때문에 많은 양의 데이터를 사용하여 모델을 학습시킬 수 있다.\n","- 하지만 문서 분류와 같은 task는 학습 데이터에 label이 존재해야 하기 때문에 이러한 데이터는 적을 수 밖에 없다.\n","- 그렇기 때문에 pre-training 단계에서 별도의 label이 필요하지 않은 대규모의 데이터를 바탕으로 self-supervised learning을 수행한다.\n","- 이를 통해 얻을 수 있는 지식을 소량의 데이터만 있는 target task에 전이학습(transfer learning)의 형태로 성능을 올릴 수 있다."]},{"cell_type":"markdown","metadata":{"id":"upknE_HZjn21"},"source":["<br>\n","\n","### 8.2.6 Parameters and Activation Unit\n","\n","- 12-layer decoder-only transformer\n","- 12 head / 768 dimensional states\n","- GELU activation unit\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1I3AZFVV7JqR-KoqfSl8SkGvlDCWQd1Ow' width=500/>\n","\n","- 출처: https://blog.openai.com/language-unsupervised/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OM24_LTt5R7c"},"source":["<br>\n","\n","### 8.2.7 Experimental Results\n","\n","- pre-train된 GPT-1 모델을 다양한 task에 fine-tuning 해봤을 때 기존의 각 task를 위해 customized된 모델의 정확도보다 일반적으로 더 높은 성능을 보이는 것을 확인할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_WTHFBYvmeokyOMvt49rQFGTapQmARcO' width=600/>\n","\n","- 출처: https://blog.openai.com/language-unsupervised/"]},{"cell_type":"markdown","metadata":{"id":"2A42uasb5gd7"},"source":["<br>\n","\n","## 8.3 BERT"]},{"cell_type":"markdown","metadata":{"id":"CkVWjGzg5sEb"},"source":["### 8.3.1 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n","\n","- BERT는 현재까지도 가장 널리 사용되는 pre-training 모델이다.\n","- 기본적으로 BERT도 GPT-1과 마찬가지로 문장에 있는 일부 단어를 맞추는 Language Modeling이라는 task를 통해 pre-training을 수행한 모델이다.\n","\n","<br>\n","\n","- 다음 단어를 예측하는 Language Model을 활용한 self-supervised learning을 Transformer 이전에 LSTM 기반의 인코더를 통해 pre-training 하는 ELMo 라는 접근법도 존재했다.\n","- LSTM 대신 Tranformer를 사용한 모델들이 다음에 등장할 단어를 예측하는 pre-train task에서도 더 많은 양의 지식을 배울 수 있는 형태로 고도화되었다.\n","\n","<br>\n","\n","- Learn through masked language modeling task\n","- Use large-scale data and large-scale model\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mWdPdlT3xxyv55H59fmoEVmrtFH6sLYH' width=900/>\n","\n","- 출처: BERT: Pre-training of deep bidirectional transformers for language understanding, NAACL’19"]},{"cell_type":"markdown","metadata":{"id":"jOwRtNyp6APj"},"source":["<br>\n","\n","### 8.3.2 Masked Language Model\n","\n","- Motivation\n","  - Language models <font color='red'>only use left context or right context</font>, but language understanding is bi-directional\n","  - GPT-1의 경우 앞에 등장하는 단어를 통해 다음에 나올 단어를 예측하는 과정을 통해 pre-training 모델을 학습했다.\n","  - 하지만 이럴 경우 시퀀스의 전후 문맥을 보지 못하고 **앞쪽의 문맥만을 고려하여 다음에 나올 단어를 예측해야 한다는 한계가 존재**했다.\n","- If we use bi-directional language model?\n","  - Problem: Words can \"see themselves\" (cheating) ini a bi-directional encoder"]},{"cell_type":"markdown","metadata":{"id":"wKacJB2x6be7"},"source":["<br>\n","\n","### 8.3.3 Pre-training Tasks in BERT\n","\n","- 위와 같은 Motivation을 바탕으로 나온 BERT의 Pre-training 기법에는  Masked Language Model(MLM)과 Next Sentence Prediction(NSP)가 있다.\n","\n","<br>\n","\n","**Masked Language Model (MLM)**\n","\n","\n","- Mask some precentage of the input tokens at random, and then predict those masked tokens.\n","- \"I study math\" 라는 문장이 주어졌을 때 각각의 3단어에 대해 일정한 확률로 각 단어를 `[MASK]`라는 단어로 치환하고 `[MASK]`에 해당하는 단어가 무엇인지를 맞추는 방식으로 모델의 학습이 진행된다.\n","\n","<br>\n","\n","**Next Sentence Prediction (NSP)**\n","\n","- NSP는 GPT-1에도 존재했던 pre-training task이다.\n","- 문장 레벨의 task에 대응하기 위해 pre-training 기법이다.\n","- Predict whether <font color='skyblue'>Sentence B</font> is an actual sentence that proceeds <font color='red'>Sentence A</font>, or a random sentence\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1LG3BVRxSka9vK1FHFKq00qQeohD66ESs' width=600/>\n","\n","- 출처: https://nlp.stanford.edu/seminar/details/jdevlin.pdf"]},{"cell_type":"markdown","metadata":{"id":"yx9kiakg80LK"},"source":["<br>\n","\n","#### 8.3.3.1 Masked Language Model (MLM)"]},{"cell_type":"markdown","metadata":{"id":"u_9gpcdCc5Ag"},"source":["\n","**MLM Training Method**\n","\n","- 얼만큼의 단어를 `[MASK]`라는 단어로 치환할 지에 대한 비율을 정하는 것이 모델 학습 전에 결정해야 할 하이퍼파라미터에 해당된다.\n","- 논문에서 제시한 적절한 비율은 15% 이다.\n","- Mask out $k\\%$ of the input words, and then predict the masked words\n","  - e.g., use $k=15\\%$\n","  - 논문에서는 `[MASK]`로 치환하는 단어의 비율이 15%가 적절하다고 제안했다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=159OVBL3OnnWTwlpUA1YHBy6L5sHkMbPI' width=600/>\n","\n","- 출처: https://nlp.stanford.edu/seminar/details/jdevlin.pdf\n","\n","<br>\n","\n","- Too little masking\n","  - Too expensive to train\n","  - `[MASK]`로 치환하는 비율을 너무 작게하는 경우 가려지지 않은 단어들을 읽어들여서 인코딩하는 과정이 너무 많은 계산을 필요로 하게 된다.\n","  - 그렇게 되면 학습의 효율이 떨어지거나 학습 속도가 느려지게 된다.\n","- Too much masking\n","  - Not enough to capture context\n","  - `[MASK]`로 치환하는 비율을 너무 크게 하면 문장에서 너무 많은 부분을 가리기 때문에 `[MASK]`에 해당하는 단어를 예측하기엔 충분한 정보가 제공되지 않는 문제가 발생할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"6G4puGcw9dQT"},"source":["<br>\n","\n","**Problem**\n","\n","- Mask token never seen during fine-tuning\n","- 그렇지만 15%의 단어를 가렸을 때이 단어들을 모두 예측하도록 하는 것은 여러 가지 부작용이 생길 수 있다.\n","\n","<br>\n","\n","- pre-train 단계에서는 15%의 단어가 `[MASK]`로 치환되어 있는 상태에 익숙해진 모델이 될것이다.\n","- 이 모델을 다른 task에 대해 tranfer learning 하게 되면 `[MASK]`라는 토큰은 더 이상 등장하지 않는다.\n","- 이러한 경우 pre-training 당시 주어진 입력의 패턴이 target task를 수행할 때의 입력 문장과는 다른 특성을 보일 수 있고 이러한 차이점이 학습을 방해하거나 transfer learning의 효과를 최대화하는 데 방해가 될 수 있다."]},{"cell_type":"markdown","metadata":{"id":"Z549S1rfc7op"},"source":["<br>\n","\n","**Solution**\n","\n","- 15% of the words to predict, but don't replace with `[MASK]` 100% of the time.\n","- 위와 같은 부작용을 해결하기 위해 15%의 단어들 중에서 아래 비율과 같이 나눠서 서로 다른 형태로 변경한다.\n","\n","<br>\n","\n","- 80% of the time, replace with `[MASK]`\n","- 15%의 단어 중 80%의 단어를 `[MASK]`로 치환한다.\n","  - `went to the store` -> `went to the [MASK]`\n","\n","<br>\n","\n","- 10% of the time, replace with a random word\n","- 15%의 단어 중 10%의 단어를 임의의 단어로 치환한다.\n","- 이는 문제의 난이도를 높여주는 것으로 생각할 수 있다.\n","  - `went to the store` -> `went to the running`\n","\n","<br>\n","\n","- 10% of the time, keep the same sentence\n","- 15%의 단어 중 10%의 단어는 변경하지 않고 그대로 사용한다.\n","  - `went to the store` -> `went to the store`"]},{"cell_type":"markdown","metadata":{"id":"9AhHXTzj9mMi"},"source":["<br>\n","\n","#### 8.3.3.2 Next Sentence Prediction (NSP)\n","\n","- To learn the relationships among sentences, predict whether Sentence B is an actual sentence that proceeds Sentence A, or a random sentence\n","- 주어진 하나의 글에서 2개의 문장을 추출한다.\n","- 두 문장을 하나의 시퀀스로 이어주고, 두 문장 사이에는 `[SEP]` 토큰을 추가하고 시퀀스의 마지막에도 `[SEP]` 토큰을 추가한다.\n","- 그리고 다수의 문장 레벨에서의 예측 task를 수행하는 역할을 담당하는 토큰으로서 `[CLS]` 토큰을 시퀀스의 맨 앞에 추가한다.\n","  - 이는 GPT-1에서 사용된 `[Extract]` 토큰에 해당한다.\n","  - GPT-1과 달리 해당 토큰을 문장의 맨 앞에 넣어주는 차이점이 있다.\n","- 별도의 문장 레벨에서의 label이 필요한 task가 아닌 여전히 입력 데이터만으로 예측을 수행할 수 있는 task를 학습시키기 위해 연속적으로 주어진 2개의 문장이 순차적으로 등장해야 하는 문장인지 아닌지에 대한 binary classification을 수행하는 task를 추가한다.\n","  - Next Sentence 인지 아닌지를 분류하는 task 수행\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1J7xP08x0iJWh0VOYeAFLJ6wj2Ttf_ouc' width=800/>\n","\n","- 출처: https://nlp.stanford.edu/seminar/details/jdevlin.pdf"]},{"cell_type":"markdown","metadata":{"id":"fwjkRaGWhVry"},"source":["<br>\n","\n","#### 8.3.3.3 MLM & NSP\n","\n","- NSP를 위해 `[CLS]`, `[SEP]` 토큰을 추가하고 각 문장의 단어들 중 일부를 `[MASK]`로 치환하여 전체 시퀀스를 Tranformer를 통해 인코딩한다.\n","- `[MASK]` 자리에 해당하는 토큰에서는 해당하는 인코딩 벡터를 통해 해당 자리에 있어야 하는 단어를 예측하도록 학습한다.\n","- 또한 동시에 `[CLS]` 토큰은 여기에 해당하는 인코딩 벡터를 통해 output layer를 하나 둬서 binary classfication을 수행하도록 한다.\n","- binary classification의 ground truth는 두 문장이 정말로 인접한 문장인지 아닌지에 따라 `IsNext`와 `NotNext`로 labeling된다."]},{"cell_type":"markdown","metadata":{"id":"82WprGNzJDFg"},"source":["<br>\n","\n","### 8.3.4 BERT Summary"]},{"cell_type":"markdown","metadata":{"id":"dH0WZjFiijjY"},"source":["#### 8.3.4.1 Model Architecture\n","\n","- BERT 모델 구조 자체는 Transformer에서 제안된 Self Attention 블록을 그대로 사용한다.\n","- 이러한 구조의 모델을 아래와 같이 두 가지 버전으로 학습된 모델을 제안했다.\n","\n","<br>\n","\n","- BERT BASE: L = 12, H = 768, A = 12\n","- BERT LARGE: L = 24, H = 1024, A = 16\n","  - L: Self Attention 블록 갯수\n","  - H: 각 Self Attention 블록에서 사용하는 인코딩 벡터의 차원 수\n","  - A: 각 layer별로 정의되는 Attention Head의 갯수"]},{"cell_type":"markdown","metadata":{"id":"ZAxpWOtzimaA"},"source":["<br>\n","\n","#### 8.3.4.2 Input Representation"]},{"cell_type":"markdown","metadata":{"id":"B9Z02y4jUwmc"},"source":["**Wordpiece embeddings (30,000 WordPiece)**\n","\n","- BERT에서는 입력 시퀀스를 넣어줄 때 단어별 임베딩 벡터를 사용하는 것이 아닌 단어를 subword 단위로 좀 더 잘게 쪼갠 단어들을 입력 벡터로 넣어준다.\n","- `bert tokenizer` \n","- 이것을 바로 Wordpiece embedding이라고 한다"]},{"cell_type":"markdown","metadata":{"id":"egP09wU5Umml"},"source":["**Learned positional embedding**\n","\n","- Transformer에서 제안된 사전에 정의된 서로 다른 주기를 갖는 사인, 코사인 함수를 사용하여 위치 정보를 embedding 벡터를 사용하는 것이 아닌 이 embedding 벡터 또한 전체적인 모델 학습 과정을 통해 최적화된 값을 도출하여 사용한다.\n","- 즉, Positional embedding 자체도 학습에 의해 결정되도록 두었다."]},{"cell_type":"markdown","metadata":{"id":"nF23TzJHUov2"},"source":["**`[CLS]` - Classification embedding**\n","\n","- NSP에서 문장의 연속 여부를 나타내는 Classification 토큰"]},{"cell_type":"markdown","metadata":{"id":"V-4wCGB0Up_G"},"source":["**`[SEP]` - Packed sentence embedding**\n","\n","- NSP에서 2개의 문장을 연결하고 마지막을 나타내는 토큰"]},{"cell_type":"markdown","metadata":{"id":"evR0fyoKUr9u"},"source":["**Segment Embedding**\n","\n","- BERT를 학습할 때 일반적으로 단어별 `[MASK]`로 치환된 단어를 예측하는 task와 2개의 문장으로 이뤄진 하나의 시퀀스가 인접 문장인지 아닌지를 예측하는 문장 레벨의 task가 있다.\n","- 하나의 시퀀스에 들어 있는 **각 문장을 구분하기 위해 사용되는 것**이 바로 Segment Embeddingn 이다."]},{"cell_type":"markdown","metadata":{"id":"BS5vsl8BmQhw"},"source":["<br>\n","\n","- The input embedding is the sum of the token embeddings, the segmentation embeddings and the position embeddings\n","  - BERT의 input embedding 벡터는 Token Embedding, Segment Embedding, Position Embedding 벡터들의 합으로 구해진다.\n","- Token Embedding, Segment Embedding, Position Embedding 는 동일한 차원의 크기를 갖는 벡터가 사용된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Fq6-MUxWalL23OAMsjl5By8q8wfGkAgB' width=800/>\n","\n","- 출처: BERT: Pre-training of deep bidirectional transformers for language understanding, NAACL’19"]},{"cell_type":"markdown","metadata":{"id":"ee3MUGQDiqmf"},"source":["<br>\n","\n","#### 8.3.4.3 Pre-training Tasks\n","\n","- Masked LM (MLM)\n","- Next Sentence Prediction (NSP)"]},{"cell_type":"markdown","metadata":{"id":"NMXBzkVnK3wI"},"source":["<br>\n","\n","### 8.3.5 BERT와 GPT의 Attention 모듈의 차이점\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mWdPdlT3xxyv55H59fmoEVmrtFH6sLYH' width=900/>\n","\n","- 출처: BERT: Pre-training of deep bidirectional transformers for language understanding, NAACL’19\n","\n","<br>\n","\n","- GPT-2의 경우 주어진 시퀀스를 인코딩할 때 다음으로 등장할 단어를 예측하는 task를 수행해야 하기 때문에 특정한 time step에서 이후에 등장할 단어에 대한 접근을 허용해서는 안된다.\n","  - 이는 Transformer의 디코더에서의 Masked Self Attention이 사용된 것과 같은 이유라고 볼 수 있다.\n","- 그러므로 GPT-2에서는 특정한 time step에서는 항상 자기자신을 포함해서 왼쪽에 있는 단어에 대한 정보만을 접근하여 다음 단어를 예측하게 된다.\n","- 그렇기 때문에 GPT의 시퀀스 인코딩을 위한 Attention 블록은 Transformer의 디코더에서 사용된 \"Masked Multi Self Attention\"이 사용된다.\n","\n","<br>\n","\n","- BERT의 경우 `[MASK]` 토큰으로 치환된 단어를 예측하게 되고, `[MASK]` 토큰을 포함하여 주어진 전체 단어에 대한 접근이 가능하다.\n","- 그러므로 BERT는 Transformer의 인코더에서 사용되는 Self Attention 모듈을 사용하게 된다."]},{"cell_type":"markdown","metadata":{"id":"d03sCjaTyW49"},"source":["<br>\n","\n","### 8.3.6 Fine-tuning Process\n","\n","- MLM과 NSP를 pre-training task로서 사용하여 사전 학습한 모델을 다양한 downstream task에 fine-tuning하는 모델 구조에 대해 알아보자.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1GwcPB1H8VISgluqofI4zdzhguZkaVfMR' width=800/>\n","\n","- 출처\n","  - https://blog.openai.com/language-unsupervised/\n","  - BERT: Pre-training of deep bidirectional transformers for language understanding, NAACL’19"]},{"cell_type":"markdown","metadata":{"id":"YRP3xzdJ7fOb"},"source":["<br>\n","\n","#### 8.3.6.1 Sentence Pair Classification Task\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16px3Hh5ANvWQWPj1DPWC32jpj2-ZY6AN' width=400/>\n","\n","- 두개의 문장의 논리적인 내포 관계 혹은 모순 관계를 분류하는 task\n","- 두개의 문장을 `[SEP]`를 통해 연결하여 하나의 시퀀스로 만든다.\n","- 이 시퀀스를 BERT를 통해 각각의 단어에 대한 인코딩 벡터를 얻은 후 `[CLS]` 토큰에 해당하는 인코딩 벡터를 output layer의 입력으로 줌으로서 예측 task를 수행한다."]},{"cell_type":"markdown","metadata":{"id":"zHkV9KI07fHJ"},"source":["<br>\n","\n","#### 8.3.6.2 Single Sentence Classification Task\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16s9XMdgu4IePAQX8cDy3_bcADCfVO4PT' width=400/>\n","\n","- 단일 문장에 대한 예측 task는 입력으로 문장이 한 번에 하나씩 주어지기 떄문에 하나의 문장에 `[CLS]` 토큰을 추가하여 입력 시퀀스를 생성한다.\n","- 이 시퀀스를 BERT를 통해 각각의 단어에 대한 인코딩 벡터를 얻은 후 `[CLS]` 토큰에 대한 인코딩 벡터를 output layer의 입력으로 줌으로서 예측 task를 수행한다."]},{"cell_type":"markdown","metadata":{"id":"yUKmt-Y07e_R"},"source":["<br>\n","\n","#### 8.3.6.3 Question Answering Task\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16uWxCJWUl20xbflnJFM_bbjOBS1wRrSO' width=400/>\n","\n","- 질의응답 task에 대해서는 뒤에서 자세히 설명한다."]},{"cell_type":"markdown","metadata":{"id":"SMm2TGbv7e4n"},"source":["<br>\n","\n","#### 8.3.6.4 Single Sentence Tagging Task\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16vU67SqTCYc6xk-wNJKs_FmnLTsfzXqv' width=400/>\n","\n","- POS Tagging 같이 문장의 각 단어의 품사를 태깅하는 task를 수행할 수 있다.\n","- `[CLS]` 토큰을 포함한 시퀀스의 각 단어를 BERT를 통해 인코딩 벡터로 변환하고 해당 벡터를 동일한 output layer를 통과시켜 각 단어별 classification task를 수행한다."]},{"cell_type":"markdown","metadata":{"id":"35fIc4do7JvE"},"source":["<br>\n","\n","### 8.3.7 BERT vs GPT-1\n","\n","- Comparison of BERT and GPT-1"]},{"cell_type":"markdown","metadata":{"id":"TrneNhqaCAdM"},"source":["<br>\n","\n","#### 8.3.7.1 Training-data size\n","\n","- GPT is trained on BookCorpus (800M words)\n","- BERT is trained on the BookCorpus and Wikipedia (2,500M words)\n","  - BERT는 GPT보다 3배 많은 학습 데이터를 사용했다."]},{"cell_type":"markdown","metadata":{"id":"GObFYIQDCMk7"},"source":["<br>\n","\n","#### 8.3.7.2 Training special tokens during training\n","\n","- BERT learns `[SEP]`, `[CLS]`, and sentence A/B embedding during pre-training\n","\n","<br>\n","\n","- GPT-1에서의 `[Extract]` 토큰의 역할을 BERT에서는 `[CLS]` 토큰이 수행한다.\n","- BERT에서는 두 문장을 구분하는 `[SEP]` 토큰이 사용된다.\n","- BERT에서는 2개의 문장이 주어졌을 때 각 문장별로 인덱스를 나타낼 수 있는 segment embedding을 입력 시퀀스에 더해줌으로서 두 문장을 잘 구분할 수 있도록 한다."]},{"cell_type":"markdown","metadata":{"id":"tCqS0pZGCNpT"},"source":["<br>\n","\n","#### 8.3.7.3 Batch size\n","\n","- GPT-1: 32,000 words\n","- BERT: 128,000 words\n","\n","<br>\n","\n","- Batch size: 하이퍼파라미터로서 한 번에 학습하는 단어의 갯수\n","- 한 번 학습 당시에 BERT가 훨씬 많은 단어를 사용한다.\n","- 더 큰 batch size를 사용하면 최종 모델의 성능이 더 좋아지고 학습도 안정화될 수 있다.\n","  - Gradient Descent 알고리즘을 수행할 때 batch size 만큼의 데이터를 통해 도출된 gradient를 바탕으로 파라미터의 업데이트하게 된다.\n","  - 이 때 더 많은 데이터를 통해 도출된 gradient를 사용할 때 학습이 더 안정적이다.\n","- 하지만 batch size를 크게 하면 한 번에 load해와야 하는 입력 데이터 뿐만 아니라 forward propagation과 backpropagation 상에서 필요로 하는 메모리 사용량도 이에 비례해서 증가해야 한다.\n","- 따라서 큰 batch size를 사용할수록 더 많은양의 메모리와 고성능의 GPU를 필요로 한다."]},{"cell_type":"markdown","metadata":{"id":"xndDj9YICPdv"},"source":["<br>\n","\n","#### 8.3.7.4 Task-specific fine-tuning\n","\n","- GPT uses the same learning rate of 5e-5 for all fine-tuning experiments\n","- BERT chooses a task-specific fine-tuning learning rate\n","\n","<br>\n","\n","- GPT의 경우 fine-tuning 할 때 다양한 downstream task에서 learning rate을 동일한 값(`5e-5`)을 사용했다.\n","- BERT의 경우 각 task별로 learning rate 값을 optimize 해줘야 한다."]},{"cell_type":"markdown","metadata":{"id":"fEWuqZfH8J-D"},"source":["<br>\n","\n","### 8.3.8 BERT: GLUE Benchmark Results\n","\n","- GLUE Benchmark Results\n","\n","<br>\n","\n","- MNLI, QQP 등과 같은 여러 자연어 처리 task를 한데 모아놓은 데이터셋(or Benchmark)을 **GLUE**라고 부른다.\n","- GLUE 데이터에 대한 공식 웹사이트에 공개되어 사용할 수 있는 각각의 task에 해당하는 다양한 데이터셋들이 있다.\n","\n","<br>\n","\n","- BERT 논문에서도 GPT와 마찬가지로 다양한 downstream task에 대한 성능을 보여줬다.\n","- BERT를 다양한 자연어 처리 task에 fine-tuning 형태로 적용했을 때 기존의 여러 알고리즘들에 비해 일관적으로 더 좋은 성능을 내는 것을 알 수 있다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_Yubew4IQnvFGg9LVnLk9qec475EA_SO' width=800/>\n","\n","- 출처: BERT: Pre-training of deep bidirectional transformers for language understanding, NAACL’19"]},{"cell_type":"markdown","metadata":{"id":"qHnFFtuL8UaL"},"source":["<br>\n","\n","### 8.3.9 Machine Reading Comprehension (MRC)\n","\n","- BERT를 fine-tuning 하여 더 높은 성능을 얻을 수 있는 대표적인 task로서 **Machine Reading Comprehension(MRC)**가 있다.\n","- MRC는 기본적으로 질의 응답(Question Answering)의 한 형태이다.\n","- 그렇지만 질문만 주어지고 해당 질문에 대한 답을 예측하는 task가 아니라 기계 독해(Machine Reading)에 기반한 task이다.\n","- 즉, 주어진 지문이 있을 때 이를 잘 이해하고 질문에서 필요로 하는 정보를 추출하여 질문에 대한 해답을 예측하는 task를 기계 독해 기반의 task라고 부른다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1NyjFa5nDPDMAFKvVbov2YuNmXh4vJSn9' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"nxrZOwBJPi4J"},"source":["<br>\n","\n","### 8.3.10 SQuAD (Stanford Question Answering Dataset)\n","\n","- MRC task는 위와 같은 간단한 예제에서 더 나아가서 좀 더 어렵고 유의미한 수준의 데이터셋이 공개되어 있다.\n","- 그 데이터셋 중 대표적으로 **SQuAD** 라는 것이 있다.\n","- SQuAD 데이터는 수많은 crowd worker들을 통해 해당 task를 수행하여 얻은 데이터를 바탕으로 구축되었다.\n","- 위키피디아에서 랜덤하게 뽑은 document에 대해 crowd worker들에게 해당 지문을 읽고 독해를 통해 풀 수 있는 문제와 해답을 출제하도록 했다.\n","- 정답에 해당하는 문구가 지문 상에 포함되어 있다.\n","\n","<br>\n","\n","- SQuAD 데이터셋은 1.1 버전과 2.0 버전이 존재한다.\n","- SQuAD 데이터셋 공식 웹사이트\n","  - [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n"]},{"cell_type":"markdown","metadata":{"id":"s5UrFmqT8w5f"},"source":["<br>\n","\n","#### 8.3.10.1 BERT: SQuAD 1.1\n","\n","- SQuAD 1.1 데이터셋에 대한 모델 성능 순위를 살펴보면 다양한 BERT의 변형 모델들이 있는 것을 확인할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1zzAiqMh-vclH3YnJn3fPAVtmZugyQ9wp' width=600/>\n","\n","<br>\n","\n","- 기계 독해 기반의 질의 응답 task에 적용되는 BERT 모델에 대해 살펴보자.\n","\n","<br>\n","\n","- Only new parameters: Start vector and end vector\n","\n","$$\n","P_{i}=\\frac{e^{S \\cdot T_{i}}}{\\sum_{j} e^{S \\cdot T_{j}}}\n","$$\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1uBPbkjicQmLLSsXgyk99f7IQIXOlMQw1' width=600/>\n","\n","<br>\n","\n","- BERT의 입력으로서 주어진 지문과 질문을 2개의 서로 다른 문장인 것처럼 `[SEP]` 토큰을 통해 연결하여 하나의 시퀀스로 만든 다음 BERT를 통해 인코딩을 진행한다.\n","- 지문 안에 있는 각각의 단어별 인코딩 벡터가 나온다.\n","- 이러한 단어 인코딩 벡터들 중에서 정답에 해당할 것 같은 문구의 위치를 예측하도록 모델을 학습시킨다.\n","\n","<br>\n","\n","- 지문에서 정답에 해당하는 문구가 시작하는 위치를 예측하기 위해 각 단어별로 인코딩 벡터가 출력으로 나왔을 때 이들을 공통된 output layer를 통해서 스칼라값을 뽑아내도록 한다.\n","- 각 단어의 인코딩 벡터가 2차원 형태일 경우 해당 벡터에 적용해야 할 output layer는 1차원, 즉 스칼라값의 출력을 만들어내는 FC layer가 될 것이다.\n","- output layer에 해당하는 FC layer의 파라미터가 fine-tuning의 대상이 되는 파라미터가 된다.\n","\n","<br>\n","\n","- 각 단어별 스칼라값을 얻은 후에는 <font color='skyblue'>정답에 해당하는 문구가 어느 단어에서부터 시작하는 지를 먼저 예측</font>하게 한다.\n","- 만약 124개의 단어가 있다고 하면 총 124개의 스칼라값이 존재할 것이고 이 값들에 대해 softmax를 적용하여 해당 값들의 ground truth로서 정답에 해당하는 첫 번째 단어의 logit 값에 가까워지도록 하는 softmax loss를 통해서 모델을 학습한다.\n","\n","<br>\n","\n","- 그런 다음 <font color='skyblue'>정답에 해당하는 문구가 끝나는 지점에 대한 예측</font>을 수행한다.\n","- 이 경우 정답에 해당하는 시작 단어를 예측하는 FC Layer와 더불어서 정답에 해당하는 마지막 단어를 예측하는 FC Layer를 별도로 만들고 해당 output layer를 통과하여 각 단어 인코딩 벡터가 스칼라값으로 변하게끔 한다.\n","- 그 후 마찬가지로 모든 스칼라값들에 대해 softmax를 적용한 후 정답에 해당하는 마지막 단어를 예측하도록 모델을 학습한다.\n","\n","<br>\n","\n","- 위와 같은 2개의 FC Layer를 학습하게 된다.\n","\n","\n","\n","<br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fFKg8qrR9Inb"},"source":["<br>\n","\n","#### 8.3.10.2 BERT: SQuAD 2.0\n","\n","- SQuAD 2.0 데이터셋에 대한 모델 성능 순위에도 다양한 BERT의 변형 모델들이 있는 것을 확인할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1t9B4rRLw6MK1eaDgxClJArPmtpNU9Os8' width=600/>\n","\n","<br>\n","\n","- Use token 0 (`[CLS]`) to emit logit for \"no answer\"\n","- \"No answer\" directly competes with answer span\n","- Threshold is optimizedd on dev set\n","\n","<br>\n","\n","$$\n","P_{i}=\\frac{e^{S \\cdot T_{i}}}{\\sum_{j} e^{S \\cdot T_{j}}}\n","$$\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1FuzsEybRadTOIShNQE7C8e_0ZmrN47S4' width=600/>\n","\n","<br>\n","\n","- SQuAD 1.1에서는 주어진 지문에 대해서 항상 정답이 존재하는 질문들을 만들었다.\n","- SQuAD 2.0에는 지문 안에서 정답을 찾을 수 없는 경우의 데이터셋까지도 기존의 데이터에 추가하여 구축하였다.\n","\n","<br>\n","\n","- 해당 데이터셋에서 예측해야 하는 task는 다음과 같다.\n","  - 주어진 지문안에 질문에 대한 해답이 있는지 없는지를 판단한다.\n","  - 해답이 있을 경우 SQuAD 1.1에서 설명한 방식의 예측 과정을 수행한다.\n","\n","<br>\n","\n","- 지문안에 해답이 있는 지 없는지를 판단하는 task는 high-level에서 보면 문단과 질문을 종합적으로 보고 판단해야 하는 task가 된다.\n","- 이때 `[CLS]` 토큰을 활용하여 지문안에 해답이 있는 지 없는지를 예측할 수 있다.\n","- 지문과 질문을 concat하여 하나의 시퀀스로 만들고 BERT를 통해 인코딩한다.\n","- 이렇게 얻은 `[CLS]` 토큰에 해당하는 인코딩 벡터를 binary classification하는 output layer의 입력으로 사용하여 지문에 해답이 있는지 없는지를 예측한다.\n","\n","<br>\n","\n","- 최종적으로 예측에 해당 모델을 사용할 떄는 먼저 `[CLS]` 토큰을 통해 답이 있는지 없는지를 예측하고 답이 있는 것으로 예측 결과가 나온 경우 정답에 해당하는 시작 단어 및 종료 단어를 예측하는 task를 수행한다.\n"]},{"cell_type":"markdown","metadata":{"id":"Oe4wNZKz9kfi"},"source":["<br>\n","\n","### 8.3.11 BERT: On SWAG\n","\n","- 다수의 문장을 다뤄야 하는 task에 BERT를 사용한 사례에 대해 살펴보자.\n","\n","<br>\n","\n","- Run each Premise + Ending through BERT\n","- Produce logit for each pair on token 0 (`[CLS]`)\n","\n","<br>\n","\n","$$\n","P_{i}=\\frac{e^{V \\cdot C_{i}}}{\\sum_{j=1}^{4} e^{V \\cdot C_{j}}}\n","$$\n","\n","<br>\n","\n","- 주어진 문장과 보기 문장들이 있을 때 특정 문장 다음에 나올만한 문장을 선택하는 task이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mjka6UGGeOs4to3SstgLdwI3dcmFP2E7' width=600/>\n","\n","- 이 경우에도 기본적으로 `[CLS]` 토큰을 사용하게 된다.\n","  - 첫 문장을 선택 가능한 4개의 문장 중 첫 번째 문장과 concat하여 BERT를 통해 인코딩을 하여 나온 `[CLS]` 토큰에 대한 인코딩 벡터에 FC Layer를 적용하여 스칼라값을 생성한다.\n","  - 그 다음 동일한 문장을 선택 가능한 4개의 문장 중 두 번째 문장과 concat하여 BERT를 통해 인코딩을 하여 나온 `[CLS]` 토큰에 대한 인코딩 벡터에 FC Layer를 적용하여 스칼라값을 생성한다.\n","- 이러한 과정을 모든 보기 문장에 대해 실시하여 각 보기 문장에 대한 스칼라값들을 얻게 된다.\n","- 이 스칼라값들을 softmax의 입력으로 주고 정답에 해당하는 위치의 ground truth가 100%가 나올 수 있도록 하는 softmax loss를 통해 전체 모델을 학습하게 된다.\n","\n","<br>\n","\n","- 해당 데이터셋에 대해서도 BERT를 활용한 모델들이 상위에 랭크된 것을 확인할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1CNzPHVhzxbCM6ihlqdjl4odLouJmBJUn' width=600/>\n","\n","- 출처: https://leaderboard.allenai.org/swag/submissions/public"]},{"cell_type":"markdown","metadata":{"id":"BkePPMDy98Oz"},"source":["<br>\n","\n","### 8.3.12 BERT: Ablation Study\n","\n","- Big models help a lot\n","  - Going from 110M to 340M params helps even on datasets with 3,600 labeled examples\n","  - Improvements have not asymptoted\n","\n","<br>\n","\n","- BERT의 또 다른 결과로서 BERT에서 제안한 모델의 사이즈가 있을 때 그 모델의 layer를 점점 더 깊게 쌓고 각 layer의 파라미터를 더 늘리는 방식으로 학습을 진행하면 모델의 사이즈가 더 커질수록 성능이 끊임없이 상승하다는 것을 발견했다.\n","- 이는 모델의 사이즈를 GPU가 허용하는 최대 범위까지 키웠을 때에도 끊임없이 개선되는 형태의 결과를 보여줬다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=152-kn4HQYTgltRi2-ppqRIfcme3H6h4F' width=600/>\n","\n","- 출처: BERT: Pre-training of deep bidirectional transformers for language understanding, NAACL’19"]},{"cell_type":"code","metadata":{"id":"xySEjEDm-V0q"},"source":[""],"execution_count":null,"outputs":[]}]}