{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09강 - Advanced Self-supervised Pre-Training Models (GPT-2, GPT-3, ALBERT, ELECTRA, DistillBERT, TinyBERT, ERNIE, KagNET).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrwCDIWHt0yp5dhU3peNTx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bORAGtnI-dif"},"source":["# 9. Advanced Self-supervised Pre-Training Models\n","\n","- GPT-2\n","- GPT-3\n","- ALBERT\n","- ELECTRA\n","- Light-weight Models\n","- Fusing Knowledge Graph into Language Model"]},{"cell_type":"markdown","metadata":{"id":"kiVgzT5N-zQ2"},"source":["<br>\n","\n","## 강의 소개\n","\n","- GPT-1과 BERT 이후 등장한 다양한 self-supervised pre-training 모델들에 대해 알아봅니다.\n","\n","<br>\n","\n","- GPT-1과 BERT 이후 pre-training task, 학습 데이터, self-attention, parameter 수 등에 있어서 여러가지 개선된 모델들이 등장했습니다.\n","- GPT 시리즈가 2와 3로 이어지면서 일부 데이터셋/task에 대해서는 사람보다 더 뛰어난 작문 능력을 보여주기도 합니다.\n","- 이로 인해, model size 만능론이 등장하며 resource가 부족한 많은 연구자들을 슬프게 만들기도 했습니다.\n","- 다른 연구 방향으로 transformer의 parameter를 조금 더 효율적으로 활용하고 더 나은 architecture/pre-training task를 찾고자 하는 ALBERT와 ELECTRA에 대해서 알아봅니다.\n","- 두 모델 모두 풍부한 실험과 명확한 motivation으로 많은 연구자들의 관심을 받은 논문입니다.\n","\n","<br>\n","\n","- 위에서 설명드린 연구방향과는 또 다른 연구 흐름으로 경량화 모델/사전 학습 언어 모델을 보완하기 위한 지식 그래프 integration에 대해 소개한 논문들을 간략하게나마 알아봅니다.\n","- 관심 있으신 분들은 이후 진행될 경량화 모델 수업과 그래프 수업을 유심히 들어주시면 좋을 것 같습니다 :)"]},{"cell_type":"markdown","metadata":{"id":"gFMitGdP_X7Z"},"source":["<br>\n","\n","## Further Reading\n","\n","- [How to Build OpenAI’s GPT-2: “ The AI That Was Too Dangerous to Release”](https://blog.floydhub.com/gpt2/)\n","- [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n","- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n","- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLR’20](https://arxiv.org/abs/1909.11942)\n","- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR’20](https://arxiv.org/abs/2003.10555)\n","- [DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, NeurIPS - Workshop'19](https://arxiv.org/abs/1910.01108)\n","- [TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLP’20](https://arxiv.org/abs/1909.10351)\n","- [ERNIE: Enhanced Language Representation with Informative Entities, ACL'19](https://arxiv.org/abs/1905.07129)\n","- [KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning, EMNLP'19](https://arxiv.org/abs/1909.02151)"]},{"cell_type":"markdown","metadata":{"id":"ttl00AlvBIUc"},"source":["<br>\n","\n","## References\n","\n","- How to Build OpenAI’s GPT-2: “ The AI That Was Too Dangerous to Release”\n","  - [https://blog.floydhub.com/gpt2/](https://blog.floydhub.com/gpt2/)\n","- decaNLP\n","  - [https://decanlp.com/](https://decanlp.com/)\n","- GPT-2\n","  - [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)\n","  - [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n","- Language Models are Few-shot Learners, NeurIPS’20\n","  - [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)\n","- Illustrated Transformer\n","  - [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)\n","- ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLR’20\n","  - [https://arxiv.org/abs/1909.11942](https://arxiv.org/abs/1909.11942)\n","- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, JMLR’20\n","  - [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)\n","- ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR’20\n","  - [https://arxiv.org/abs/2003.10555](https://arxiv.org/abs/2003.10555)\n","- DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n","  - [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)\n","- TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLP’20\n","  - [https://arxiv.org/abs/1909.10351](https://arxiv.org/abs/1909.10351)\n","- ERNIE: Enhanced Language Representation with Informative Entities\n","  - [https://arxiv.org/abs/1905.07129](https://arxiv.org/abs/1905.07129)\n","- KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning\n","  - [https://arxiv.org/abs/1909.02151](https://arxiv.org/abs/1909.02151)\n"]},{"cell_type":"markdown","metadata":{"id":"TW-fDJIXUPii"},"source":["<br>\n","\n","## 9.1 GPT-2\n","\n","> GPT-2: Language Models are Unsupervised Multi-task Learners"]},{"cell_type":"markdown","metadata":{"id":"kppblre2UPfX"},"source":["<br>\n","\n","### 9.1.1 Overview\n","\n","- Just a really big transformer LM\n","- Trained on 40GB of text\n","  - Quite a bit of effort going into making sure the dataset is good quality\n","  - Take webpages from reddit links with high karma\n","- Language model can perfrom <font color='red'>down-stream tasks in a zero-shot setting</font> - without any parameter or architecture modification\n","\n","<br>\n","\n","- GPT-2는 모델 구조 측면에서 GPT-1과 크게 다르지 않다.\n","- 다만 Transformer Layer를 좀 더 많이 쌓아서 모델을 키웠다.\n","- Pre-traininig task는 여전히 Language Modeling(LM) task(다음 단어 예측)를 통해 학습을 진행했다.\n","- 학습 데이터는 굉장히 증가한 크기인 40GB를 사용했다.\n","- 데이터셋을 대규모로 사용하는 과정에서 높은 품질(quality)의 데이터셋을 사용했다.\n","  - 잘 쓰여진 글로부터 다양한 지식을 배울 수 있도록 하는 방식을 유도했다.\n","- 여러 down-stream task가 생성 모델이라는 language generation task의 zero-shot 세팅으로서 다뤄질 수 있다는 잠재적인 능력도 보여줬다."]},{"cell_type":"markdown","metadata":{"id":"ZriertZyUPcw"},"source":["<br>\n","\n","- GPT-2 모델은 지금까지 주어진 텍스트를 바탕으로 다음 단어를 예측하는 task를 사용하여 학습되기 때문에 이 학습된 모델을 활용할 때 아래 그림처럼 주어진 첫 문단이 있다면 이 것을 이어 받아서 하나의 긴 글을 완성할 수 있는 능력을 가지게 된다.\n","- 아래 그림에서 GPT-2를 통해서 얼마나 말이 되는 문장이 생성되었는 지를 볼 수 있다.\n","- 이처럼 GPT-2가 사람이 봐도 이해할 수 있을 정도의 품질을 갖는 문장을 생성할 수 있다는 것이 많은 사람들을 놀라게 했다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18E5pa9YiKS7ims-ecw3sj-to7_49SIvL' width=800/>\n","\n","- 출처: https://blog.floydhub.com/gpt2/"]},{"cell_type":"markdown","metadata":{"id":"cMKKk3T7UPZx"},"source":["<br>\n","\n","### 9.1.2 Motivation (decaNLP)\n","\n","- The Natural Language Decathlon: Multitask Learning as Question Answering\n","  - Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18EBagdh88FFbdph9zRk5FtScoV7VGgu6' width=900/>\n","\n","- 출처: https://decanlp.com/\n","\n","<br>\n","\n","- decaNLP 논문이 GPT-2의 motivation이 되었다.\n","- decaNLP 논문의 핵심\n","  - 주어진 문장이 긍정인지 부정인지 예측하는 task와 질문 문장에 대한 대답 문장을 생성하는 task가 있다고 할 때 2개의 task는 output의 구조가 다르기 때문에 딥러닝 관점에서 봤을 때 모델의 구조가 상이하다.\n","  - 그렇지만 해당 논문에서는 **모든 자연어 처리 task들이 질의 응답(Question Answering) task로 바뀔 수 있다**고 말했다.\n","  - 이를 바탕으로 모든 task를 자연어 생성의 형태로 통합하여 학습을 하는 것을 제안한 논문이다.\n","  - ex) 긍부정 분류 task\n","    - 주어진 문장이 긍정인지 부정인지를 질문하는 문장을 만들어서 질의 응답 task로 변환할 수 있다.\n","    - \"Do you think whether this sentence is positive or negative?\"\n","  - ex) 문서 요약 task\n","    - 주어진 문단 마지막에 해당 문단에 요약된 정보를 묻는 질문 문장을 만들어서 질의 응답 task로 변환할 수 있다.\n","    - \"What is the summarization of the above paragraph?\"\n","  - ex) 번역 task (Eng -> Kor)\n","    - 주어진 영어 문장 마지막에 해당 문장에 대한 한국어 문장을 묻는 질문 문장을 만들어서 질의 응답 task로 변환할 수 있다.\n","    - \"What is the translated sentence in Korean?\""]},{"cell_type":"markdown","metadata":{"id":"vEl003rUUPVd"},"source":["<br>\n","\n","### 9.1.3 Datasets\n","\n","- A promising source of diverse and nearly unlimited text is web scrape such as common crawl\n","  - They scraped all outbound links from Reddit, a social media platform, WebText\n","    - 45M links\n","      - Scraped web pages which have been curated/filtered by humans\n","      - Received at least 3 karma (up-vote)\n","  - 8M removed Wikipedia documents\n","  - Use dragnet and newspaper to extract content from links\n","\n","<br>\n","\n","- 많은 양의 데이터를 사용함과 동시에 높은 수준의 글을 선별하여 잘 쓰여진 글로부터 지식을 효과적으로 얻을 수 있도록 Reddit이라는 커뮤니티 웹사이트에서 데이터를 추출했다.\n","  - Reddit: 질문에 대한 대답과 토론이 이루어지는 웹사이트\n","- 해당 사이트에서 외부 링크를 포함하는 포스팅 및 답글이 많은 사람들로부터 좋아요를 받았을 경우 외부 링크를 통해 이동했을 때 나타나는 포스트에 대해서도 데이터를 추출했다.\n","  - all outbound links: 외부 링크\n","  - 3 karma (up-vote): 3개 이상의 좋아요를 받은 글들"]},{"cell_type":"markdown","metadata":{"id":"ZA1uA0IUUPRp"},"source":["<br>\n","\n","### 9.1.4 Preprocess\n","\n","- Byte pair encoding (BPE)\n","- Minimal fragmentation(파편화) of words across multiple vocab tokens\n","\n","<br>\n","\n","- GPT-2의 또 다른 기술적인 특징으로 BERT에서 사용된 WordPiece와 비슷하게 BPE라는 subword level의 단어 인코딩 기법을 사용했다."]},{"cell_type":"markdown","metadata":{"id":"RhwsoiKCUPPI"},"source":["<br>\n","\n","### 9.1.5 Model - Modification\n","\n","- Layer normalization was moved to the input of each sub-block, similar to a pre-activation residual network\n","- Additional layer normalization was added after the final self-attention block.\n","- Scaled the weights of residual layer at initialization by a factor $1/\\sqrt{n}$ where $n$ is the number of residual layer\n","  - 각 layer를 random initialization할 때 layer가 위로 갈수록 layer의 인덱스에 비례 또는 반비례하여 초기화되는 값을 더 작은 값으로 만들었다.\n","  - 위쪽으로 갈수록 거기에서 쓰이는 선형 변환에 해당하는 값들이 점점 0에 가까워지도록 하여 위쪽에 있는 layer의 역할이 점점 더 줄어들 수 있도록 했다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18EelUiBDHQJiyzIn9fORIMlz9GBBb49G' width=400/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18K1HLH4_Hg2Kmr_pzGVJigRUbQr65L5f' width=900/>\n","\n","- 출처: https://openai.com/blog/better-language-models/"]},{"cell_type":"markdown","metadata":{"id":"AkS8A9cwALgA"},"source":["<br>\n","\n","### 9.1.6 Performance in tasks"]},{"cell_type":"markdown","metadata":{"id":"aZPsWDvLAeR6"},"source":["#### 9.1.6.1 Question Answering\n","\n","- Use conversation question answering dataset(CoQA)\n","  - Achieved 55 $F_1$ scores, exceeding the performance 3 out of 4 baselines without labeled dataset\n","  - Fine-tuned BERT achieved 89 $F_1$ performance\n","\n","<br>\n","\n","- decaNLP에서 제안된 모든 자연어처리 task가 질의응답 task로 바꿀 수 있다는 점에 입각하여 GPT-2에서 했던 흥미로운 실험 중 하나는 다음과 같다.\n","- 주어진 대화형 질의응답 데이터셋 CoQA가 있을 때 이 task를 수행하기 위해서는 해당 데이터셋을 통해 fine-tuning하는 과정을 일반적으로 수행하게 된다.\n","- 하지만 여기서는 zero-shot learning의 세팅에서 CoQA에 대한 예측 task를 수행함에 있어서 대화형 질문글을 주고 나서 다음에 나올 답을 예측하도록 했다.\n","- 즉, CoQA 데이터셋을 통해 모델을 fine-tuning하지 않고 해당 데이터셋에 대해 얼마나 정확하게 정답 문장을 만들어내는 지 테스트를 했다.\n","- 그 결과 55% 정도의 f1 score가 나왔고, 이는 fine-tuning을 한 후 얻은 89%의 f1 score에는 못미치지만 어느정도는 예측을 수행할 수 있다는 가능성을 발견할 수 있었다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zzwT8EG_UPJd"},"source":["#### 9.1.6.2 Summarization\n","\n","- CNN and Daily Mail Dataset\n","\n","<br>\n","\n","- Add text TL;DR: after the article and generate 100 tokens\n","  - TL;DR: Too long, didn't read\n","\n","<br>\n","\n","- 요약을 위한 task를 수행하기 위해서도 fine-tuning이라는 과정 없이 zero-shot 세팅으로 inference를 수행할 수 있다.\n","- 아래 그림과 같이 기사(article)를 주고 GPT-2가 다음 단어를 순차적으로 예측하는 task를 수행하기 때문에 마지막 부분에 `TL;DR` 이라는 단어를 준다.\n","  - `TL;DR`: 너무 길어서 읽지 못하겠다!\n","- `TL;DR`이 등장하면 앞쪽에 있던 글을 한 줄로 요약하게끔 했다.\n","- 이를 통해 CNN and Daily Mail Dataset에 대해서도 fine-tuning 없이 zero-shot 세팅을 통해 요약을 수행할 수 있었다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18NkVsWcPqnaEiApUK72nwLPFLUifm_a3' width=600/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18Nqe4a_qC4aHjSSUOJChjQHFvravSsJh' width=400/>\n","\n","- 출처: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"]},{"cell_type":"markdown","metadata":{"id":"o9oV422tUPGE"},"source":["<br>\n","\n","#### 9.1.6.3 Translation\n","\n","- User WMT14 en-fr dataset for evaluation\n","\n","<br>\n","\n","- Use LMs on a context of example pairs of the format:\n","  - English sentence = French sentence\n","- Achieve 5 BLEU score in word-by-word substitution(대체)\n","  - Slightly worse than MUSE (Conneau et al., 2017)\n","\n","<br>\n","\n","- 번역 task에 대해서도 주어진 문장이 있을 때 해당 문장 다음에 번역하고자 하는 언어로 번역할 수 있게끔 마지막 문장을 붙여준다.\n","  - \"they say in French\"\n","  - \"in French\"\n","- 이렇게 하면 앞서 나온 문장을 다른 나라의 언어로 잘 번역하는 사례를 보여주고 있다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18XWDJAlHkTjFojlFpNopG6gHbasvFe4m' width=500/>\n","\n","- 출처: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"]},{"cell_type":"markdown","metadata":{"id":"5InOE6GL_8uq"},"source":["<br>\n","\n","## 9.2 GPT-3\n","\n","> Language Models are Few-Shot Learner, NeurlPS'20"]},{"cell_type":"markdown","metadata":{"id":"S_x8ZSdebidu"},"source":["<br>\n","\n","### 9.2.1 Overview\n","\n","- Scaling up language models greatly improves task-agnostic, few-shot performance\n","- An autoregressive language model with 175 billion parameters in the few-shot setting\n","- 96 Attention layers, Batch size of 3.2M\n","\n","<br>\n","\n","- GPT-3는 GPT-2를 훨씬 더 개선한 모델이다.\n","- 개선의 방향은 모델의 구조 측면에서의 변화 보다는 GPT-2가 가지고 있던 파라미터의 숫자에 비해서 비교할 수 없을 정도로 훨씬 더 많은 파라미터 수를 갖도록 Transformer의 self-attention block을 더 많이 쌓은 것이다.\n","- 그리고 역시 더 많은 데이터 및 더 큰 배치 사이즈를 통해 학습을 진행했고, 이를 통해 지속적인 성능 향상을 얻을 수 있었다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16x1QvZIens33yvLemjO-G4Tmm1s0syHH' width=800/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hKvSK9BcbvCl"},"source":["<br>\n","\n","### 9.2.2 Few-Shot Learners\n","\n","- GPT-2에서 보여준 zero-shot setting의 가능성을 놀라운 수준으로 끌어올렸다.\n","\n","<br>\n","\n","- Prompt: the prefix given to the model"]},{"cell_type":"markdown","metadata":{"id":"EA91NFuPBh-x"},"source":["<br>\n","\n","#### 9.2.2.1 Zero-shot\n","\n","- Predict the answer given only a natural language description of the task\n","\n","<br>\n","\n","- 번역에 대한 데이터를 학습에 전혀 사용하지 않고 GPT-3 모델을 가져와서 하고자 하는 task와 관련된 정보를 담고 있는 \"task description\" 문장을 주면 \"prompt\" 문장에 있는 내용을 번역하게끔 했다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17N5nWl2caAIQ355Z_UygsHEk-zWlvdJL' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"pKGKc2YoBlix"},"source":["<br>\n","\n","#### 9.2.2.2 One-shot\n","\n","- See a single example of the task in addition to the task description\n","\n","<br>\n","\n","- 하고자 하는 task를 \"task description\"을 통해 주고 예시 문장인 \"example\"을 제공한다.\n","- 그 다음 예시와 동일한 패턴으로 이루어진 \"prompt\"를 줘서 번역을 수행했다.\n","- 즉, one-shot setting의 경우 학습 데이터로서 번역에 대한 데이터를 한쌍만 제공했다는 것이 된다.\n","- 여기서 특이한 점은 **모델의 구조를 일부라도 변경하지 않고 inference 과정 중에 \"example\" 문장을 학습 데이터의 일부로서 제시**했을 때 번역 수행 결과가 zero-shot setting 보다 훨씬 더 좋아진다는 점이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17O4nfZ-otvgwRpuHBlmFp_CEqgLOQ7Mu' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"IgbBdpKGBo5G"},"source":["<br>\n","\n","#### 9.2.2.3 Few-shot\n","\n","- See a few examples of the task\n","\n","<br>\n","\n","- 예시 문장인 \"example\"을 한 개 이상 제공하게 되면 유의미하게 더 높은 성능을 내는 것을 알 수 있었다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17OBCFY0MVWMMnTqQW08jF9T8M3yJRnCU' width=600/>\n"]},{"cell_type":"markdown","metadata":{"id":"2U7ymp9YDorY"},"source":["<br>\n","\n","#### 9.2.2.4 Concolusion\n","\n","- 별도의 fine-tuning 과정 없이 GPT-3 모델을 그대로 가져와서 예시 문장을 보여주고 하나의 시퀀스 내에서 패턴을 동적으로 빠르게 학습한 후 번역을 수행할 수 있는 GPT-3의 놀라운 성능을 보여주고 있다."]},{"cell_type":"markdown","metadata":{"id":"2C8FIBqu7t6Q"},"source":["<br>\n","\n","### 9.2.3 Performance\n","\n","- Zero-shot performance improves steadily with model size\n","- Few-shot performance increases more rapidly\n","\n","<br>\n","\n","- GPT-3에서 보여준 다양한 결과 중에 모델 사이즈를 점점 더 키우면 키울수록 zero-shot, one-shot, few-shot에서의 성능이 올라가는 차이(gap)가 더 빠르게 올라가는 것을 확인할 수 있었다.\n","- 즉, 큰 모델을 사용할수록 모델의 동적인 적응 능력이 훨씬 더 뛰어나다는 것을 말해준다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=188OSRih1RVKr4p6BJ_JZnxPPrRgAAd6k' width=700/>\n","\n","- 출처: Language Models are Few-show Learners, NeurIPS’20"]},{"cell_type":"markdown","metadata":{"id":"UU2LQVxO8G6A"},"source":["<br>\n","\n","## 9.3 ALBERT\n","\n","> ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"]},{"cell_type":"markdown","metadata":{"id":"W_ZGXp6r8cyW"},"source":["<br>\n","\n","### 9.3.1 Overview\n","\n","- Is having better NLP models as easy as having larger models?\n","\n","<br>\n","\n","- Obstacles (장애물)\n","  - Memory Limitation\n","  - Training Speed\n","- Solutions\n","  - Factorized Embedding Parameterization\n","  - Cross-layer Parameter Sharing\n","  - (For Performance) Sentence Order Prediction\n","\n","<br>\n","\n","- 앞서 살펴본 pre-training 모델들은 대규모의 메모리 요구량과 학습에 필요로 하는 많은 파라미터를 갖는 형태로 점점 더 발전해왔다.\n","- 이를 위헤서는 더 많은 GPU 메모리를 필요로 하고 거대한 모델을 학습하는 데에 많은 학습 데이터와 시간을 필요로 했다.\n","- ALBERT라는 모델에서는 기존의 BERT 모델의 사이즈가 크다는 점을 개선하여 성능 하락없이 모델의 사이즈를 줄이고 학습 속도를 빠르게 만들 수 있으면서 추가적으로 새로운 형태의 문장 레벨의 self-supervised learning의 pre-training task를 제안했다."]},{"cell_type":"markdown","metadata":{"id":"0WbUGwTw81_e"},"source":["<br>\n","\n","### 9.3.2 Factorized Embedding Parameterization\n","\n","- Self Attention block을 점차 쌓아감으로서 만들어지는 BERT 또는 GPT 모델을 볼 때 Residual Connection이 있기 때문에 입력에 주어지는 단어 임베딩 벡터의 차원 수가 self attention block을 지나서도 계속해서 유지되는 것을 볼 수 있다.\n","\n","<br>\n","\n","- 이 단어 임베딩 벡터의 차원 수가 너무 작으면 정보를 담을 수 있는 공간이 제한된다는 단점이 있을 수 있다.\n","- 반면 차원 수가 너무 커지면 모델 사이즈도 커지고 거기에 필요로 하는 연산량도 증가하게 된다.\n","\n","<br>\n","\n","- self attention block을 쌓아나가면서 의미론적으로 유의미한 정보를 쌓아가게 된다.\n","- 첫 번째 블록에서의 단어별 임베딩 벡터들은 상대적으로 위쪽 블록에 있는 단어별 임베딩 벡터들에 비해 적은 정보를 갖고 있다.\n","- 그래서 ALBERT 모델에서는 임베딩 layer의 차원 수를 줄이는 추가적인 기법을 사용했다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17SZiAZwzrA3TYy1aj96bSxD0Vi1aBUEV' width=900/>\n","\n","- 출처: http://jalammar.github.io/illustrated-transformer/"]},{"cell_type":"markdown","metadata":{"id":"BIF0rYLD89dt"},"source":["<br>\n","\n","- V = Vocabulary size\n","- H = Hidden-state dimension\n","- E = Word embedding dimension\n","\n","<br>\n","\n","- 첫 번째, 두 번째 self attention block에서 고정해서 사용해야 하는 dimension이 4라고 하자.\n","\n","<br>\n","\n","- embedding layer에서는 시퀀스의 각 단어들이 self attention block의 입력으로 사용되기 위해 어쩔 수 없이 4차원으로 임베딩 되어야 한다.\n","- 이 경우 단어 임베딩 벡터를 입력으로 주기 전에 이 벡터의 차원을 감소시켜서 필요로하는 파라미터와 계산량을 줄일 수 있는 기법을 제시했다.\n","- 먼저 4보다 작은 차원(=2)을 갖도록 각 단어들을 임베딩 한 후 layer 하나를 추가하여 2차원의 단어 임베딩 벡터를 4차원으로 늘려주도록 한다.\n","- 추가된 layer에서 적용되는 선형 변환을 통해 2차원의 단어 임베딩 벡터를 4차원으로 바꿔줄 수 있게 된다.\n","- 이 과정이 기존의 $V \\times H$ 크기의 행렬을 low-rankmatrix factorization 이라는 기법을 통해서 $V \\times E$와 공통으로 사용되는 $E \\times H$로 만들어줌으로서 전체적인 파라미터의 수를 줄여줄 수 있다.\n","\n","<br>\n","\n","- ex) V=500, H=100, E=15\n","  - 기존 방법의 파라미터 수: 500x100 = 50,000\n","  - 새로운 방법의 파라미터 수: 500x15 + 15x100 = 7,500 + 1,500 = 9,000\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17X1CGi8W4Uum6O0BqFRbGJa6V68N6PId' width=900/>\n","\n","- 출처: http://jalammar.github.io/illustrated-transformer/"]},{"cell_type":"markdown","metadata":{"id":"lnWbaY7Q9PUK"},"source":["<br>\n","\n","### 9.3.3 Cross-layer Parameter Sharing\n","\n","- 각각의 self attention block에서 학습해야 할 파라미터들에 대해 알아보자.\n","\n","<br>\n","\n","- 먼저 Q, K, V 벡터로 만들어주기 위해 사용되는 선형 변환의 행렬들이 존재한다.\n","- Multi-Head를 사용하기 때문에 Q, K, V에 해당하는 선형변환 행렬들이 head의 갯수만큼 존재하게 된다.\n","  - $W_0^Q, W_0^K, W_0^V, \\dots, W_n^Q, W_n^K, W_n^V$\n","\n","<br>\n","\n","- 또한 각 head별로 나오는 attention weighted된 value 벡터들을 하나로 concat한 다음 원래의 hidden state vector의 차원으로 바꿔주기 위한 선형 변환의 행렬이 학습의 대상이 된다.\n","  - $W^O$\n","\n","<br>\n","\n","- self attention block이 계속 쌓이게 되면 위와 같은 행렬들이 각각의 self-attention block별로 존재하게 된다.\n","- ALBERT에서는 서로 다른 self attention block에 존재하는 선형 변환 행렬들을 공유하는 방법을 제안한 것이다.\n","\n","<br>\n","\n","- 선형 변환 행렬들을 공유하는 방법들은 다음과 같다.\n","\n","<br>\n","\n","- Shared-FFN\n","  - Only sharing feed-forward network parameters across layers\n","  - $W^O$만 공유\n","- Shared-attention\n","  - Only sharing attention parameters across layers\n","  - $W_0^Q, W_0^K, W_0^V, \\dots, W_n^Q, W_n^K, W_n^V$만 공유\n","- All-shared\n","  - Both of them\n","  - $W_0^Q, W_0^K, W_0^V, \\dots, W_n^Q, W_n^K, W_n^V$와 $W^O$를 모두 공유\n","\n","<br>\n","\n","- 위와 같은 공유 방법을 통해 학습시킨 모델의 다양한 task에 대한 성능을 아래 그림을 통해 비교할 수 있다.\n","  - all-shared의 경우가 파라미터의 수가 가장 적고 not-shared에 비해 성능이 떨어지긴 하지만 성능의 하락폭이 많지는 않은 것을 볼 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17_9vKGiGaKzYcTrznGOZbX10X4QDxVD1' width=800/>\n","\n","- 출처: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLR’20"]},{"cell_type":"markdown","metadata":{"id":"bsBQO_Rl9nxn"},"source":["<br>\n","\n","### 9.3.4 Sentence Order Prediction\n","\n","- Next Sentence Prediction pretraining task in BERT isi too easy\n","- 기존의 BERT에서 사용되던 pre-training 기법이 2가지가 있었다.\n","  - MLM\n","  - NSP\n","- BERT 이후의 후속 논문들에서 NSP task는 그다지 실효성이 없고 NSP task를 제거하고 MLM만을 이용해 pre-training을 수행한 모델들의 성능이 많이 떨어지지 않는다는 것을 말하고 있다.\n","  - BERT에서 Negative samples(next sentence에 해당하지 않는 문장들)을 학습 데이터로 만드는 경우 서로 다른 2개의 문장을 뽑아내는 각각의 document간의 내용이 겹치지 않고 유사한 단어들이 많이 존재하지 않을 가능성이 높다.\n","  - 이렇게 되면 2개의 문장이 연속된 문장이 아님을 예측하는 것이 굉장히 쉬워질 수 있다.\n","  - 또 반대의 경우로 실제로 인접한 2개의 문장을 뽑아온 경우에는 2개의 문장에 유사한 단어들이 많아서 연속된 문장임을 쉽게 예측하게 된다.\n","\n","<br>\n","\n","- ALBERT에서도 NSP task를 좀 더 유의미한 task로 확장했다.\n","\n","<br>\n","\n","- Predict the ordering of two consecutive(연속적인) segments of text\n","  - Negative samples the same two consecutive segments but with their order swapped\n","\n","<br>\n","\n","- 2개의 문장이 실제로 Next Sentence인지 아닌지를 예측하는 것이 아니라 항상 **같은 문장 내에서 연속적으로 등장하는 2개의 문장을 가져온다.**\n","- 2개의 문장을 원래의 순서에 맞게 배치하여 concat한 것에 대해서는 sentence order가 맞다고 예측하게끔 학습을 진행한다.\n","- 2개의 문장을 원래의 순서와 다르게 배치하여 concat한 것에 대해서는 sentence order가 틀리다고 예측하게끔 학습을 진행한다.\n","\n","<br>\n","\n","- NSP(Next Sentence Prediction)와 SOP(Sentence Order Prediction)의 성능 차이는 아래 표에서 확인할 수 있다.\n","  - None의 경우와 NSP의 성능이 비슷한 것을 볼 수 있다.\n","  - SOP의 성능이 가장 좋은 것을 볼 수 있다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17fEvy2Fi6uaTIKFGKqkiHDtkh9Mn2-fG' width=800/>\n","\n","- 출처: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLR’20"]},{"cell_type":"markdown","metadata":{"id":"YmIlq-j9-Eie"},"source":["<br>\n","\n","### 9.3.5 GLUE Results\n","\n","- ALBERT를 포함한 여러 모델들의 다양한 자연어 처리 task를 benchmark 데이터셋으로 포함하는 GLUE에 대한 결과는 아래와 같다.\n","  - 다양한 BERT의 변형 모델들 중에 ALBERT의 성능이 전체적으로 좋은 것을 볼 수 있다.\n","  - cf) ALBERT도 BERT와 같이 모델 사이즈나 파라미터의 갯수의 차이가 있는 여러 버전의 ALBERT 모델이 존재한다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17gCEUbPiR-yEVsKtuJlwCMCdG2RDQE2U' width=800/>\n","\n","- 출처: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLR’20"]},{"cell_type":"markdown","metadata":{"id":"4wLg8OyH-HMJ"},"source":["<br>\n","\n","## 9.4 ELECTRA\n","\n","> ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately"]},{"cell_type":"markdown","metadata":{"id":"QksK7jLU-XqO"},"source":["<br>\n","\n","### 9.4.1 Overview\n","\n","- Learn to distinguish(구별하는) real input tokens from plausible(그럴듯한) but synthetically(종합적으로) generated replacements\n","- Pre-training text encoder as discriminators rather than generators\n","- **Discriminator** is the main networks for pre-training.\n","\n","<br>\n","\n","- 기존의 BERT나 GPT-2와는 조금 다른 형태로 pre-training을 수행한 모델이다.\n","- ELECTRA는 BERT에서 사용한 Masked Language Model 혹은 GPT-2에서 사용한 Standard Language Model에서 한발 더 나아간 모델이다.\n","\n","<br>\n","\n","- Generator 모델이 Masked Language Modeling을 통해 단어를 복원하는 task를 수행한다.\n","  - 주어진 문장에서 일부 단어를 `[Mask]`로 치환한다.\n","  - 이를 Generator를 통해 다시 예측한 단어로 복원한다.\n","  - Generator를 기존의 BERT 모델로 생각할 수 있다.\n","\n","<br>\n","\n","- Discriminator 모델이 모든 단어들에 대해서 특정 단어가 예측에 의해 복원된 단어(`replaced`)인지 아니면 원래부터 존재하는 단어(`original`)인지를 예측하는 task를 수행한다.\n","  - Discriminator의 모델 구조는 기존의 BERT나 GPT-3과 비슷하게 Transformer에서 제안된 Self-Attention block을 쌓은 형태이지만 한 가지 다른 점은 각 단어별로 예측을 수행하는 binary classification layer를 갖는다는 것이다.\n","\n","<br>\n","\n","- Generator와 Discriminator 2가지 모델이 서로 적대적 관계 훈련(Adversarial Learning)이 수행된다.\n","  - Adversarial Learning을 수행하는 또 다른 모델로는 \"Generative Adversarial Network, GAN\"이 있다.\n","- ELECTRA는 GAN 모델의 아이디어에서 착안해서 자연어 처리에서의 pre-training 모델을 제안한 것이다.\n","\n","<br>\n","\n","- 위 과정을 반복적으로 수행함으로서 Discriminator를 점점 더 고도화시킬 수 있다.\n","- 최종적으로 이러한 방식으로 학습을 진행한 후에는 pre-trained된 모델로 사용할 수 있는 부분이 Generator와 Discriminator가 존재하게 된다.\n","- 이 2개의 모델 중 Generator를 사용하는 것이 아닌 Discriminator 모델을 실제 다양한 down-stream task에 fine-tuning하여 사용하게 된다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17ie39Lkq-gmXzr4cOAuo5DiMYP3_vwd_' width=800/>\n","\n","- 출처: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR’20"]},{"cell_type":"markdown","metadata":{"id":"t974sRFA-d49"},"source":["<br>\n","\n","### 9.4.2 Replaced token detection vs Masked language model\n","\n","- <font color='red'>Replaced token detection pre-training</font> vs <font color='skyblue'>masked language model pre-training</font>\n","  - Outperforms MLM-based methods such as BERT given the same model size, data, and compute\n","\n","<br>\n","\n","- ELECTRA는 Replaced token detection을 통해 학습됐고, BERT는 Masked Language Modeling을 통해 학습됐다.\n","- 아래의 왼쪽 그림과 같이 pre-training을 하는데 필요로 하는 계산량을 기준으로 성능을 비교해보자.\n","- 학습을 많이 할수록(i.e., 계산량이 증가할수록) ELECTRA의 GLUE benchmark task에 대한 성능이 점점 더 올라가는 것을 볼수 있다.\n","- ELECTRA의 경우 같은 pre-training 계산량에 대해서 BERT보다 좋은 성능을 나타내는 것을 볼 수 있다.\n","\n","<br>\n","\n","- ELECTRA는 ALBERT와 더불어 많은 down-stream task에 대해서 많이 활용되고 있다.\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17tE8pelOBdF44Jlxsyvo6Pzf4ASdCXOU' width=800/>\n","\n","- 출처: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR’20"]},{"cell_type":"markdown","metadata":{"id":"w62iYrU4_91f"},"source":["<br>\n","\n","## 9.5 Light-weight Models\n","\n","- pre-trained된 모델을 다양한 방식으로 고도화하는 연구들이 계속해서 진행되고 있다.\n","- 그 중에 한 방향으로 모델의 경량화(Light-weight Model)라는 부분이 있다.\n","- 기존의 BERT, GPT-2, GPT-3, ELECTRA 등의 모델들이 Self-Attention block을 점점 더 많이 쌓음으로서 좋은 성능을 냈지만, 이러한 모델을 pre-training 하는 데 많은 GPU resource와 학습 시간 및 계산량이 필요했다.\n","- 그렇기 때문에 이러한 모델들이 다양한 연구나 현업에 활용하는 데 걸림돌이 되었다.\n","- 이러한 부분을 적은 layer 및 파라미터 수를 갖는 경량화된 모델로 발전 및 확장시키는 형태의 연구가 진행되고 있다.\n","- 경량화 모델 연구의 추세는 기존의 큰 사이즈의 모델들이 가지는 성능을 최대한 유지하면서도 모델의 크기와 계산 속도를 빠르게 하는 것에 초점이 맞추어져 있다.\n","- 경량화된 모델은 클라우드 서버나 고성능의 GPU resource를 사용하지 않고서도 휴대폰과 같은 소형 device에서도 load하여 계산을 수행할 때 사용된다.\n","\n","<br>\n","\n","- 모델을 경량화하는 다양한 방식이 존재하는데 아래의 Distill 방식을 사용한 2개의 모델에 대해 소개한다."]},{"cell_type":"markdown","metadata":{"id":"WZJOVEoWB2L-"},"source":["<br>\n","\n","### 9.5.1 DistillBERT\n","\n","> DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, NeurlPS 2019 Workshop\n","\n","<br>\n","\n","- A triple loss, which is a distillation(증류) loss over the soft target probabilities of the teacher model leveraging the full teacher distribution\n","\n","<br>\n","\n","- DIstillBERT는 Transformer의 구현체를 다양한 라이브러리의 형태로 편하게 사용할 수 있도록 제공해주는 HuggingFace라는 회사에서 발표한 논문이다.\n","\n","<br>\n","\n","- 이 논문에는 teacher model과 student model이 존재한다.\n","- teacher model은 student model을 가르치는 역할을 수행하고 student model은 teacher model에 비해서는 layer의 갯수나 파라미터의 갯수가 더 작은 경량화된 모델로 설계된다.\n","- 큰 사이즈의 teacher model이 내는 여러 output과 패턴을 student model이 잘 모사할 수 있도록 학습을 진행한다.\n","\n","<br>\n","\n","- teacher model이 주어진 문장에서 masked language modeling을 수행한 다음 softmax를 통과하여 vocabulary 크기 만큼의 차원을 갖는 확률분포를 얻을 것이다.\n","- student model도 마찬가지로 teacher model과 갖은 과정을 통해 확률분포를 얻을 것이고, 이 **확률분포에 대한 ground truth로서 teacher model의 확률분포를 사용**하게 된다.\n","- 이렇게 함으로서 student model이 teacher model이 하는 행동 및 예측 결과를 모사할 수 있도록 학습이 진행되게 된다.\n","\n","<br>\n","\n","- DistillBERT는 Knowledge Distillation라는 개념을 BERT에 적용하여 모델을 경량화시킨 모델이 된다."]},{"cell_type":"markdown","metadata":{"id":"qubH_PndCJFe"},"source":["<br>\n","\n","### 9.5.2 TinyBERT\n","\n","> TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLP 2020\n","\n","<br>\n","\n","- Two-stage learning framework, which performs Transformer distillation at both the pre-training and task-specific learning stages\n","\n","<br>\n","\n","- TinyBERT도 마찬가지로 Knowledge Distillation을 사용하여 BERT를 경량화한 모델이 된다.\n","  - 여기에도 teacher model과 student model이 존재한다.\n","\n","<br>\n","\n","- DistillBERT와 같이 teacher model의 확률분포를 student model의 확률분포에 대한 ground truth로 사용한다.\n","- 그와 동시에 teacher model의 **임베딩 layer 및 각 self-attention block이 가지는 attention matrix, 그리고 hidden vector까지도 student model이 닮아가도록 학습을 진행**한다.\n","  - 결국은 파라미터 및 중간 결과물까지 모두 다 닮도록 학습을 진행하는 것이다.\n","\n","<br>\n","\n","- 그렇지만 일반적으로 student model은 hidden state vector의 차원 수가 teacher model의 hideen state vector의 차원 수보다 작기 때문에 두 벡터의 차원수 차이 때문에 loss를 적용하기 어려울 수 있다.\n","- 그래서 이 논문에서는 차원이 서로 다른 hidden state vector가 최대한 유사해지도록 하는 loss를 적용하기 위해 차원이 더 큰 벡터를 차원이 작은 벡터로 변환하는 FC Layer를 추가하여 이 부분도 학습이 진행되도록 했다."]},{"cell_type":"markdown","metadata":{"id":"3tCH-HxgCYnk"},"source":["<br>\n","\n","## 9.6 Fusing Knowledge Graph into Language Model\n","\n","- 또 다른 최신 연구 흐름으로 기존의 pre-training 모델과 지식 그래프라고 부르는 외부적인 정보를 잘 결합하는 형태가 존재한다.\n","- BERT 모델이 2018년도에 등장한 이후에 해당 모델이 언어적 특성을 잘 이해하고 있는지 아닌지를 분석하는 연구들이 많이 진행됐다.\n","- 그 결과로 BERT는 주어진 문장에 대해 문맥을 잘 파악하고 단어들간의 유사도는 잘 파악하지만 **주어진 문장에 포함되어 있지 않은 추가적인 정보가 필요한 경우에는 해당 정보를 효과적으로 활용하지 못하는 모습을 보여줬다.**\n","\n","<br>\n","\n","- 예를 들어 \"꽃을 심기 위해 땅을 팠다\"라는 문장과 \"집을 짓기 위해 땅을 팠다\"라는 문장이 주어졌다고 하자.\n","- BERT는 각 문장에 대한 \"어떤 도구를 이용하여 땅을 팠는가?\"의 질문에 대한 답을 제대로 생성하지 못한다.\n","- 하지만 사람은 각 문장에 대해 \"모종삽\"과 \"포크레인\"이라는 답을 할 수 있다.\n","- 이렇게 사람이 대답을 할 수 있는 이유는 주어진 문장에서 담고 있는 정보 뿐만 아니라 사람이 갖고 있는 외부 지식(상식)을 활용할 수 있기 때문이다.\n","\n","<br>\n","\n","- 이러한 외부 정보 및 지식들을 Knowledge Graph로서 잘 정의하고 어떻게 하면 기존의 BERT와 같은 Language Model을 통해 pre-training한 모델들과 잘 결합하여 외부지식이 필요한 경우에도 다양한 task를 수행할 수 있을까와 관련된 연구들이 진행되고 있다."]},{"cell_type":"markdown","metadata":{"id":"Cl8pvwMiCh5m"},"source":["<br>\n","\n","### 9.6.1 ERNIE\n","\n","> ERNIE: Enhanced Language Representation with Informative Entities (ACL 2019)\n","\n","<br>\n","\n","- Informative entities in a knowledge graph enhance language representation\n","- Information fusion layer takes the concatenation of the token embedding and entity embedding"]},{"cell_type":"markdown","metadata":{"id":"cgUQvU7qCzzp"},"source":["<br>\n","\n","### 9.6.2 KagNET\n","\n","> KagNET: Knowledge-Aware Graph Networks for Commonsense Reasoning (EMNLP\n","2019)\n","\n","<br>\n","\n","- A knowledge-aware reasoning framework for learning to answer commonsense(상식적인) questions\n","- For each pair of question and answer candidate, it retrieves(검색하다) a sub-graph from an external knowledge graph to capture relevant knowledge"]},{"cell_type":"code","metadata":{"id":"J4hKo12ODOim"},"source":[""],"execution_count":null,"outputs":[]}]}