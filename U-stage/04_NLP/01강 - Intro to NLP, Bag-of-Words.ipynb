{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01강 - Intro to NLP, Bag-of-Words.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMo3N1Dc1lVbo/mGqKkJPbT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"h0JWbZuJRxDW"},"source":["# 1. Intro to NLP, Bag-of-Words"]},{"cell_type":"markdown","metadata":{"id":"3QehawhOR0_9"},"source":["## 강의 소개\n","\n","- 자연어 처리의 첫 시간으로 NLP에 대해 짧게 소개하고 자연어를 처리하는 가장 간단한 모델 중 하나인 Bag-of-Words를 소개합니다.\n","- Bag-of-Words는 단어의 표현에 있어서 one-hot-encoding을 이용하며, 단어의 등장 순서를 고려하지 않는 아주 간단한 방법 중 하나입니다. 간단한 모델이지만 많은 자연어 처리 task에서 효과적으로 동작하는 알고리즘 중 하나입니다.\n","- 그리고, 이 Bag-of-Words를 이용해 문서를 분류하는 Naive Bayes Classifier에 대해서 설명합니다.\n","- 이번 강의에서는 단어를 벡터로 표현하는 방법, 문서를 벡터로 표현하는 방법에 대해 고민해보면서 강의를 들어주시면 감사하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"LvozBNb5R8fN"},"source":["<br>\n","\n","## 1.1 Intro to Natural Language Processing(NLP)\n","\n","- Academic Disciplines related to NLP\n","- Trends of NLP"]},{"cell_type":"markdown","metadata":{"id":"gAyDxy92SHvt"},"source":["<br>\n","\n","### 1.1.1 Goal of This Course\n","\n","- Natural language processing (NLP), which aims at properly understanding and generating human languages, emerges as a crucial(결정적인) application of artificial intelligence, with the advancements of deep neural networks.\n","- This course will cover various deep learning approaches as well as their applications such as language modeling, machine translation, question answering, document classification, and dialog systems."]},{"cell_type":"markdown","metadata":{"id":"1cgy12NxSUoN"},"source":["<br>\n","\n","### 1.1.2 Academic Disciplines related to NLP"]},{"cell_type":"markdown","metadata":{"id":"1Db30mPtSo_F"},"source":["#### 1.1.2.1 Natural language processing\n","\n","> *major conferences: ACL, EMNLP, NAACL*\n","\n","- Includes state-of-the-art deep learning-based models and tasks\n","- Low-level parsing\n","  - Tokenization : 문장을 단어 단위로 쪼개기\n","  - stemming : 단어의 원형 찾기\n","- Word and phrase level\n","  - Named entity recognition(NER) : 고유명사 인식 task\n","  - part-of-speech(POS) tagging : 단어의 문장 내에서의 품사, 성분(주어, 목적어, 본동사 등)이 무엇인 지 알아내는 task\n","  - noun-phrase chunking\n","  - dependency parsing\n","  - coreference resolution\n","- Sentence level\n","  - Sentiment analysis : 감성 분석 (긍부정)\n","  - machine translation : 기계 번역\n","- Multi-sentence and paragraph(문단) level\n","  - Entailment prediction : 두 문장 사이의 논리적인 내포(?) 및 모순관계 예측\n","  - question answering : 질의 응답\n","  - dialog systems : 챗봇과 같은 대화 시스템\n","  - summarization : 문서 요약"]},{"cell_type":"markdown","metadata":{"id":"qWPuObfCSl8n"},"source":["<br>\n","\n","#### 1.1.2.2 Text mining\n","\n","> *major conferences: KDD, The WebConf (formerly, WWW), WSDM, \n","CIKM, ICWSM)*\n","\n","- Extract useful information and insights from text and document data\n","  - e.g., analyzing the trends of AI-related keywords from massive news data \n","- Document clustering (e.g., topic modeling)\n","  - e.g., clustering news data and grouping into different subjects\n","- Highly related to computational social science\n","  - e.g., analyzing the evolution(진화) of people’s political(정치적) tendency based on social media data"]},{"cell_type":"markdown","metadata":{"id":"rj09ccxQTBa3"},"source":["<br>\n","\n","#### 1.1.2.3 Information retrieval (정보 검색)\n","\n","> *major conferences: SIGIR, WSDM, CIKM, RecSys*\n","\n","- Highly related to computational social science\n","  - This area is not actively studied now\n","  - It has evolved(진화했다) into a **recommendation system**, which is still an active area of research"]},{"cell_type":"markdown","metadata":{"id":"TaqPt19TTLrM"},"source":["<br>\n","\n","### 1.1.3 Trends of NLP\n","\n","- Text data can basically be viewed as a sequence of words, and <font color='red'>each word can be represented as a vector</font> through a technique such as Word2Vec or GloVe.\n","  - word embedding\n","- <font color='red'>RNN-family models</font> (LSTMs and GRUs), which take the sequence of these vectors of words as input, are the main architecture of NLP tasks.\n","- Overall performance of NLP tasks has been improved since <font color='red'>attention modules and Transformer models</font>, which replaced RNNs with self-attention, have been introduced a few years ago.\n","- As is the case for Transformer models, most of the advanced NLP models have been originally developed for improving machine translation tasks."]},{"cell_type":"markdown","metadata":{"id":"BRbmdxsoTXu1"},"source":["- In the early days, <font color='red'>customized models for different NLP tasks</font> had developed separately.\n","- Since Transformer was introduced, huge models were released by stacking its basic module, self-attention, and these models are trained with large-sized datasets through language modeling tasks, one of the <font color='red'>self-supervised training setting that does not require additional labels</font> for a particular task.\n","  - e.g., BERT, GPT-3 …\n","- Afterwards, above models were applied to other tasks through <font color='red'>transfer learning(전이학습)</font>, and they outperformed all other customized models in each task. \n","- Currently, these models has now become essential part in numerous(수많은) <font color='red'>NLP tasks, so NLP research become difficult with limited GPU resources</font>, since they are too large to train."]},{"cell_type":"markdown","metadata":{"id":"omDZYsWhTu-X"},"source":["<br>\n","\n","## 1.2 Bag-of-Words\n","\n","- Bag-of-Words\n","- Bag-of-Words Example (NaiveBayes Classifier)"]},{"cell_type":"markdown","metadata":{"id":"puPEVIpRUBbP"},"source":["<br>\n","\n","### 1.2.1 Bag-of-Words Representation"]},{"cell_type":"markdown","metadata":{"id":"HmcI3MPGUrqH"},"source":["**Step 1. Bag-of-Words\n","Bag-of-Words Example (NaiveBayes Classifier)**\n","\n","- Example sentences\n","  - \"John really really loves this movie\", \"Jane really likes this song\"\n","- Vocabulary\n","  - `{\"John\", \"really\", \"loves\", \"this\", \"movie\", \"Jane\", \"likes\", \"song\"}`"]},{"cell_type":"markdown","metadata":{"id":"4k6jATuUUqh-"},"source":["<br>\n","\n","**Step 2. Encoding unique words to one-hot vectors**\n","\n","- Vocabulary\n","  - `{\"John\", \"really\", \"loves\", \"this\", \"movie\", \"Jane\", \"likes\", \"song\"}`\n","- one-hot vectors\n","  - John : `[1 0 0 0 0 0 0 0]`\n","  - really : `[0 1 0 0 0 0 0 0]`\n","  - loves : `[0 0 1 0 0 0 0 0]`\n","  - this : `[0 0 0 1 0 0 0 0]`\n","  - movie : `[0 0 0 0 1 0 0 0]`\n","  - Jane : `[0 0 0 0 0 1 0 0]`\n","  - likes : `[0 0 0 0 0 0 1 0]`\n","  - song : `[0 0 0 0 0 0 0 1]`"]},{"cell_type":"markdown","metadata":{"id":"qqoebqWRVcmC"},"source":["\n","- For any pair of words, the distance is $\\sqrt{2}$\n","- For any pair of words, cosine similarity is 0"]},{"cell_type":"markdown","metadata":{"id":"b_-csj1EVMoJ"},"source":["<br>\n","\n","**Bag-of-Words Vector**\n","\n","- A sentence/document can be represented as the sum of one-hot vectors\n","  - Sentence 1: \"John really really loves this movie\"\n","    - John + really + really + loves + this + movie: `[1 2 1 1 1 0 0 0]`\n","  - Sentence 2: \"Jane really likes this song\"\n","    - Jane + really + likes + this + song: `[0 1 0 1 0 1 1 1]`"]},{"cell_type":"markdown","metadata":{"id":"GaEiHOToVoCM"},"source":["<br>\n","\n","### 1.2.2 NaiveBayes Classifier for Document Classification"]},{"cell_type":"markdown","metadata":{"id":"eZ0X7pNSVzxP"},"source":["**Bag-of-Words for Document Classification**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1L0wo5eMkxVIh0UWY6JcRD18u7xnzidrK' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"zsC0XEq0VyBG"},"source":["<br>\n","\n","**Bayes' Rule Applied to Documents and Classes**\n","\n","- d개의 문서를 c개의 클래스로 분류할 수 있다고 하자.\n","- For a document **<font color='red'>d</font>** and a class **<font color='red'>c</font>**\n","\n","$$\n","\\begin{aligned}\n","c_{M A P} &=\\underset{c \\in C}{\\operatorname{argmax}} P(c \\mid d)\n","\\qquad \\quad \\text{MAP is “maximum a posteriori” = most likely class}\n","\\\\\n","&=\\underset{c \\in C}{\\operatorname{argmax}} \\frac{P(d \\mid c) P(c)}{P(d)} \n","\\qquad \\quad \\text{Bayes Rule}\n","\\\\\n","&=\\underset{c \\in C}{\\operatorname{argmax}} P(d \\mid c) P(c)\n","\\qquad \\quad \\text{Dropping the denominator}\n","\\end{aligned}\n","$$\n","\n","- $P(d)$ : 특정 문서가 뽑힐 확률, 여기선 문서가 고정되어 있기 때문에 상수값으로 볼 수 있다."]},{"cell_type":"markdown","metadata":{"id":"2Hvwmty8Wjg1"},"source":["- For a document **<font color='red'>d</font>**, which consists of a sequence of words **<font color='red'>w</font>**, and a class **<font color='red'>c</font>**\n","- The probability of a document can be represented by multiplying the \n","probability of each word appearing\n","  - 각 단어들이 등장할 확률이 c가 고정되어 있는 경우 서로 독립이라고 가정할 수 있는 경우 문서가 등장할 확률을 해당 문서 안에 있는 단어들이 등장할 확률의 곱으로 표현할 수 있다.\n","- $P(d \\mid c) \\mathrm{P}(c)=P\\left(w_{1}, w_{2}, \\ldots, w_{n} \\mid c\\right) \\mathrm{P}(c) \\rightarrow \\mathrm{P}(c) \\prod_{w_{i} \\in W} P\\left(\\mathrm{w}_{i} \\mid c\\right)$\n","  - by conditional independence assumption\n","\n","- $P(d \\mid c)$ : c가 고정되었을 때 문서 d가 나타날 확률"]},{"cell_type":"markdown","metadata":{"id":"oHSRjt5rXIWt"},"source":["<br>\n","\n","**Example**\n","\n","- For a document **<font color='red'>d</font>**, which consists of a sequence of words **<font color='red'>w</font>**, and a class **<font color='red'>c</font>**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1bfi6MR8LBDjU91E9qv2mluWisSXbDYhF' width=800/>\n","\n","- $P\\left(c_{c v}\\right)=\\frac{2}{4}=\\frac{1}{2}$\n","- $P\\left(c_{N L P}\\right)=\\frac{2}{4}=\\frac{1}{2}$"]},{"cell_type":"markdown","metadata":{"id":"wcQhLMqlXjrF"},"source":["<br>\n","\n","**Example**\n","\n","- For each word $w_i$ we can calculate conditional probability for class $c$\n","  - $P\\left(w_{k} \\mid c_{i}\\right)=\\frac{n_{k}}{n}$, where $n_k$ is occurrences of $w_k$ in documents of topic $c_i$\n","  - 특정 클래스에 해당하는 문서의 단어들 중 해당 단어의 등장 횟수를 확률값으로 사용\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ziE1bB2mRwIPszgwQSGqifgKgWn22bTv' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"9tsC1uIBakSs"},"source":["<br>\n","\n","**For a test document $d_5$ = \"Classification task uses transformer\"**\n","\n","-  We calculate the conditional probability of the document for each class\n","- We can choose a class that has the highest probability for the document\n","- 각각의 클래스의 확률값과 특정 문서 내의 모든 단어들의 확률값의 곱을 곱하여 해당 문서가 특정 클래스에 속할 확률을 구할 수 있다.\n","\n","<br>\n","\n","- $P\\left(\\mathrm{c}_{\\mathrm{CV}} \\mid d_{5}\\right)=\\mathrm{P}\\left(\\mathrm{c}_{\\mathrm{CV}}\\right) \\prod_{\\mathrm{w} \\in \\mathrm{W}} P\\left(\\mathrm{w} \\mid \\mathrm{c}_{\\mathrm{CV}}\\right)=\\frac{1}{2} \\times \\frac{1}{10} \\times \\frac{1}{10} \\times \\frac{1}{10} \\times \\frac{1}{10}=0.00005$\n","- $P\\left(\\mathrm{c}_{\\mathrm{NLP}} \\mid d_{5}\\right)=\\mathrm{P}\\left(\\mathrm{c}_{\\mathrm{NLP}}\\right) \\prod_{\\mathrm{w} \\in \\mathrm{W}} P\\left(\\mathrm{w} \\mid \\mathrm{c}_{\\mathrm{NLP}}\\right)=\\frac{1}{2} \\times \\frac{1}{13} \\times \\frac{2}{13} \\times \\frac{1}{13} \\times \\frac{1}{13} \\approx 0.00003$\n","\n","<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ziE1bB2mRwIPszgwQSGqifgKgWn22bTv' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"WfBRC4Gaa6Ec"},"source":["<br>\n","\n","## 1.3 실습: NavieBayes Classifier\n","\n","1. 주어진 데이터를 전처리합니다.\n","2. NaiveBayes 분류기 모델을 구현하고 학습 데이터로 이를 학습시킵니다.\n","3. 간단한 test case로 결과를 확인합니다."]},{"cell_type":"markdown","metadata":{"id":"6PNbJiS9uTpi"},"source":["<br>\n","\n","### 1.3.1 필요 패키지 import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bCu_pe5uWYC","executionInfo":{"status":"ok","timestamp":1630908564650,"user_tz":-540,"elapsed":6784,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"1ffc710d-15d1-439c-a028-420086d59823"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"]}]},{"cell_type":"code","metadata":{"id":"TVzr_Mx3uXby"},"source":["from tqdm import tqdm\n","\n","# 다양한 한국어 형태소 분석기가 클래스로 구현되어 있음\n","from konlpy import tag \n","\n","from collections import defaultdict\n","\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"srhEoab0uhCK"},"source":["<br>\n","\n","### 1.3.2 학습 및 테스트 데이터 전처리\n","\n","- Sample 데이터를 확인합니다.\n","- 긍정(1), 부정(0) 2가지 class로 구성되어 있습니다."]},{"cell_type":"code","metadata":{"id":"zqRb46dEuwZZ"},"source":["train_data = [\n","  \"정말 맛있습니다. 추천합니다.\",\n","  \"기대했던 것보단 별로였네요.\",\n","  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n","  \"완전 최고입니다! 재방문 의사 있습니다.\",\n","  \"음식도 서비스도 다 만족스러웠습니다.\",\n","  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n","  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n","  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n","  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n","  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"\n","]\n","train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n","\n","test_data = [\n","  \"정말 좋았습니다. 또 가고 싶네요.\",\n","  \"별로였습니다. 되도록 가지 마세요.\",\n","  \"다른 분들께도 추천드릴 수 있을 만큼 만족했습니다.\",\n","  \"서비스가 좀 더 개선되었으면 좋겠습니다. 기분이 좀 나빴습니다.\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v90D82zluwqk"},"source":["<br>\n","\n","- KoNLPy 패키지에서 제공하는 Twitter(Okt) tokenizer를 사용하여 tokenization합니다."]},{"cell_type":"code","metadata":{"id":"sRGR15hMuz8Z"},"source":["tokenizer = tag.Okt()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZK7ac3ju2ji"},"source":["def make_tokenized(data):\n","    tokenized = [] # 단어 단위로 나뉜 리뷰 데이터\n","    \n","    for sent in tqdm(data):\n","        tokens = tokenizer.morphs(sent)\n","        tokenized.append(tokens)\n","    \n","    return tokenized"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HU_Jl7-lvCRM","executionInfo":{"status":"ok","timestamp":1630908736925,"user_tz":-540,"elapsed":7449,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"bfde2cbf-04dc-4e38-e45c-7c7a496363ed"},"source":["train_tokenized = make_tokenized(train_data)\n","test_tokenized = make_tokenized(test_data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n","100%|██████████| 4/4 [00:00<00:00, 71.86it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wl1xR0wLvEYh","executionInfo":{"status":"ok","timestamp":1630908739854,"user_tz":-540,"elapsed":536,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"b3c9f065-f187-411a-ac0e-2468c51ef0a5"},"source":["train_tokenized"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['정말', '맛있습니다', '.', '추천', '합니다', '.'],\n"," ['기대했던', '것', '보단', '별로', '였네요', '.'],\n"," ['다',\n","  '좋은데',\n","  '가격',\n","  '이',\n","  '너무',\n","  '비싸서',\n","  '다시',\n","  '가고',\n","  '싶다는',\n","  '생각',\n","  '이',\n","  '안',\n","  '드네',\n","  '요',\n","  '.'],\n"," ['완전', '최고', '입니다', '!', '재', '방문', '의사', '있습니다', '.'],\n"," ['음식', '도', '서비스', '도', '다', '만족스러웠습니다', '.'],\n"," ['위생',\n","  '상태',\n","  '가',\n","  '좀',\n","  '별로',\n","  '였습니다',\n","  '.',\n","  '좀',\n","  '더',\n","  '개선',\n","  '되',\n","  '기를',\n","  '바랍니다',\n","  '.'],\n"," ['맛', '도', '좋았고', '직원', '분들', '서비스', '도', '너무', '친절했습니다', '.'],\n"," ['기념일',\n","  '에',\n","  '방문',\n","  '했는데',\n","  '음식',\n","  '도',\n","  '분위기',\n","  '도',\n","  '서비스',\n","  '도',\n","  '다',\n","  '좋았습니다',\n","  '.'],\n"," ['전반', '적', '으로', '음식', '이', '너무', '짰습니다', '.', '저', '는', '별로', '였네요', '.'],\n"," ['위생', '에', '조금', '더', '신경', '썼으면', '좋겠습니다', '.', '조금', '불쾌했습니다', '.']]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"9j0NyflQvGxd"},"source":["<br>\n","\n","- 학습데이터 기준으로 가장 많이 등장한 단어부터 순서대로 vocab에 추가합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vhXw64nrvLN5","executionInfo":{"status":"ok","timestamp":1630908801743,"user_tz":-540,"elapsed":497,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"895355b4-1457-4588-acaa-dd670fb5f170"},"source":["word_count = defaultdict(int) # Key: 단어, Value: 등장 횟수\n","\n","for tokens in tqdm(train_tokenized):\n","    for token in tokens:\n","        word_count[token] += 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 47880.18it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sXcSz5QvV35","executionInfo":{"status":"ok","timestamp":1630908815040,"user_tz":-540,"elapsed":501,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"d67dc970-eb50-48fd-84a5-2669c9f2d7a2"},"source":["word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n","print(len(word_count))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["66\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsSpP5Hyvv4Z","executionInfo":{"status":"ok","timestamp":1630908923284,"user_tz":-540,"elapsed":469,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"b219ad5d-2bf4-4678-d283-1590fb2c538e"},"source":["word_count[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('.', 14),\n"," ('도', 7),\n"," ('별로', 3),\n"," ('다', 3),\n"," ('이', 3),\n"," ('너무', 3),\n"," ('음식', 3),\n"," ('서비스', 3),\n"," ('였네요', 2),\n"," ('방문', 2)]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXUpdmUkvZJJ","executionInfo":{"status":"ok","timestamp":1630908856290,"user_tz":-540,"elapsed":4,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"fbb4a252-0c0c-444c-e2fb-f025544c0c6f"},"source":["a = defaultdict(int)\n","a['b'] +=1\n","sorted(a)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['b']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1Yxk_Zave0y","executionInfo":{"status":"ok","timestamp":1630908873083,"user_tz":-540,"elapsed":463,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"d385cf00-9fff-49aa-e46b-2ac4e26e2e14"},"source":["w2i = {}  # Key: 단어, Value: 단어의 index\n","for pair in tqdm(word_count):\n","  if pair[0] not in w2i:\n","    w2i[pair[0]] = len(w2i)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 66/66 [00:00<00:00, 263641.97it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6g2bMD3uv3ej","executionInfo":{"status":"ok","timestamp":1630908943234,"user_tz":-540,"elapsed":4,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"8b0061ed-d2a5-4d69-c947-50b217445313"},"source":["len(w2i)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["66"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1h1NM-6NvnVa","executionInfo":{"status":"ok","timestamp":1630908890857,"user_tz":-540,"elapsed":657,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"5d57b4e4-2ed6-471d-a699-e7f89dd8337b"},"source":["w2i"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'!': 35,\n"," '.': 0,\n"," '가': 41,\n"," '가격': 23,\n"," '가고': 26,\n"," '개선': 43,\n"," '것': 20,\n"," '기념일': 52,\n"," '기대했던': 19,\n"," '기를': 45,\n"," '너무': 5,\n"," '는': 61,\n"," '다': 3,\n"," '다시': 25,\n"," '더': 12,\n"," '도': 1,\n"," '되': 44,\n"," '드네': 30,\n"," '만족스러웠습니다': 39,\n"," '맛': 47,\n"," '맛있습니다': 16,\n"," '바랍니다': 46,\n"," '방문': 9,\n"," '별로': 2,\n"," '보단': 21,\n"," '분들': 50,\n"," '분위기': 54,\n"," '불쾌했습니다': 65,\n"," '비싸서': 24,\n"," '상태': 40,\n"," '생각': 28,\n"," '서비스': 7,\n"," '신경': 62,\n"," '싶다는': 27,\n"," '썼으면': 63,\n"," '안': 29,\n"," '에': 13,\n"," '였네요': 8,\n"," '였습니다': 42,\n"," '완전': 32,\n"," '요': 31,\n"," '위생': 10,\n"," '으로': 58,\n"," '음식': 6,\n"," '의사': 37,\n"," '이': 4,\n"," '입니다': 34,\n"," '있습니다': 38,\n"," '재': 36,\n"," '저': 60,\n"," '적': 57,\n"," '전반': 56,\n"," '정말': 15,\n"," '조금': 14,\n"," '좀': 11,\n"," '좋겠습니다': 64,\n"," '좋았고': 48,\n"," '좋았습니다': 55,\n"," '좋은데': 22,\n"," '직원': 49,\n"," '짰습니다': 59,\n"," '최고': 33,\n"," '추천': 17,\n"," '친절했습니다': 51,\n"," '합니다': 18,\n"," '했는데': 53}"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"25PYC3tUvrlC"},"source":["<br>\n","\n","### 1.3.3 모델 Class 구현\n","\n","NaiveBayes Classifier 모델 클래스를 구현합니다.\n","\n","- `self.k`: Smoothing을 위한 상수.\n","- `self.w2i`: 사전에 구한 vocab.\n","- `self.priors`: 각 class의 prior 확률.\n","- `self.likelihoods`: 각 token의 특정 class 조건 내에서의 likelihood.\n"]},{"cell_type":"code","metadata":{"id":"HCB797brwASJ"},"source":["class NaiveBayesClassifier():\n","    def __init__(self, w2i, k=0.1):\n","        self.k = k\n","        self.w2i = w2i\n","        self.priors = {}\n","        self.likelihoods = {}\n","\n","    def train(self, train_tokenized, train_labels):\n","        self.set_priors(train_labels) # priors 계산\n","        self.set_likelihoods(train_tokenized, train_labels) # Likelihoods 계산\n","\n","    def inference(self, tokens):\n","        log_prob0 = 0.0\n","        log_prob1 = 0.0\n","\n","        for token in tokens:\n","            if token in self.likelihoods:  # 학습 당시 추가했던 단어에 대해서만 고려.\n","                log_prob0 += math.log(self.likelihoods[token][0])\n","                log_prob1 += math.log(self.likelihoods[token][1])\n","\n","        # 마지막에 prior를 고려.\n","        log_prob0 += math.log(self.priors[0])\n","        log_prob1 += math.log(self.priors[1])\n","\n","        if log_prob0 >= log_prob1:\n","            return 0\n","        else:\n","            return 1\n","\n","    def set_priors(self, train_labels):\n","        class_counts = defaultdict(int)\n","        for label in tqdm(train_labels):\n","            class_counts[label] += 1\n","\n","        for label, count in class_counts.items():\n","            self.priors[label] = class_counts[label] / len(train_labels)\n","\n","    def set_likelihoods(self, train_tokenized, train_labels):\n","        token_dists = {}  # 각 단어의 특정 class 조건 하에서의 등장 횟수.\n","        class_counts = defaultdict(int)  # 특정 class에서 등장한 모든 단어의 등장 횟수.   \n","\n","        for i, label in enumerate(tqdm(train_labels)):\n","            count = 0\n","            for token in train_tokenized[i]:\n","                if token in self.w2i: # 학습 데이터로 구축한 vocab에 있는 token만 고려\n","                    if token not in token_dists:\n","                        token_dists[token] = {0:0, 1:0}\n","                    token_dists[token][label] += 1\n","                    count += 1\n","            class_counts[label] += count\n","\n","        for token, dist in tqdm(token_dists.items()):\n","            if token not in self.likelihoods:\n","                self.likelihoods[token] = {\n","                    0:(token_dists[token][0] + self.k) / (class_counts[0] + len(self.w2i)*self.k),\n","                    1:(token_dists[token][1] + self.k) / (class_counts[1] + len(self.w2i)*self.k),\n","                }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5UCkMzWxroT"},"source":["<br>\n","\n","### 1.3.4 모델 학습\n","\n","- 모델 객체를 만들고 학습 데이터로 학습시킵니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InJm83xkx2EB","executionInfo":{"status":"ok","timestamp":1630909536833,"user_tz":-540,"elapsed":13,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"cc0e80ed-c808-4cdc-de75-68044a27bd80"},"source":["classifier = NaiveBayesClassifier(w2i)\n","classifier.train(train_tokenized, train_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 8259.76it/s]\n","100%|██████████| 10/10 [00:00<00:00, 23353.59it/s]\n","100%|██████████| 66/66 [00:00<00:00, 143432.16it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"uLiUiqxlx3ar"},"source":["<br>\n","\n","### 1.3.5 테스트\n","\n","- Test sample에 대한 결과는 다음과 같습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJ674uM1yOua","executionInfo":{"status":"ok","timestamp":1630909563011,"user_tz":-540,"elapsed":6,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"b609bc31-9703-4b28-8b51-4522a10b3ed5"},"source":["preds = []\n","for test_tokens in tqdm(test_tokenized):\n","  pred = classifier.inference(test_tokens)\n","  preds.append(pred)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 18787.48it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_tkuvyByPw5","executionInfo":{"status":"ok","timestamp":1630909580341,"user_tz":-540,"elapsed":536,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"54dc10d7-0b01-4f5d-bffc-9afec292dc1d"},"source":["preds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 0, 1, 0]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"6IcKIvCcyT-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630909591616,"user_tz":-540,"elapsed":470,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"582fef05-97e5-40f9-bc9b-5ba7ad68a114"},"source":["test_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['정말 좋았습니다. 또 가고 싶네요.',\n"," '별로였습니다. 되도록 가지 마세요.',\n"," '다른 분들께도 추천드릴 수 있을 만큼 만족했습니다.',\n"," '서비스가 좀 더 개선되었으면 좋겠습니다. 기분이 좀 나빴습니다.']"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"lmO608pkyWv5"},"source":[""],"execution_count":null,"outputs":[]}]}