{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI Math 3~4강 - 경사하강법 (순한맛, 매운맛).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOftVsSwvUzs+GHcVw50txh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MDSNjWKYHDwD"},"source":["# 3. 경사하강법 (Gradient Descent)"]},{"cell_type":"markdown","metadata":{"id":"4XraCoQqHJ5A"},"source":["## 3.1 미분 (differentiation)"]},{"cell_type":"markdown","metadata":{"id":"5FZidSdwE2VB"},"source":["### 3.1.1 미분이란?\n","\n","- 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구\n","- 최적화에서 제일 많이 사용하는 기법\n","- 기울기 = 변화율\n","- 미분은 변화율의 극한(limit)으로 정의한다. (미분 = 변화율의 극한)\n","\n","$\n","\\quad\n","f'(x) = \\underset{h \\rightarrow 0}{lim} \\, \\frac{f(x+h) - f(x)}{h}\n","$"]},{"cell_type":"markdown","metadata":{"id":"SKzWJKZlHrmn"},"source":["- `sympy.diff` 를 가지고 미분을 컴퓨터로 계산할 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"pnjQTvxCEQGC","executionInfo":{"status":"ok","timestamp":1627884395164,"user_tz":-540,"elapsed":13,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"6dae7c30-2264-4c6a-fb04-7a7c61acbd1d"},"source":["import sympy as sym\n","from sympy.abc import x\n","\n","sym.diff(sym.poly(x**2 + 2*x + 3), x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle \\operatorname{Poly}{\\left( 2 x + 2, x, domain=\\mathbb{Z} \\right)}$","text/plain":["Poly(2*x + 2, x, domain='ZZ')"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"T-6-DZCcEW6T"},"source":["<br>\n","\n","### 3.1.2 그림을 통한 미분 이해\n","\n","- 미분은 함수 $f$ 와 주어진 점 $(x, f(x))$ 에서의 **접선의 기울기**를 구한다.\n","- 미분을 계산하려면 함수의 모양이 매끄러워야(연속) 한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17hGiiEWHNkKu2F-JnO-60BUx4qAt6kT3' width=500/>\n","\n","- 아래 함수에서 $h$를 0으로 보내면 $(x, f(x))$ 에서 접선의 기울기로 수렴한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1bYF5hDCJqHnNsHnZ63r91Bd1a5m53M0N' width=500/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Ulx6YvYgo4r7OByiVwwJUOiIwWwMscQz' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"o2FafFNoFRBj"},"source":["<br>\n","\n","### 3.1.3 미분의 활용\n","\n","- 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는 지, 감소하는 지 알 수 있다.\n","  - 함수값을 증가시키고 싶다 : 미분값을 더함\n","  - 함수값을 감소시키고 싶다 : 미분값을 뺌"]},{"cell_type":"markdown","metadata":{"id":"purtHZpGF_Bv"},"source":["<br>\n","\n","#### 3.1.3.1 함수값 증가 (경사 상승, gradient ascent)\n","\n","- 미분값이 음수인 경우 $x + f'(x) < x$ 는 왼쪽으로 이동하여 함수값이 증가\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WbqNetOOb1bRADiowTRsNbrOSVYKK46H' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"Oet_878RHXEj"},"source":["- 미분값이 양수인 경우 $x + f'(x) > x$ 는 오른쪽으로 이동하여 함수값이 증가\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=171ERmBZ0iD9kiFDnWBQL6iAATqNlam4B' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"6fNdTovcIQAa"},"source":["- **미분값을 더하면 경사상승법(gradient ascent)**이라 하며 함수의 **극대값**의 위치를 구할 때 사용한다.\n","- 목적함수를 **최대화**할 때 사용한다.\n","- 경사 상승 방법은 극값에 도달하면 움직임을 멈춘다.\n","  - 극값에선 미분값이 0 이므로 더 이상 업데이트가 안 된다.\n","  - 그러므로 목적함수 최적화가 자동으로 끝난다."]},{"cell_type":"markdown","metadata":{"id":"fVYWG3ZRG2wa"},"source":["<br>\n","\n","#### 3.1.3.2 함수값 감소 (경사 하강, gradient descent)\n","\n","- 미분값이 음수인 경우 $x - f'(x) > x$ 는 오른쪽으로 이동하여 함수값이 감소\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1retvA2IoVjXpnsAq2Y_1JC6EyH7nK9fP' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"wwlDs7ZwHA3t"},"source":["- 미분값이 양수인 경우 $x - f'(x) < x$ 는 왼쪽으로 이동하여 함수값이 감소\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1YoB_EMbEsfBobF7DzSbxOrCigX-LPiRu' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"KJYntBSUIJ6F"},"source":["- **미분값을 빼면 경사하강법(gradient descent)**이라 하며 함수의 **극소값**의 위치를 구할 때 사용한다.\n","- 목적함수를 **최소화**할 때 사용한다.\n","- 경사 하강 방법은 극값에 도달하면 움직임을 멈춘다.\n","  - 극값에선 미분값이 0 이므로 더 이상 업데이트가 안 된다.\n","  - 그러므로 목적함수 최적화가 자동으로 끝난다."]},{"cell_type":"markdown","metadata":{"id":"ZNbTI15DJIuM"},"source":["<br>\n","\n","### 3.1.4 경사하강법: 알고리즘\n","\n","```python\n","# Input: gradient(미분을 계산하는 함수), init(시작점), lr(학습률), eps(알고리즘 종료조건)\n","# Output: var\n","\n","var = init\n","grad = gradient(var)\n","while (abs(grad) > eps): # 컴퓨터로 계산할 때 미분이 정확히 0이 되는 것은 불가능하므로 eps 보다 작을 때 종료하는 조건이 필요\n","    var = var - lr * grad # lr은 학습률로서 미분을 통해 업데이트하는 속도를 조절한다.\n","    grad = gradient(var)\n","```"]},{"cell_type":"markdown","metadata":{"id":"NIOG2tOCKOel"},"source":["<br>\n","\n","### 3.1.5 경사하강법 알고리즘 예시\n","\n","- 함수가 $f(x) = x^2 + 2x + 3$ 일 때 경사하강법으로 최소점을 찾는 코드"]},{"cell_type":"code","metadata":{"id":"KUgemAovIjky","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627225099790,"user_tz":-540,"elapsed":1989,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"508194f9-1148-4be6-92c4-fd03b4242bf6"},"source":["import sympy as sym\n","from sympy.abc import x\n","import numpy as np\n","\n","def func(val):\n","    fun = sym.poly(x**2 + 2*x + 3)\n","    return fun.subs(x, val), fun\n","\n","def func_gradient(fun, val):\n","    _, function = fun(val)\n","    diff = sym.diff(function, x)\n","    return diff.subs(x, val), diff\n","\n","def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):\n","    cnt=0\n","    val = init_point\n","    diff, _ = func_gradient(fun, init_point)\n","    while np.abs(diff) > epsilon:\n","        val = val - lr_rate * diff\n","        diff, _ = func_gradient(fun, val)\n","        cnt += 1\n","\n","    print(f\"함수: {fun(val)[1]}, 연산횟수: {cnt}, 최소점: ({val}, {fun(val)[0]})\")\n","\n","gradient_descent(fun=func, init_point=np.random.uniform(-2, 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["함수: Poly(x**2 + 2*x + 3, x, domain='ZZ'), 연산횟수: 579, 최소점: (-0.999995040756980, 2.00000000002459)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KL_cVEjiLIRE"},"source":["<br>\n","\n","## 3.2 벡터의 미분\n","\n","- 미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구이다. (최적화에서 제일 많이 사용하는 기법)"]},{"cell_type":"markdown","metadata":{"id":"BD9M185OOfbt"},"source":["<br>\n","\n","### 3.2.1 변수가 벡터인 경우의 미분 (편미분)\n","\n","- 벡터가 입력인 다변수 함수의 경우 **편미분(partial differentiation)**을 사용한다.\n","\n","$\n","\\qquad\n","\\partial_{x_i} f(\\mathbb{x}) = \n","\\underset{h \\rightarrow 0}{lim} \\, \\frac{f(\\mathbb{x}+h \\mathbb{e}_i) - f(\\mathbb{x})}{h}\n","$\n","\n","- $\\mathbb{e}_i$ : $i$ 번째 값만 1이고 나머지는 0인 단위 벡터\n","- $i$ 번째 방향에서만의 변화율을 구할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"oBIh210tMC0P"},"source":["$\n","\\quad\n","f(x, y) = x^2 + 2xy + 3 + cos(x + 2y)\n","$\n","\n","$\n","\\quad\n","\\partial_x f(x, y) = 2x + 2y - sin(x + 2y)\n","$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"Aafq0O7hMTpL","executionInfo":{"status":"ok","timestamp":1627225463909,"user_tz":-540,"elapsed":292,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"562d6cd8-8e13-4a90-da16-65c89e390bfa"},"source":["import sympy as sym\n","from sympy.abc import x, y\n","\n","sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sympy/polys/polytools.py:79: SymPyDeprecationWarning: \n","\n","Mixing Poly with non-polynomial expressions in binary operations has\n","been deprecated since SymPy 1.6. Use the as_expr or as_poly method to\n","convert types instead. See https://github.com/sympy/sympy/issues/18613\n","for more info.\n","\n","  useinstead=\"the as_expr or as_poly method to convert types\").warn()\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle 2 x + 2 y - \\sin{\\left(x + 2 y \\right)}$","text/plain":["2*x + 2*y - sin(x + 2*y)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"CjRuAqSZMcus"},"source":["<br>\n","\n","- 각 변수별로 편미분을 계산한 **그레디언트(gradient) 벡터**를 이용하여 경사하강/경사상승법에 사용할 수 있다.\n","\n","$\n","\\qquad\n","\\nabla f = \\left( \\partial_{x_1} f, \\, \\partial_{x_2} f, \\, \\cdots, \\partial_{x_d} f \\right) \\quad \\leftarrow \\quad \\text{gradient vector}\n","$\n","\n","$\n","\\qquad\n","\\partial_{x_i} f(\\mathbb{x}) = \n","\\underset{h \\rightarrow 0}{lim} \\, \\frac{f(\\mathbb{x}+h \\mathbb{e}_i) - f(\\mathbb{x})}{h}\n","$\n","\n","- 앞서 사용한 미분값인 $f'(x)$ 대신 벡터 $\\nabla f$를 사용하여 변수 $\\mathbb{x} = (x_1, \\dots, x_d)$ 를 동시에 업데이트 가능하다."]},{"cell_type":"markdown","metadata":{"id":"X5L36p4xNvbc"},"source":["<br>\n","\n","### 3.2.2 그레디언트 벡터\n","\n","- 각 점 $(x, y, z)$ 공간에서 $f(x,y)$ 표면을 따라 $- \\nabla f$ 벡터를 그리면 아래와 같이 $f(x,y)$ 의 극소점을 향하는 화살표들이 그려진다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1cTd5ox-B82fYhV0v-5hvn3-zSOWObRTh' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"YiXwQ16fO1sM"},"source":["- 그레디언트를 이해햐기 위해 함수 $f(x,y)$ 의 등고선(contour)을 그려 해석해보자.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1LZAgP-tiqo4XddTyjQ49cacjKWy0EQlC' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"ZH0ZZY8KPcC0"},"source":["- 그레디언트 벡터 $\\nabla f(x,y)$ 는 각 점 $(x,y)$ 에서 **가장 빨리 증가하는 방향**으로 흐르게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=15fKzK4-zBY8aD5WQTmGmZ8tQARMcUqjW' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"1NkAutAAP1iM"},"source":["- $- \\nabla f$ 는 $\\nabla (-f)$ 와 같고, 이는 각 점에서 **가장 빨리 감소하게 되는 방향**과 같다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ipPPUyuvi1tq2Fap01R1cTDt7w2qR5-5' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"IBK3oQjkP-Ij"},"source":["<br>\n","\n","### 3.2.3 경사하강법: 알고리즘\n","\n","```python\n","# Input: gradient(그레디언트 벡터를 계산하는 함수), init(시작점), lr(학습률), eps(알고리즘 종료조건)\n","# Output: var\n","\n","var = init\n","grad = gradient(var)\n","while (norm(grad) > eps): # 벡터는 절대값 대신 노름(norm)을 계산해서 종료조건을 설정한다.\n","    var = var - lr * grad\n","    grad = gradient(var)\n","```"]},{"cell_type":"markdown","metadata":{"id":"kX4c42_HQpu0"},"source":["<br>\n","\n","## 3.3 경사하강법을 통한 선형회귀분석"]},{"cell_type":"markdown","metadata":{"id":"9L-IWK7hkOpb"},"source":["### 3.3.1 선형회귀분석 복습\n","\n","- `np.linalg.pinv` 를 이용하면 데이터를 선형 모델(linear model)로 해석하는 **선형회귀식**을 찾을 수 있다.\n","- 데이터의 개수($n$)가 변수의 개수($m$)보다 많거나 같아야 함\n","- 선형 모델은 무어펜로즈 역행렬을 이용하여 회귀분석이 가능하다."]},{"cell_type":"markdown","metadata":{"id":"UpxwGGJZkibK"},"source":["- 이번에는 무어펜로즈 역행렬을 이용하지 않고 경사하강법을 이용해 적절한 선형 모델을 찾아보자.\n","  - 역행렬을 통해 선형회귀식을 구하는 것은 선형 모델일 때만 가능하다.\n","  - 비선형 모델에서도 선형회귀식을 찾을 수 있도록 경사하강법을 이용한다. (일반화)"]},{"cell_type":"markdown","metadata":{"id":"qPv6O7-fcam0"},"source":["<br>\n","\n","### 3.3.2 경사하강법으로 선형회귀 계수 구하기\n","\n","- 선형회귀의 목적식은 $|| \\mathbf{y} - \\mathbf{X} \\beta ||_2$ 이고(L2노름) **이를 최소화하는 $\\beta$**를 찾아야 하므로 다음과 같은 **그레디언트 벡터**를 구해야 한다.\n","  - 해당 목적식을 $\\beta$ 로 미분\n","  - 주어진 벡터에서 미분값을 뺌\n","\n","$\n","\\qquad\n","\\begin{align*}\n","\\nabla_{\\beta} || \\mathbf{y} - \\mathbf{X} \\beta ||_2 &= \n","\\left(\n","\\partial_{\\beta_1} || \\mathbf{y} - \\mathbf{X} \\beta ||_2 , \\; \\dots, \\;\n","\\partial_{\\beta_d} || \\mathbf{y} - \\mathbf{X} \\beta ||_2\n","\\right)\n","\\end{align*}\n","$\n","\n","<br>\n","\n","- $|| \\mathbf{y} - \\mathbf{X} \\beta ||_2$ 가 아닌 $|| \\mathbf{y} - \\mathbf{X} \\beta ||_2^2$ 를 최소화해도 된다."]},{"cell_type":"markdown","metadata":{"id":"XEJxjl_veFCs"},"source":["- k번째 계수에 해당하는 $beta_k$ 를 가지고 목적식을 편미분하는 식은 아래와 같다.\n","- 여기서 사용되는 L2 노름은 n개의 데이터를 가지고 계산되는 L2 노름이다.\n","- i=1 부터 n까지 더한 값을 1/n 으로 나눠 평균을 구한 후 제곱근을 취해준다.\n","\n","$\n","\\qquad\n","\\partial_{\\beta_k} || \\mathbf{y} - \\mathbf{X} \\beta ||_2 = \n","\\partial_{\\beta_k}\n","\\left\\{\n","\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^d X_{ij} \\beta_j \\right)^2\n","\\right\\}^{1/2}\n","$"]},{"cell_type":"markdown","metadata":{"id":"u8qsxKFgfDvc"},"source":["- 해당 식을 $\\beta_k$ 에 대해 편미분 하면 다음과 같은 식이 된다.\n","\n","$$\n","- \\frac{\\mathbf{X}_{\\cdot k}^T \\left( \\mathbf{y} - \\mathbf{X} \\beta \\right)}{n || \\mathbf{y} - \\mathbf{X} \\beta ||_2}\n","$$\n","\n","- $\\mathbf{X}_{\\cdot k}^T$ : 행렬 $\\mathbf{X}$ 의 $k$ 번 째 열(column) 벡터를 전치시킨 것"]},{"cell_type":"markdown","metadata":{"id":"Hpt6LyEJfs28"},"source":["- 위 방법을 통해 그레디언트 벡터를 다음과 같이 표현할 수 있다.\n","\n","$$\n","\\nabla_{\\beta} || \\mathbf{y} - \\mathbf{X} \\beta ||_2 = \n","\\left(\n","- \\frac{\\mathbf{X}_{\\cdot 1}^T \\left( \\mathbf{y} - \\mathbf{X} \\beta \\right)}{n || \\mathbf{y} - \\mathbf{X} \\beta ||_2}\n",", \\; \\dots, \\;\n","- \\frac{\\mathbf{X}_{\\cdot d}^T \\left( \\mathbf{y} - \\mathbf{X} \\beta \\right)}{n || \\mathbf{y} - \\mathbf{X} \\beta ||_2}\n","\\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"G1aY1sTpgEok"},"source":["- 복잡한 계산이지만 사실 $\\mathbf{X} \\beta$ 를 계수 $\\beta$ 에 대해 미분한 결과인 $\\mathbf{X}^T$ 만 곱해지는 것이다.\n","\n","$$\n","\\nabla_{\\beta} || \\mathbf{y} - \\mathbf{X} \\beta ||_2 = \n","- \\frac{\\mathbf{X}^T \\left( \\mathbf{y} - \\mathbf{X} \\beta \\right)}{n || \\mathbf{y} - \\mathbf{X} \\beta ||_2}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"yrXCj3BRgZRF"},"source":["- 이제 목적식을 최소화하는 $\\beta$ 를 구하는 경사하강법 알고리즘은 다음과 같다.\n","\n","$\n","\\qquad\n","\\begin{align*}\n","\\beta^{(t+1)} &\\; \\leftarrow \\;\n","\\beta^{(t)} - \\lambda \\nabla_\\beta || y - \\mathbf{X} \\beta^{(t)} || \\\\\n","&\\; \\leftarrow \\;\n","\\beta^{(t)} + \\frac{\\lambda}{n} \\frac{\\mathbf{X}^T \\left( \\mathbf{y} - \\mathbf{X} \\beta^{(t)} \\right)}{|| \\mathbf{y} - \\mathbf{X} \\beta^{(t)} ||}\n","\\end{align*}\n","$"]},{"cell_type":"markdown","metadata":{"id":"qKV_Wh1ChCXV"},"source":["<br>\n","\n","- $|| \\mathbf{y} - \\mathbf{X} \\beta ||_2$ 대신 $|| \\mathbf{y} - \\mathbf{X} \\beta ||_2^2$ 을 최소화하면 식이 좀 더 간단해진다.\n","\n","$\n","\\qquad\n","\\begin{align*}\n","\\nabla_{\\beta} || \\mathbf{y} - \\mathbf{X} \\beta ||_2^2 &= \n","\\left(\n","\\partial_{\\beta_1} || \\mathbf{y} - \\mathbf{X} \\beta ||_2^2 , \\; \\dots, \\;\n","\\partial_{\\beta_d} || \\mathbf{y} - \\mathbf{X} \\beta ||_2^2\n","\\right) \\\\ &=\n","- \\frac{2}{n} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\beta)\n","\\end{align*}\n","$\n","\n","<br>\n","\n","$\n","\\qquad\n","\\beta^{(t+1)} \\; \\leftarrow \\;\n","\\beta^{(t)} + \\frac{2 \\lambda}{n} \\mathbf{X}^T \\left(\\mathbf{y} - \\mathbf{X} \\beta^{(t)}\\right)\n","$"]},{"cell_type":"markdown","metadata":{"id":"D84ZkIowjdA2"},"source":["<br>\n","\n","### 3.3.3 경사하강법 기반 선형 회귀 알고리즘\n","\n","- 종료조건을 일정 **학습횟수**로 변경한 점만 뺴고 앞에서 배운 경사하강법 일고리즘과 같다.\n","\n","```python\n","# Input: X, y, lr(학습률), T(학습횟수)\n","# Output: beta\n","# norm : L2-노름을 계산하는 함수\n","\n","for t in range(T):\n","    error = y - X @ beta\n","    grad = - transpose(X) @ error\n","    beta = beta - lr * grad\n","```"]},{"cell_type":"markdown","metadata":{"id":"VLAdLskUlV7U"},"source":["<br>\n","\n","- 이제 경사하강법 알고리즘으로 역행렬을 이용하지 않고 회귀계수를 계산할 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXyh1fgHj7ye","executionInfo":{"status":"ok","timestamp":1627299065662,"user_tz":-540,"elapsed":353,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"7a29e2c7-ab33-4803-9a41-c42070b956df"},"source":["import numpy as np\n","\n","X = np.array([[1, 1],\n","              [1, 2],\n","              [2, 2],\n","              [2, 3]])\n","y = np.dot(X, np.array([1, 2])) + 3\n","\n","beta_gd = [10.1, 15.1, -6.5] # [1, 2, 3] 이 정답\n","X_ = np.array([np.append(x,[1]) for x in X]) # intercept 항 추가\n","\n","for t in range(5000):\n","    error = y - X_ @ beta_gd\n","    #error = error / np.linalg.nrom(error)\n","    grad = -np.transpose(X_) @ error\n","    beta_gd = beta_gd - 0.01 * grad\n","\n","print(beta_gd)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.00000367 1.99999949 2.99999516]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OLNKmTxplBun"},"source":["- 그러나 경사하강법 알고리즘에선 **학습률**과 **학습횟수**가 중요한 hyperparameter 가 된다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdXDI9NymqDs","executionInfo":{"status":"ok","timestamp":1627299453892,"user_tz":-540,"elapsed":510,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"cf8b7611-293d-4c12-c7fd-287a2348ded4"},"source":["# 학습 횟수를 작게한 경우\n","import numpy as np\n","\n","X = np.array([[1, 1],\n","              [1, 2],\n","              [2, 2],\n","              [2, 3]])\n","y = np.dot(X, np.array([1, 2])) + 3\n","\n","beta_gd = [10.1, 15.1, -6.5] # [1, 2, 3] 이 정답\n","X_ = np.array([np.append(x,[1]) for x in X]) # intercept 항 추가\n","\n","for t in range(100): # 5000 -> 100\n","    error = y - X_ @ beta_gd\n","    #error = error / np.linalg.nrom(error)\n","    grad = -np.transpose(X_) @ error\n","    beta_gd = beta_gd - 0.01 * grad\n","\n","print(beta_gd)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 3.41314549  4.63604548 -6.69249764]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P6gbU2wImwDB"},"source":["<br>\n","\n","## 3.4 경사하강법은 만능일까?\n","\n","- 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 **적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장**되어 있다.\n","  - 볼록한 함수는 그레디언트 벡터가 항상 최소점을 향한다.\n","- 특히 선형회귀의 경우 목적식 $|| \\mathbf{y} - \\mathbf{X} \\beta ||_2$ 은 **회귀계수 $\\beta$ 에 대해 볼록함수**이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장된다.\n","- 하지만 **비선형회귀** 문제의 경우 목적식이 볼록하지 않을 수 있으므로 **수렴이 항상 보장되지는 않는다.**\n","  - 특히 딥러닝을 사용하는 경우 목적식은 대부분 볼록함수가 아니다.\n","  - 그러므로 딥러닝에는 변형된 경사하강법을 사용한다."]},{"cell_type":"markdown","metadata":{"id":"8sTRFXbznBkV"},"source":["<br>\n","\n","## 3.5 확률적 경사하강법 (SGD, Stochastic Gradient Descent)"]},{"cell_type":"markdown","metadata":{"id":"0BYaIeItquVG"},"source":["### 3.5.1 확률적 경사하강법이란?\n","\n","- 확률적 경사하강법은 모든 데이터를 사용해서 업데이트하는 대신 **데이터 한 개 또는 일부를 활용**하여 업데이트한다.\n","  - 데이터 한개 : SGD\n","  - 데이터 일부 : mini-batch SGD\n","- 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있다.\n","- SGD 라고 해서 만능은 아니지만 딥러닝의 경우 **SGD가 경사하강법보다 실증적으로 더 낫다**고 검증되었다.\n"]},{"cell_type":"markdown","metadata":{"id":"biYqUYyEnTjt"},"source":["- SGD는 데이터의 일부를 가지고 파라미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는 데 도움이 된다.\n","  - 전체 데이터 $(\\mathbf{X}, \\mathbf{y})$ 를 쓰지 않고 미니배치 $\\left(\\mathbf{X}_{(b)}, \\mathbf{y}_{(b)}\\right)$ 를 써서 업데이트하므로 연산량이 $b/n$ 로 감소한다."]},{"cell_type":"markdown","metadata":{"id":"tcNezqXzp_1w"},"source":["<br>\n","\n","### 3.5.2 확률적 경사하강법의 원리: 미니배치 연산\n","\n","- 경사하강법은 전체 데이터 $\\mathcal{D} = \\left(\\mathbf{X}, \\mathbf{y} \\right)$ 를 가지고 목적식의 그레디언트 벡터인 $\\nabla_\\theta L \\left(\\mathcal{D}, \\theta \\right)$ 를 계산한다.\n","  - $L \\left(\\mathcal{D}, \\theta \\right)$ : 전체 데이터 $\\mathcal{D}$ 와 파라미터 $\\theta$ 로 측정한 목적식\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mAzOlTsX_aW2QRJHdvlEUHQyl8bVzBVj' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"xgLk3jCVrsIJ"},"source":["<br>\n","\n","- SGD는 미니배치 $\\mathcal{D}_{(b)} =  \\left(\\mathbf{X}_{(b)}, \\mathbf{y}_{(b)} \\right) \\subset \\mathcal{D}$ 를 가지고 그레디언트 벡터를 계산한다.\n","  - 미니배치 $\\mathcal{D}_{(b)}$ 를 가지고 목적식의 그레디언트를 근사해서 계산\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1d04gIo90ajL-dPuJcB6vnQwh4i6pdUzq' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"gjJkt92ZsZ-t"},"source":["- 미니배치는 확률적으로 선택하므로 목적식 모양이 바뀌게 된다.\n","  - 매번 다른 미니배치를 사용하기 때문에 곡선 모양이 바뀌게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1KXPWnnk8Go2OJP65pWivUqtSpWyIzVKV' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"mAVb5S_jszkm"},"source":["- SGD는 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 **머신러닝 학습에 더 효율적**이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1jvS3z69zWWam3_zu2RCh00fSTx4FYhfj' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"Dw9zSg-wtm1N"},"source":["<br>\n","\n","### 3.5.3 확률적 경사하강법의 원리: 하드웨어\n","\n","- 만일 일반적인 경사하강법처럼 모든 데이터를 업로드하면 메모리가 부족하여 Out-of-memory 가 발생한다.\n","- 미니배치로 쪼갠 데이터를 사용하면 GPU에서 행렬 연산과 모델 파라미터를 업데이트하는 동안 CPU는 전처리와 GPU에서 업로드할 데이터를 준비한다."]},{"cell_type":"code","metadata":{"id":"IjHbMK3UunUA"},"source":[""],"execution_count":null,"outputs":[]}]}