{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI Math 7강 - 통계학.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNVHyessvuizdazokuaJmgp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y0BLs7fTm6g9"},"source":["# 4. 통계학"]},{"cell_type":"markdown","metadata":{"id":"wgeO5aernAjz"},"source":["## 4.1 통계적 모델링\n","\n","- **통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것**이 목표이다.\n","- 이는 기계학습과 통계학이 공통적으로 추구하는 목표이다.\n","- 데이터를 통해서 정답의 분포를 확실하게 알 수는 없다.\n","- 또한 사용할 수 있는 분포도 매우 다양하다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1tUgOXaz60AGDPUFpDURCKRHN_rO48wOM' width=800/>\n","\n","- 그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하다.\n","- 따라서 **근사적으로 확률분포를 추정**할 수 밖에 없다.\n","- 예측모형의 목적은 분포를 정확하게 맞추는 것 보다는 데이터와 추정 방법의 불확실성을 고려해서 위험을 최소화하는 것"]},{"cell_type":"markdown","metadata":{"id":"z-pBBASznE56"},"source":["<br>\n","\n","### 4.1.1 모수적(parametric) 방법론\n","\n","- 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) **가정한 후** 그 분포를 결정하는 모수(parameter)를 추정하는 방법론\n","- ex) 정규분포를 가지고 확률분포를 모델링할 경우, 정규분포의 두 가지 모수인 평균과 분산을 추정하는 방법을 통해 데이터를 학습한다."]},{"cell_type":"markdown","metadata":{"id":"CST4_9qHnG38"},"source":["<br>\n","\n","### 4.1.2 비모수(noparametric) 방법론\n","\n","- 특정 확률분포를 **가정하지 않고** 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌는 방법론\n","- 기계학습의 많은 방법론은 비모수 방법론에 속한다.\n","- (주의) \n","  - 비모수 방법론이라고 해서 모수가 없는 것은 아니다.\n","  - 모수가 무한히 많거나 모수가 데이터에 따라서 바뀌는 경우가 비모수 방법론이다."]},{"cell_type":"markdown","metadata":{"id":"9SF0M4VEnJkb"},"source":["<br>\n","\n","## 4.2 확률분포 가정하기 (예제)"]},{"cell_type":"markdown","metadata":{"id":"WDyLU171nLfT"},"source":["### 4.2.1 확률분포를 가정하는 방법 : 히스토그램 모양 관찰\n","\n","- 데이터가 2개의 값(0 또는 1)만 가지는 경우 $\\rightarrow$ 베르누이분포\n","- 데이터가 $n$개의 이산적인 값을 가지는 경우 $\\rightarrow$ 카테고리분포, 다항분포\n","- 데이터가 $[0,1]$ 사이의 실수값을 가지는 경우 $\\rightarrow$ 베타분포\n","- 데이터가 0 이상의 값을 가지는 경우 $\\rightarrow$ 감마분포, 로그정규분포 등\n","- 데이터가 $\\mathbb{R}$ 전체에서 값을 가지는 경우 $\\rightarrow$ 정규분포, 라플라스분포 등"]},{"cell_type":"markdown","metadata":{"id":"wwIXiUMPnNmD"},"source":["- 단, 기계적으로 확률분포를 가정해서는 안되며, **데이터를 생성하는 원리를 먼저 고려하는 것이 원칙**이다.\n","- 각 분포마다 검정하는 방법들이 있으므로 모수를 추정한 후에는 반드시 검정을 해야 한다."]},{"cell_type":"markdown","metadata":{"id":"FGTAgXY3nPzy"},"source":["<br>\n","\n","## 4.3 데이터로 모수를 추정\n","\n","- 데이터의 확률분포를 가정했다면 모수를 추정해볼 수 있다."]},{"cell_type":"markdown","metadata":{"id":"DxMO5NpRnSNb"},"source":["<br>\n","\n","### 4.3.1 정규분포의 모수\n","\n","- 정규분포의 모수는 평균 $\\mu$ 과 분산 $\\sigma^2$ 이다.\n","- 이 두 가지 모수를 추정하는 통계량(statistic)은 다음과 같다."]},{"cell_type":"markdown","metadata":{"id":"PAqUtzzUnUi6"},"source":["<br>\n","\n","**표본평균** ($\\bar{X}$)\n","\n","- 모집단의 평균을 추정하는 통계량\n","\n","- $\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i$\n","  - 주어진 데이터의 산술평균\n","\n","- $\\mathbb{E} [\\hat{X}] = \\mu$\n","  - 표본평균의 기대값은 원래 데이터에서 관찰되는 모집단의 평균과 일치한다."]},{"cell_type":"markdown","metadata":{"id":"PmW-s0FSnYT6"},"source":["<br>\n","\n","**표본분산** ($S^2$)\n","\n","- 모집단의 분산을 추정하는 통계량\n","\n","- $S^2 = \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X}_i)^2$\n","  - 주어진 데이터에서 표본평균을 뺀 값의 제곱의 산술평균\n","  - 표본분산을 구할 때 $N$이 아니라 $N-1$ 로 나누는 이유는 불편(unbiased) 추정량을 구하기 위해서이다.\n","    - 표본분산의 기대값을 취했을 때 모집단의 분산과 일치하기 위해서 사용\n","- $\\mathbb{E} [S^2] = \\sigma^2$\n","  - 표본분산의 기대값은 원래 데이터에서 관찰되는 모집단의 분산과 일치한다."]},{"cell_type":"markdown","metadata":{"id":"9YTCCW4InasC"},"source":["<br>\n","\n","### 4.3.2 통계량(statistic)의 활용\n","\n","- 이렇게 구한 표본평균과 표본분산을 가지고 주어진 데이터의 확률분포의 모수를 추정해볼 수 있다.\n","- 추정된 모수를 가지고 원래 데이터의 성질 및 정보를 취합할 수 있다.\n","- 이를 통해 예측을 하거나 의사결정을 내릴 때 이와 같은 통계량을 사용해볼 수 있다."]},{"cell_type":"markdown","metadata":{"id":"5n66Kj4yndni"},"source":["<br>\n","\n","### 4.3.2 표집분포 (sampling distribution)\n","\n","- **통계량(표본평균, 표본분산)**의 확률분포를 **표집분포**라고 부른다. (표본들의 분포 x)\n","- (주의) : 표본분포(sample distribution)와 표집분포(sampling distribution)은 다른 것이다."]},{"cell_type":"markdown","metadata":{"id":"UXEiSbTengFa"},"source":["<br>\n","\n","### 4.3.3 중심극한정리 (Central Limit Theorem)\n","\n","- **표본평균의 표집분포**는 $N$이 커질수록 정규분포 $\\mathcal{N} (\\mu, \\sigma^2/N)$ 를 따른다.\n","- 이를 중심극한정리 라고 부른다.\n","- 모집단의 분포가 정규분포를 따르지 않아도 성립한다.\n","  - 모집단의 분포가 정규분포를 따르지 않으면 표본분포(sample distribution)은 $N$이 커져도 정규분포를 따르지 않는다.\n","  - 모집단의 분포가 정규분포를 따르지 않아도 표본평균의 표집분포(sampling distribution)은 $N$이 커지면 정규분포를 따른다.\n","\n","- ex) 베르누이 확률분포(이항분포)를 따르는 확률변수의 분포\n","  - 처음 데이터를 모았을 때 표본평균의 분포를 확인해보면 처음에는 양 극단으로 나뉜 데이터가 관찰된다.\n","  - 데이터를 모으면 모을수록 베르누이분포들의 표본평균의 표집분포는 정규분포를 따른다.\n","  - 평균값은 하나의 값에 몰려 있다.\n","  - 반면 분산은 점점 짧아진다.\n","    - 데이터의 개수 $N$이 증가하면 표본분산의 $\\sigma^2/N$ 이 0으로 간다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1aGOZ_6IpYH5caSBK-B4KojdrcMBAmGTK' width=700/>"]},{"cell_type":"markdown","metadata":{"id":"9_i636ArnmHS"},"source":["<br>\n","\n","## 4.4 최대가능도 추정법 (MLE)\n","\n","- 표본평균이나 표본분산은 정규분포 뿐만이 아니라 다른 분포에서도 계산할 수 있는 중요한 통계량 중 하나이다.\n","- 하지만 확률분포마다 사용하는 모수가 다르므로 확률분포의 성질을 추정하는 모수를 결정하는 적절한 통계량이 달라지게 된다.\n","- **이론적으로 가장 가능성이 높은 모수를 추정하는 방법** 중 하나는 **최대가능도 추정법(Maximum Likelihood Estimation, MLE)**이다.\n","- 최대가능도 추정법을 이용해서 주어진 확률분포를 어떤 식으로 가정하느냐에 상관없이 이론적으로 가능성이 가장 높은 모수를 추정한다.\n","\n","$$\n","\\hat{\\theta}_{\\text{MLE}} = \n","\\underset{\\theta}{\\text{argmax}} \\; L(\\theta \\,;\\, \\mathbb{x}) = \n","\\underset{\\theta}{\\text{argmax}} \\; P(\\mathbb{x} \\,|\\, \\theta)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"mPQWUWQenpZ7"},"source":["<br>\n","\n","### 4.4.1 가능도(likelihood) 함수 : $L(\\theta \\,;\\, \\mathbb{x})$\n","\n","- 모수 $\\theta$ 를 따르는 분포가 $\\mathbb{x}$ 를 관찰할 **가능성**을 뜻한다. (모수 $\\theta$ 에 대한 **확률로 해석하면 안된다.**)\n","- 가능도 함수는 확률질량(밀도)함수와 같은 것이지만 **관점의 차이**가 있다.\n","  - 확률질량(밀도)함수\n","    - 모수 $\\theta$ 가 주어져 있을 때 데이터 $\\mathbb{x}$ 에 대한 함수\n","  - 가능도 함수\n","    - 주어진 데이터 $\\mathbb{x}$ 에 대해서 모수 $\\theta$ 를 변수로 둔 함수\n","    - 데이터가 주어진 상황에서 모수 $\\theta$를 변형시킴에 따라 값이 바뀌는 함수"]},{"cell_type":"markdown","metadata":{"id":"rj9O2ecJnrpz"},"source":["<br>\n","\n","### 4.4.2 로그가능도 최적화\n","\n","- 데이터 집합 $X$ 가 **독립적으로 추출**되었을 경우 **로그가능도를 최적화**한다.\n","  - 데이터 집합 $X$ 의 각 행벡터가 **독립적으로 추출**되었을 경우 가능도 함수를 **확률질량(밀도)함수 $P(\\mathbb{x}_i \\,|\\, \\theta)$ 의 곱셈**으로 표현할 수 있다.\n","    - 곱셈으로 표현되는 것은 데이터 집합의 확률분포가 독립적으로 추출되었을 경우에 가능한 공식이다.\n","  - 이 경우 로그 함수의 성질을 이용해서 가능도 함수에 로그를 씌워주게 되면 **로그 가능도**가 된다.\n","  - 이는 **확률질량(밀도)함수 $P(\\mathbb{x}_i \\,|\\, \\theta)$의 덧셈**으로 표현할 수 있다.\n","\n","$$\n","L(\\theta \\,;\\, \\mathbb{x}) = \n","\\Pi_{i=1}^n P(\\mathbb{x}_i \\,|\\, \\theta)\n","\\quad \\Rightarrow \\quad\n","log \\; L(\\theta \\,;\\, \\mathbb{x}) = \n","\\sum_{i=1}^n log \\; P(\\mathbb{x}_i \\,|\\, \\theta)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"8mxIJ2aSnuQN"},"source":["<br>\n","\n","### 4.4.3 로그가능도를 사용하는 이유\n","\n","- 로그가능도를 최적화하는 모수 $\\theta$ 는 가능도를 최적화하는 MLE가 된다.\n","\n","- 데이터의 숫자가 적으면 상관없지만 만일 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능하다.\n","  - 0 ~ 1 사이의 확률을 수억번 곱해주는 연산은 컴퓨터는 **연산 오차** 때문에 연산이 불가능하다.\n","- 데이터가 독립일 경우, 로그를 사용하면 **가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문**에 컴퓨터로 연산이 가능해진다.\n","- 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그 가능도를 사용하면 가능도를 사용했을 때의 연산량 $O(n^2)$ 을 $O(n)$ 으로 줄여준다.\n","- 대게의 손실함수의 경우 경사하강법을 사용하므로 **음의 로그가능도(negative log-likelihood)**를 최적화하게 된다."]},{"cell_type":"markdown","metadata":{"id":"Yw8ayB1ynxHb"},"source":["<br>\n","\n","### 4.4.4 최대가능도 추정법 예제 : 정규분포 (연속확률변수)\n","\n","> 정규분포를 따르는 확률변수 $X$ 로부터 독립적인 표본 $\\{x_1, \\dots, x_n\\}$ 을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하면?\n","\n","$$\n","\\hat{\\theta}_{\\text{MLE}} = \n","\\underset{\\theta}{\\text{argmax}} \\; L(\\theta \\,;\\, \\mathbb{x}) = \n","\\underset{\\theta}{\\text{argmax}} \\; P(\\mathbb{x} \\,|\\, \\theta) = \n","\\underset{\\mu,\\,\\sigma^2}{\\text{argmax}} \\; P(X \\,|\\, \\mu, \\, \\sigma^2)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Fh3ZDS64n2AS"},"source":["<br>\n","\n","- 최대가능도 추정법은 주어진 데이터를 가지고 가능도 함수 $L(\\theta \\,;\\, \\mathbb{x})$ 를 최적화하는 모수 $\\theta$ 를 찾는 것이다.\n","\n","$$\n","\\begin{align*}\n","log \\; L(\\theta \\,;\\, \\mathbb{x}) &=\n","\\sum_{i=1}^n log \\; P(x_i \\,|\\, \\theta) \\\\\n","&= \\sum_{i=1}^n log \\; \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{|x_i - \\mu|^2}{2\\sigma^2}} \\\\\n","&= -\\frac{n}{2} log \\; 2\\pi\\sigma^2 - \\sum_{i=1}^n \\frac{|x_i - \\mu|^2}{2\\sigma^2}\n","\\end{align*}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"7Mq1-tF8n4bN"},"source":["- $\\theta = (\\mu, \\, \\sigma)$ 에 대해 마지막 수식을 미분해서 최적화를 할 수 있다.\n","\n","$$\n","0 = \\frac{\\partial \\, log \\; L}{\\partial \\, \\mu} = - \\sum_{i=1}^{n} \\frac{x_i - \\mu}{\\sigma^2}\n","$$\n","\n","$$\n","0 = \\frac{\\partial \\, log \\; L}{\\partial \\, \\sigma} = - \\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^{n} |x_i - \\mu |^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"e13ofsgCdA2P"},"source":["- 두 미분이 모두 0이 되는 $\\mu, \\sigma$ 를 찾으면 가능도를 최대화하게 된다.\n","\n","$$\n","\\hat{\\mu}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n x_i\n","$$\n","\n","$$\n","\\hat{\\sigma}_{MLE}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2\n","$$\n","\n","- MLE는 불편추정량을 보장하진 않는다.\n","  - $n-1$ : x\n","  - $n$ : o"]},{"cell_type":"markdown","metadata":{"id":"D_V_1YkKn6S0"},"source":["<br>\n","\n","### 4.4.5 최대가능도 추정법 예제 : 카테고리 분포 (이산확률변수)\n","\n","- 카테고리 분포 : d개의 차원에서 하나의 값을 선택한 확률변수\n","- d개 중 선택한 값은 1이고 나머지는 0인 원-핫 벡터로 x값이 주어진다.\n","- 카테고리 분포 Multinoulli($\\mathbb{x};\\; p_1, \\, \\dots, \\, p_d$) 를 따르는 확률변수 $X$ 로부터 독립적인 표본 $\\left\\{ \\mathbb{x}_1, \\dots, \\mathbb{x}_n \\right\\}$ 을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하면?\n","\n","$$\n","\\hat{\\theta}_{\\text{MLE}} = \n","\\underset{p_1, \\dots, p_d}{\\text{argmax}} \\; P(\\mathbb{x}_i \\,|\\, \\theta) = \n","\\underset{\\mu,\\,\\sigma^2}{\\text{argmax}} \\; log \\left( \\Pi_{i=1}^n \\Pi_{k=1}^d p_{k}^{x_{i, k}} \\right)\n","$$\n","\n","- 카테고리 분포의 모수는 다음 제약식을 만족해야 한다.\n","  - $p_k$ : k 차원에서 값이 1 또는 0이 될 확률\n","\n","$$\n","\\sum_{k=1}^d p_k = 1\n","$$"]},{"cell_type":"markdown","metadata":{"id":"kKGLMypRfvM4"},"source":["$$\n","log \\left( \\Pi_{i=1}^n \\Pi_{k=1}^d p_{k}^{x_{i, k}} \\right) =\n","\\sum_{k=1}^d \\left( \\sum_{i=1}^n x_{i,k} \\right) log \\, p_k\n","$$\n","\n","$$\n","n_k = \\sum_{i=1}^n x_{i,k}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"CHbBRM-WghY7"},"source":["- 오른쪽 제약식을 만족하면서 왼쪽 목적식을 최대화하는 것이 구하고자 하는 MLE 이다.\n","\n","$$\n","log \\left( \\Pi_{i=1}^n \\Pi_{k=1}^d p_{k}^{x_{i, k}} \\right) =\n","\\sum_{k=1}^d n_k \\, log \\, p_k \\quad \\text{with} \\quad \\sum_{k=1}^d p_k = 1\n","$$"]},{"cell_type":"markdown","metadata":{"id":"8oXoxSiegp0f"},"source":["- 라그랑주 승수법을 통해 최적화 문제를 풀 수 있다.\n","\n","$$\n","\\mathcal{L} (p_1, \\dots, p_k, \\lambda) = \n","\\sum_{k=1}^d n_k \\, log \\, p_k + \n","\\lambda \\left(1 - \\sum_k p_k\\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"nVeBry_FhCYa"},"source":["- 라그랑주 목적식을 각각의 모수 $p_k$ 에 대해 미분하고 라그랑주 승수인 $\\lambda$ 에 대해서도 미분해준다.\n","\n","$$\n","0 = \\frac{\\partial \\, \\mathcal{L}}{\\partial \\, p_k} = \n","\\frac{n_k}{p_k} - \\lambda\n","$$\n","\n","$$\n","0 = \\frac{\\partial \\, \\mathcal{L}}{\\partial \\, \\lambda} = \n","1 - \\sum_{k=1}^d p_k\n","$$"]},{"cell_type":"markdown","metadata":{"id":"mUgBo8eVhn_F"},"source":["- 위 두 식을 조합하여 $p_k$ 에 대한 식을 구할 수 있다.\n","\n","$$\n","p_k = \\frac{n_k}{\\sum_{k=1}^d n_k} = \\frac{n_k}{n} \\quad n : \\text{데이터개수}\n","$$\n","\n","- 카테고리 분포의 MLE는 경우의 수를 세어서 비율을 구하는 것이다."]},{"cell_type":"markdown","metadata":{"id":"PnZgWid3n8gj"},"source":["<br>\n","\n","## 4.5 딥러닝에서 최대가능도 추정법\n","\n","- 최대가능도 추정법을 이용해서 기계학습 모델을 학습할 수 있다.\n","- 딥러닝 모델의 가중치 $\\theta = (W^{(1)}, \\dots, W^{(L)})$ 라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 $(p_1, \\dots, p_k)$ 를 모델링한다.\n","- 원-핫 벡터로 표현한 정답 레이블 $\\mathbb{y} = (y_1, \\dots, y_K)$ 을 관찰 데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있다.\n","\n","$$\n","\\hat{\\theta}_{MLE} = \n","\\underset{\\theta}{\\text{argmax}} \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^K y_{i,k} log (MLP_\\theta (\\mathbb{x}_i)_k)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"GWyVNZaHn-1U"},"source":["<br>\n","\n","## 4.6 확률분포의 거리\n","\n","- 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도한다.\n","- 데이터공간에 두 개의 확률분포 $P(\\mathbb{x})$, $Q(\\mathbb{x})$ 가 있을 경우 **두 확률분포 사이의 거리(distance)**를 계산할 때 다음과 같은 함수들을 제공한다.\n","  - 총변동 거리 (TV, Total Variation Distance)\n","  - 쿨백-라이블러 발산 (KL, Kullback-Leibler Divergence)\n","  - 바슈타인 거리 (Wasserstein Distance)"]},{"cell_type":"markdown","metadata":{"id":"6QseEIRaoBED"},"source":["<br>\n","\n","### 4.6.1 쿨백-라이블러 발산\n","\n","- 쿨백-라이블러 발산(KL DIvergence)은 다음과 같이 정의한다.\n"]},{"cell_type":"markdown","metadata":{"id":"je__LVlwlXvx"},"source":["- 이산확률변수\n","\n","$$\n","\\mathbb{KL} (P || Q) = \n","\\sum_{\\mathbb{x} \\in \\mathcal{X}} P(\\mathbb{x}) log \\left( \\frac{P(\\mathbb{x})}{Q(\\mathbb{x})} \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"nCkx2LpWlaN1"},"source":["- 연속확률변수\n","\n","$$\n","\\mathbb{KL} (P || Q) = \n","\\int_\\mathcal{x}  P(\\mathbb{x}) log \\left( \\frac{P(\\mathbb{x})}{Q(\\mathbb{x})} \\right) d\\mathbb{x}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"5pU0ypq0lv1c"},"source":["<br>\n","\n","- 쿨백 라이블러는 다음과 같이 분해할 수 있다.\n","\n","$$\n","\\mathbb{KL} (P || Q) = \n","- \\underset{\\text{크로스 엔트로피}}{\\mathbb{E}_{\\mathbb{x} \\, \\sim \\, P(\\mathbb{x})} \\left[ log Q(\\mathbb{x})\\right]}\n","+ \\underset{\\text{엔트로피}}{\\mathbb{E}_{\\mathbb{x} \\, \\sim \\, P(\\mathbb{x})} \\left[ log P(\\mathbb{x})\\right]}\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lUi9zgAAmi7g"},"source":["- 분류 문제에서 정답레이블을 $P$, 모델 예측을 $Q$ 라 두면 **최대가능도 추정법은 쿨백-라이블러 발산을 최소화**하는 것과 같다.\n","  - MLE == 쿨백-라이블러 발산 최소화"]},{"cell_type":"code","metadata":{"id":"WnKpJ3e8mrts"},"source":[""],"execution_count":null,"outputs":[]}]}