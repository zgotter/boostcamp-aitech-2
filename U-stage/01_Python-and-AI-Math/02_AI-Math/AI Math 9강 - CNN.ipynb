{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI Math 9강 - CNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNq+fyFpgKZrlchcAV06NyL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vuhDrrOw1MtL"},"source":["# 9. CNN"]},{"cell_type":"markdown","metadata":{"id":"cYqKMrqD2Fif"},"source":["## 9.1 Convolution 연산 이해하기"]},{"cell_type":"markdown","metadata":{"id":"ldaLexrJ3pyF"},"source":["### 9.1.1 fully connected\n","\n","- 지금까지 배운 다층신경망(MLP)은 각 뉴런들이 선형모델과 활성함수로 모두 연결된 (fully connected) 구조였다.\n","  - 주어진 입력 벡터에 대해서 가중치 행렬 $W_i$ 에 해당하는 행과 입력 벡터의 내적(행렬곱)을 통해서 잠재변수에 해당하는 $h_i$ 를 얻는다."]},{"cell_type":"markdown","metadata":{"id":"qLPvkm1j3dOn"},"source":["\n","- 각 성분 $h_i$ 에 대응하는 가중치 행 $W_i$ 이 필요하다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1dNk3VYu9zyUXx9JxUWfIJyvg8eExpEfy' width=600/>\n","(출처: https://priorprobability.com)"]},{"cell_type":"markdown","metadata":{"id":"Tq9A6CZX3bF1"},"source":["- 만일 $i$ 가 바뀌면 사용되는 가중치도 바뀐다.\n","- 이로 인해 가중치 행렬의 크기가 커지게 되고 학습이 필요한 파라미터의 숫자가 매우 커지게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1-zSqeARWSUgQF9eWhkG0tuEPaUh94Zfg' width=600/>\n","(출처: https://priorprobability.com)"]},{"cell_type":"markdown","metadata":{"id":"c3mKep3d3xT-"},"source":["<br>\n","\n","### 9.1.2 Convolution 연산\n","\n","- Convolution 연산은 이와 달리 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조이다.\n","- Convolution 연산은 커널(kernel) 이라는 고정된 가중치 행렬을 사용한다.\n","- 고정된 커널을 입력 벡터 상에서 움직여 가면서 선형 모델과 합성 함수가 적용되는 구조이다."]},{"cell_type":"markdown","metadata":{"id":"m6ilHNYM39a7"},"source":["- $V$ : 커널\n","- $k$ : 커널의 사이즈\n","- 모든 $i$ 에 대해 적용되는 커널은 $V$ 로 같고 커널의 사이즈만큼 $\\mathbb{x}$ 상에서 이동하면서 적용한다.\n","  - 입력 벡터 $\\mathbb{x}$ 를 모두 사용하는 것이 아니라 커널 사이즈인 $k$ 에 대응되는 사이즈 만큼 입력 벡터에서 추출하여 사용한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1nZtW3PCYFoDyFZK6QLvB7dJAHDKQAuQA' width=600/>\n","(출처: https://priorprobability.com)"]},{"cell_type":"markdown","metadata":{"id":"oWCL7jbd4Pac"},"source":["- 활성화 함수를 제외한 Convolution 연산도 선형변환에 속한다.\n","  - 커널을 사용하는 convolution 연산도 선형변환의 한 종류이다.\n","  - 중요한 것은 가중치 행렬이 $i$ 에 따라서 변하는 것이 아니라 고정된 가중치 행렬을 사용한다는 점이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1UF0H-diJww4aJIEfNeIYadKdMEU0tuWb' width=600/>\n","(출처: https://priorprobability.com)"]},{"cell_type":"markdown","metadata":{"id":"RDSg7Uyv7Y39"},"source":["- 가중치 행렬은 $i$ 갯수에 상관없이 커널 사이즈 $k$ 는 고정된 상태로 커널이 공통적으로 적용된다.\n","- 이를 통해 파라미터 갯수를 많이 줄일 수 있다."]},{"cell_type":"markdown","metadata":{"id":"mwB1KWnV4gZc"},"source":["<br>\n","\n","### 9.1.3 Convolution 연산의 수학적 의미\n","\n","- convolution 연산은 2가지로 분리할 수 있다.\n","  - continuous : 정의역이 연속인 공간에서 적분을 통해 계산할 수 있다.\n","  - discrete : 정의역이 이산적인 공간에서 급수를 통해 계산할 수 있다.\n","- 이 2가지는 적분이냐 급수이냐의 차이만 있을 뿐 적용되는 방식은 동일하다."]},{"cell_type":"markdown","metadata":{"id":"xnnknAo28DyF"},"source":["- 두 개의 함수 $f$ 와 $g$ 가 있을 때 $x$ 라는 신호(입력)에 대해서 값을 계산할 때는 전체 정의역에서 $f$ 와 $g$ 를 각각 $z$ 를 움직여가면서 두 함수를 곱해준 값을 적분하거나 더해주면서 convolution 연산을 수행하게 된다.\n","  - $f$ : 커널(kernel)\n","  - $g$ : 신호(signal)\n","  - 신호와 커널의 역할은 바뀔수도 있음\n","  - 함수 중 $x-z$ 또는 $i-a$ term 이 들어간 것이 신호로 사용된다."]},{"cell_type":"markdown","metadata":{"id":"mByWfXdJ84Cg"},"source":["\n","- Convolution 연산의 수학적인 의미는 신호(signal)를 **커널을 이용해 국소적으로 증폭 또는 감소**시켜서 정보를 추출 또는 필터링하는 것이다.\n","- Convolution 을 수식으로만 이해하는 것은 매우 어렵다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1HcRvwFfhKMaolACaq1lOhRegF0Tl1T9o' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"Gyzs6O1S4-ts"},"source":["- CNN 에서 사용하는 연산은 사실 convolution 이 아니고 cross-correlation 이라 부른다.\n","  - 빼기를 사용하지 않고 더하기를 사용하는 cross-correlation 을 사용한다.\n","  - 정확하게는 cross-correlation 연산이라고 보는 게 맞다.\n","  - 하지만 범용적으로 convolution 연산이라고 부르는 이유는 전체 공간에서는 더하기인 지 빼기 인지가 중요하지 않기 때문이다.  \n","  (컴퓨터 연산에서는 더하기 or 빼기 가 큰 차이가 있다.)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=17pUAeCS3V3Hn_ZiTMIDCSDMkl13IzSrb' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"qGPTbfZp5Hwz"},"source":["<br>\n","\n","### 9.1.4 그래프를 통한 이해\n","\n","- convolution 연산을 graphical 하게 이해해보자.\n","- 파란색 : 신호\n","- 빨간색 : 커널\n","- 검은색 : 결과\n","- 커널은 정의역 내에서 움직여도 변하지 않고**(translation invariant)** 주어진 신호(파란색)에 **국소적(local)**으로 적용한다.\n","  - 노란색 : 국소적으로 적용되는 연산\n","- 원래 신호(파란색) 함수를 결과(검은색)로 변환시켜서 정보를 확산, 감소, 추출하는 역할을 convolution 연산이 수행한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ISXs0yj6m8nYkos1JNZMiZ5d8pEecaHW' width=600/>\n","(출처: Wikipedia)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=18-VDSta0pNiYuxihbqfbgguEu802xOzs' width=600/>\n","(출처: Wikipedia)"]},{"cell_type":"markdown","metadata":{"id":"VloFYmsN-Lyd"},"source":["<br>\n","\n","## 9.2 영상처리에서 Convolution\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1oSXiIP1uonwyI6fwqnh2FalZDV2X80h2' width=600/>\n","(출처: http://setosa.io/ev/image-kernels/)"]},{"cell_type":"markdown","metadata":{"id":"duBLzsY-BKbQ"},"source":["\n","- 다양한 종류의 kernel 에 따라 영상이 어떻게 변하는 지 확인할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1pBbM4rVoJUbWOzJGLdImY5OBKfhp3ri4' width=600/>\n","(출처: http://setosa.io/ev/image-kernels/)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1R17_H26Ih00hwr6HEoT4W-8EhmA3wbFP' width=600/>\n","(출처: http://setosa.io/ev/image-kernels/)"]},{"cell_type":"markdown","metadata":{"id":"bv7l8Q54AJie"},"source":["<br>\n","\n","## 9.3 다양한 차원에서의 Convolution\n","\n","- 앞에서 살펴본 convolution 연산은 1차원 convolution 연산이다.\n","- Convolution 연산은 1차원뿐만 아니라 다양한 차원에서 계산 가능하다.\n","  - 1차원 convolution : 한 변수 $i$ 에 대해 움직임\n","  - 2차원 convolution : 두 변수 $i, j$ 의 위치에 대해 동시에 움직임\n","  - 3차원 convolution : 세 변수 $i, j, k$ 의 위치에 대해 동시에 움직임\n"]},{"cell_type":"markdown","metadata":{"id":"zuF3hkw_Csut"},"source":["- 데이터의 성격에 따라 사용하는 커널이 달라진다.\n","  - 몇 차원의 convolution 을 사용할 지는 데이터의 종류에 따라서 사용 방법이 달라진다.\n","  - ex) 음성이나 텍스트 같은 1D 데이터 -> 1D-conv 사용\n","  - ex) 흑백 영상같은 2D 데이터 -> 2D-conv 사용\n","  - ex) 컬러 영상같은 3D 데이터 -> 3D-conv 사용\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1z85fI4A3hmpOvZECRl6lUdb17V-VUkll' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"wJ7cLQFlC_KF"},"source":["- $i, j, k$ 가 바뀌어도 커널 $f$ 의 값은 바뀌지 않는다.\n","  - 차원이 높아진다 하더라도 $i, j, k$ 의 위치가 변하더라도 커널의 값은 바뀌지 않는다.\n","  - **convolution 연산의 핵심은 커널이 위치에 따라서 바뀌지 않는다는 것**이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Juu6QpK4_GastmSEexctlgnj6uKUldwP' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"91YbN8UDDJZE"},"source":["<br>\n","\n","## 9.4 2차원 Convolution 연산 이해하기"]},{"cell_type":"markdown","metadata":{"id":"c3hkoVmxIzfC"},"source":["### 9.4.1 2D-Conv 연산 수행 과정\n","\n","- 2D-Conv 연산에서 커널은 행렬 구조의 2차원 모양이다.\n","- 2D-Conv 연산은 커널(kernel)을 입력벡터 상에서 x 방향과 y 방향으로 움직여가면서 선형모델과 합성함수가 적용되는 구조이다."]},{"cell_type":"markdown","metadata":{"id":"dLWLbcjMGIOY"},"source":["- $i$ 와 $j$ 가 고정된 상황에서 $p$ 와 $q$ 를 움직여 가면서 연산을 수행한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1kWUcWzCBNp9kLwvWErbnD5Xem6Dl_K-F' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"EvXHRgTwFeYM"},"source":["- 입력에 주어져 있는 데이터에 커널의 크기 만큼 적용하여 계산한다.\n","- 커널에는 $0, 1, 2, 3$ 의 값들이 들어 있다.\n","  - $f(p,q)$ : $p$-행 위치, $q$-열 위치\n","- $p$, $q$ 의 변화에 따라서 $i$ 와 $j$ 를 넣어주게 되면 이것이 convolution 연산이 된다.\n","- 입력에 대해서 커널 사이즈에 맞게 계산이 된다.\n","- 커널과 입력이 겹치는 부분에 대해 행렬곱이 아닌 성분곱(element wise)을 수행하여 모든 값을 더해준다.\n","  - 0x0 + 1x1 + 2x3 + 3x4 = 19\n","- 이 결과 19가 convolution 연산의 첫 번째 계산 결과물이 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1O4LrKt33h-1v4YcCttL6hGEe9-2LIEfh' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"dEYZyEwWFols"},"source":["- 이 커널을 가로, 세로로 한 칸씩 움직이면서 계산하는 것이 2D-Conv 연산이다.\n","  - 커널의 값은 변하지 않고 입력에 사용되는 값만 변한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1h6ZwS_vwCg2SFVygzdG__ql_4xgKNMHW' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"pFoUgxOmFsxs"},"source":["<br>\n","\n","### 9.4.2 2D-Conv 연산 출력의 크기\n","\n","- 입력 크기와 커널 크기에 따라서 Convolution 연산의 출력 크기를 미리 구할 수 있다.\n","- 입력 크기를 $(H, W)$, 커널 크기를 $(K_H, K_W)$, 출력 크기를 $(O_H, O_W)$ 라 하면 출력 크기는 다음과 같이 계산한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=189-_p02zFf5P5aWq4NzNrWyXn8i2V0yz' width=300/>\n","\n","- ex) 입력 : 28x28, 커널 : 3x3, 2D-Conv 연산 수행\n","  - 출력의 크기 : (28-3+1)x(28-3+1) = 26x26\n","\n","- 이렇게 미리 계산한 convolution 연산의 출력 크기를 바탕으로 다음 convolution 연산에서 활용할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"efZzeKoeIm7G"},"source":["<br>\n","\n","### 9.4.3 채널이 여러 개인 2D-Conv ($\\approx$ 3차원)\n","\n","- 채널이 여러 개인 2차원 입력의 경우 2차원 Convolution 을 채널 개수만큼 커널을 만들어서 적용한다고 생각하면 된다.\n","  - 3차원부터는 행렬이 아닌 텐서라 부른다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=143osxxcnh7dTsGjp6It9ndFXzZxVCHd4' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"Y1RO6R9HKDjM"},"source":["- 채널이 여러 개인 경우 커널의 채널 수와 입력의 채널 수가 같아야 한다.\n","  - **채널 개수 == 커널 개수**\n","- 채널이 여러 개인 2D-Conv 에선 각 채널의 갯수 만큼 2차원 입력들을 분리한 상태에서 채널 갯수만큼 커널을 만든 다음 이 커널들을 각각의 2차원 입력에 대해 Conv 연산을 적용한 후에 그 결과물들을 더해줘서 2D-Conv 를 수행한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_nT3Yq5_3Cypul1eDZTDt7-rR1C_dSfl' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"Dw3VTL7ZKIwn"},"source":["- 텐서를 직육면체 블록으로 이해하면 좀 더 이해하기 쉽다. \n","- 채널이 여러 개인 2D-Conv 연산을 수행하면 출력은 채널의 갯수가 1개가 된다.\n","  - 입력의 채널 개수와 커널의 채널 개수를 갖게 해서 각각 convolution 연산을 수행하여 그 값들을 더해줬기 때문에 출력의 채널 개수는 1개가 된다.\n","- $O_H$, $O_W$ 계산법은 앞 슬라이드 참조\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1EwI3ZwTi6pw75a9-8TM8iTyF4wRVmucX' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"dEMK0XQtKzy8"},"source":["- 출력이 여러 개의 채널의 갖게 하고 싶으면 어떻게 해야 할까?\n","  - 커널을 여러 개 만들면 된다.\n","  - **커널의 갯수를 조정하여 출력의 채널 갯수를 결정할 수 있다.**\n","- 커널을 $O_C$ 개 사용하면 출력도 텐서가 된다.\n","- 이 것이 오늘날 CNN 연산에서 사용되는 Convolution Operator의 기본적인 형태이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1nzpM47ZGTCjpHoFWEqgF2-p1ln7q3jMP' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"rx6BMFAmLAik"},"source":["<br>\n","\n","## 9.5 Convolution 연산의 역전파 이해하기\n","\n","- Convolution 연산은 커널이 모든 입력데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 된다."]},{"cell_type":"markdown","metadata":{"id":"RHWWucZ3Nnr9"},"source":["- continuous 일 경우의 수식 (discrete 일 때도 마찬가지로 성립)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1bxonqtQVFoPPq17qQOsYh-MUcZ2n-3Yp' width=400/>\n","\n","- convolution 연산에 해당하는 변수 x에 대해 미분을 하게 되면 적분이든 급수든 관계없이 미분기호가 안에 들어가게 되면 신호(signal)에 해당하는 함수 $g$에 미분이 그대로 적용된다.\n","- 이는 $g$의 도함수 $g'$ 와 $f$ 가 convolution 을 수행한 것이 된다.\n","  - convolution 연산에 미분을 해도 convolution 연산이 된다."]},{"cell_type":"markdown","metadata":{"id":"1YsiUagwOAwN"},"source":["- 그림으로 이해하면 훨씬 쉽다."]},{"cell_type":"markdown","metadata":{"id":"DVBy4DujQ48d"},"source":["\n","- 5개의 입력 데이터가 있고 커널의 사이즈가 3인 1D-Conv 를 생각해보자.\n","- 공식을 통해 출력의 크기는 $5-3+1=3$ 이라는 것을 예상할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1LahqWx8ypptjRx05VVRaFOTBrFDQg3-E' width=250/>"]},{"cell_type":"markdown","metadata":{"id":"rMk-aKJGOmDl"},"source":["- 이 상황에서 convolution 연산을 수행하게 되면 다음과 같이 출력이 계산된다.\n","  - 빨간색 화살표 : $W_1$ 적용하여 연산\n","  - 파란색 화살표 : $W_2$ 적용하여 연산\n","  - 초록색 화살표 : $W_3$ 적용하여 연산"]},{"cell_type":"markdown","metadata":{"id":"uMRJhvWYQtvu"},"source":["\n","- $o_1 = x_1 \\times w_1 + x_2 \\times w_2 + x_3 \\times w_3$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1J5GxPB7sCkA7NC48DDbnI4vts-fqw3wH' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"bwtz96c9OuIg"},"source":["- $o_2 = x_2 \\times w_1 + x_3 \\times w_2 + x_4 \\times w_3$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Cx0gEqlFq__hSJkhfka6967E14UEoIGX' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"3YtFWndKPAy9"},"source":["- $o_3 = x_3 \\times w_1 + x_4 \\times w_2 + x_5 \\times w_3$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ZH3_qy_G_4r19sMOuzrSmfUMZApCcMKI' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"N7LDJ9GSPL40"},"source":["- 이제 역전파 단계에 대해 생각해보자.\n","- 출력 벡터에 손실 함수에 대한 각각의 미분값이 전달된다.\n","  - 각 $\\delta$ 는 미분값을 의미한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1h8JpyqWlI60rKbfg6LpggCosAltHaWeP' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"B8QhqDSkRfWl"},"source":["- 역전파 단계에서 다시 커널을 통해 그레디언트가 전달된다.\n","- $x_3$ 에서 $w_1$, $w_2$, $w_3$ 들이 적용될 때 각 출력에는 다음과 같이 $w$ 가 적용됐다. (순전파 그림의 화살표 확인)\n","  - $o_1$ 에는 $w_3$ 이 적용됨\n","  - $o_2$ 에는 $w_2$ 이 적용됨\n","  - $o_3$ 에는 $w_1$ 이 적용됨\n","- 즉, $x_3$ 라는 입력이 각각 $o_1$, $o_2$, $o_3$ 에서 적용이 될 때 사용된 가중치에 따라서 그레디언트 벡터에 연결되는 가중치들이 결정된다.\n","  - $\\delta_1$ 에 $w_3$ 이 적용됨\n","  - $\\delta_2$ 에 $w_2$ 이 적용됨\n","  - $\\delta_3$ 에 $w_1$ 이 적용됨\n","- 이렇게 적용되어 계산된 값들이 출력 벡터에서 $x_3$ 로 전달된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16Uy_ajBC-NOoHbPJ7F3rrNdlBP7m_e4r' width=500/>\n","\n","- 입력 벡터에서는 곱해졌던 커널들을 통해서 그레디언트가 전달된다."]},{"cell_type":"markdown","metadata":{"id":"W7KO2rs-Rr3V"},"source":["- 각각의 커널들은 그레디언트가 어떻게 전달될까?\n","  - $w_1$\n","    - $o_3$ 가 $x_3$ 에 대해서 $w_1$ 을 통해서 그레디언트를 전달했기 때문에 $\\delta_3$ 는  $x_3$ 가 곱해져서 $w_1$으로 전달된다.\n","  - $w_2$\n","    - $o_2$ 가 $x_3$ 에 대해서 $w_2$ 을 통해서 그레디언트를 전달했기 때문에 $\\delta_2$ 는 $x_3$ 가 곱해져서 $w_2$으로 전달된다.\n","  - $w_3$\n","    - $o_1$ 가 $x_3$ 에 대해서 $w_3$ 을 통해서 그레디언트를 전달했기 때문에 $\\delta_1$ 는 $x_3$ 가 곱해져서 $w_3$으로 전달된다.\n","- 커널에는 $\\delta$ 에 입력값 $x_3$ 을 곱해서 전달한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1o6HnqVOfSfamGDTxTElq_IGfq3RMxDmD' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"ABzqs6oIR1aF"},"source":["- 각각의 커널들은 $x_3$ 이외에 다른 입력에 대해서도 적용됐기 때문에 다른 입력들에 적용됐던 그래디언트들이 똑같이 전달된다.\n","- ex) $w_1$ 에는 다음과 같은 그래디언트들이 전달된다.\n","  - $\\delta_1 x_1$ : $x_1$ 을 통해 들어온 그래디언트\n","  - $\\delta_2 x_2$ : $x_2$ 을 통해 들어온 그래디언트\n","  - $\\delta_3 x_3$ : $x_3$ 을 통해 들어온 그래디언트\n","  - 이 값들을 모두 더해 손실함수에 대한 $w_1$ 의 그레디언트 벡터를 계산하게 된다.\n","- 각 커널에 들어오는 모든 그레디언트를 더하면 결국 convolution 연산과 같다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1t_NJuYseVa-UFaoMS5D0eqPgzNJRmV7Y' width=600/>"]},{"cell_type":"code","metadata":{"id":"oPATPAN9SCmv"},"source":[""],"execution_count":null,"outputs":[]}]}