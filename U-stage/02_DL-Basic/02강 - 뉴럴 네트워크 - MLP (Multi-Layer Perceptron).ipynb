{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02강 - 뉴럴 네트워크 - MLP (Multi-Layer Perceptron).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPSoxC5YGBrWMyjxqUMoUAP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xDgaA2W7uKb9"},"source":["# 2. 뉴럴 네트워크 - MLP (Multi-Layer Perceptron)"]},{"cell_type":"markdown","metadata":{"id":"GL2qOG2muNoG"},"source":["## 2.1 Nueral Networks\n","\n","- \"Neural networks are computing systems vaguely(희미하게) inspired by the biological neural networks that constitue(구성되다) animal brains.\" (wikipedia)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Pwrerrc1cfnduIv2xoEbcEgFwtSBn42n' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"-YGry01qD5Eh"},"source":["\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rpxfeW_YckdpnXGJK6slXZA84E1h_DRD' width=800/>\n","\n","- 비행기 개발 시 처음에는 새를 모방했다.\n","- 시간이 지날수록 처음의 동물 모양은 사라진다."]},{"cell_type":"markdown","metadata":{"id":"VesLNqMvu1RY"},"source":["<br>\n","\n","- Neural networks are function approximators that stack affine transformations followed by nonlinear transformations.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1GzjHel1YrDQCnOLnhcCuyUbApB_DCBbF' width=600/>\n","\n","- 인공 신경망 : 함수 근사\n","- affine transformation : 행렬곱\n","- nonlinear transformation : 비선형 연산"]},{"cell_type":"markdown","metadata":{"id":"giOESfvXu9FP"},"source":["<br>\n","\n","## 2.2 Linear Neural Networks\n","\n","- Let's start with the most simple example."]},{"cell_type":"markdown","metadata":{"id":"bHZA_9_mFTE5"},"source":["### 2.2.1 선형 회귀 모델\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1g5XwfBvMa2y0WAZqRpfB9l4K5rxAXCiX' width=600/>\n","\n","- 선형회귀 (linear regression) \n","- 입력이 1차원이고, 출력도 1차원이다."]},{"cell_type":"markdown","metadata":{"id":"F6qlDHNevG0H"},"source":["<br>\n","\n","### 2.2.2 어떻게 W 와 b 를 찾을까?\n","\n","- We compute the partial derivatives w.r.t. the optimization variables.\n","- backpropagation 사용\n","- loss function 을 줄이는 것이 목표\n","- 파라미터가 어느 방향으로 움직였을 때 loss function 이 줄어드는 지를 찾고 그 방향으로 파라미터를 이동\n","  - 미분의 음수 방향으로 파라미터 update"]},{"cell_type":"markdown","metadata":{"id":"3DS4LdiNF1oY"},"source":["- $N$ 개의 데이터를 모두 활용했을 때 타깃 데이터와 예측 데이터의 차이의 제곱을 최소화하는 loss 에 대한 $w$ 의 편미분\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1LHdRhCDmmByKC6Y_S9FNKJC1BUPKvF4N' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"FvQZt27rGEqe"},"source":["- $N$ 개의 데이터를 모두 활용했을 때 타깃 데이터와 예측 데이터의 차이의 제곱을 최소화하는 loss 에 대한 $b$ 의 편미분\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1QCoZD-n9ym1Y_syGJUxCstbPuMk2qnu8' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"hlNtMsfTvVOu"},"source":["<br>\n","\n","- Then, we iteratively update the optimization variables.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1oN4LnxzBGXkoN2tiGNVFA_JR288XRfSk' width=600/>\n","\n","$$\n","\\begin{aligned}\n","w & \\leftarrow w-\\eta \\frac{\\partial \\operatorname{loss}}{\\partial w} \\\\\n","b & \\leftarrow b-\\eta \\frac{\\partial l o s s}{\\partial b}\n","\\end{aligned}\n","$$\n","\n","- 이렇게 $w$ 와 $b$ 를 업데이트하는 방법을 Gradient Descent(GD) 라고 한다.\n","- 적절한 step size 를 정하는 것도 중요하다."]},{"cell_type":"markdown","metadata":{"id":"-9puapf4vuEE"},"source":["<br>\n","\n","### 2.2.3 다차원 입출력\n","\n","- Of course, we can handle multi dimensional input and output.\n","- 다차원 입력 및 출력을 다룰 땐 행렬을 사용한다.\n","- 행렬을 통한 변환을 affine tranformation 이라고 한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1msLR577_D2wgaq_Iw_ZjE0BfoG3YcYrb' width=600/>\n"]},{"cell_type":"markdown","metadata":{"id":"FE7s7ow6v7cm"},"source":["- One way of interpreting a matrix is to regard it as a mapping between two vector spaces. \n","- 행렬의 곱셈에 대한 해석은 두 벡터 공간 사이의 선형변환이라고 할 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1FdF8Nfgy8Dk8-6_t4lykzTDbhzcrdzU5' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"EcmCV2GXHX_w"},"source":["- $W, b$ 를 가지고 입력 $x$ 를 $y$ 라는 출력으로 보낸다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1yZgRDBHe8zynNgZ4xbq-B3xTf13UAbpk' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"wQmJhd7AwFGO"},"source":["<br>\n","\n","## 2.3 Beyond Linear Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"DKc-z14VIXCZ"},"source":["### 2.3.1 non-linear tranformation 의 필요성\n","\n","- What if we stack more?\n","- 네트워크를 어떻게 여러 개 쌓을 수 있을까?\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rICN32cDtYa9463IjGvXwmpoqEESRxIL' width=600/>\n","\n","- 먼저 단순히 $W$ 를 하나 곱해주게 되면 이는 $W$ 사이의 행렬곱이 되어 하나의 $W$ 와의 차이가 없게 된다.\n"]},{"cell_type":"markdown","metadata":{"id":"KtldPGr1wNYD"},"source":["- We need nonlinearity.\n","- 그러므로 필요한 것이 비선형 변환(nonlinear tranformation) 이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1FIPnj0sOb9GL7Y2Rej-Z7O4UC2xahHzs' width=600/>\n","\n","- 비선형 변환을 통해 얻어지는 feature vector 를 다시 선형변환을 수행하는 과정을 반복한다.\n"]},{"cell_type":"markdown","metadata":{"id":"klH8q7hVwUzj"},"source":["<br>\n","\n","### 2.3.2 Activation functions\n","\n","- Activation functions : 비선형 변환의 한 종류\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Jq3M4TJcgtYCFL5aKf1nP77NLCoyfaTl' width=600/>\n","\n","- 어떤 Activation function 이 좋은 지는 문제마다 다르다.\n","- 중요한 것은 non-linear tranformation 이 들어가야지만 네트워크를 깊게 쌓았을 때 의미가 있다는 것이다."]},{"cell_type":"markdown","metadata":{"id":"I8XfR9HGwgrQ"},"source":["<br>\n","\n","### 2.3.3 Universal Approximators\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mvObTf7bm_IvbmGHLmQbNDcfhHivYror' width=800/>\n","\n","- hidden layer 가 하나 있는 Neural Network 는 대부분의 measurable function 을 근사할 수 있다.\n","- 여기서 근사는 우리가 원하는 지점까지를 의미한다.\n","- 즉, hidden layer 가 하나만 있는 Neural Network의 표현력은 대부분의 continuous function 들을 다 포함한다.\n","- 그렇기 때문에 잘 된다!\n","\n","<br>\n","\n","- Caution: It only guarantees the existence of such networks.\n","- 하지만 이러한 표현을 할 때 주의할 점은 이 것은 존재성만을 보인다는 것이다.\n","  - 이런 걸 만족하는 Neural Network 가 세상 어디엔 존재한다는 것을 말하는 것일 뿐 내가 학습시킨 Neural Network 가 그러한 성질을 갖는다는 것을 말하는 것이 아니다."]},{"cell_type":"markdown","metadata":{"id":"FaiLR0dPwm-p"},"source":["<br>\n","\n","## 2.4 Multi-Layer Perceptron"]},{"cell_type":"markdown","metadata":{"id":"eRyd6IgEJ_lH"},"source":["### 2.4.1 MLP 의 기본 구조\n","\n","- This class of architectures are often called multi-layer perceptrons.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1DjzFZJvdDKcp7nywG4rVXRWyeBzT0GIx' width=600/>\n"]},{"cell_type":"markdown","metadata":{"id":"mBs1QAVAwu2Z"},"source":["- Of course, it can go deeper.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=10aiSRO4A2qAmPHKXv3N6xMqw-GDf7nrX' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"RalIaIaPw2KB"},"source":["<br>\n","\n","### 2.4.2 MLP 의 loss function\n","\n","- What about the loss functions?\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1stoGC7cb-a3WMhe8_eWofaD-vDz2P9TV' width=600/>\n","\n","- loss function울 제곱을 통해 정의할 때와 절대값을 취해 정의할 때는 성질이 달라진다.\n","- outlier 가 있을 때 이 것에 모델이 맞출려고 하다가 망가질 수 있다.\n","- MSE 는 target data 를 찾는 데 항상 도움이 되지 않을수도 있다.\n","- 따라서 중요한 것은 loss function 이 어떠한 성질을 가지고 있고 이게 왜 내가 원하는 결과를 얻어낼 수 있는 지에 대한 이야기를 해야 한다."]},{"cell_type":"markdown","metadata":{"id":"9oxKsbCO1dcg"},"source":["- 분류 문제의 output 은 one-hot vector 로 표현된다.\n","- 따라서 $\\sum_{d=1}^{D} y_{i}^{(d)} \\log \\hat{y}_{i}^{(d)}$ 에서 $\\log \\hat{y}_{i}^{(d)}$ 는 하나만 1이고 나머지는 전부 0이다.\n","  - $\\hat{y}$ 을 logit 이라고 부른다.\n","- CE 의 의미는 output 중에서 특정 클래스에 해당하는 값만 높이겠다는 것을 의미한다.\n","  - 각 클래스의 인덱스가 중요한 것이 아님\n","  - 다른 클래스의 값에 비해 높기만 하면 된다."]}]}