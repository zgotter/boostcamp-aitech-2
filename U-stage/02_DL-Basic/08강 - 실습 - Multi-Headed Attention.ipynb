{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08강 - 실습 - Multi-Headed Attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMetbwvK2AiqhJrPlTQncNO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BHRchNzRVj3J"},"source":["# 8. Multi-Headed Attention\n","\n","- 차원이 어떻게 바뀌는 지를 이해해야 한다."]},{"cell_type":"markdown","metadata":{"id":"V6htOz0SVwmV"},"source":["## 8.1 Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZEpZyzkVoDS","executionInfo":{"status":"ok","timestamp":1628816755142,"user_tz":-540,"elapsed":3994,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"0730e25b-0cae-491d-bdb4-c142e5154e9c"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","print (\"PyTorch version:[%s].\"%(torch.__version__))\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print (\"device:[%s].\"%(device))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["PyTorch version:[1.9.0+cu102].\n","device:[cpu].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t8ql4VGwVvk0"},"source":["<br>\n","\n","## 8.2 Scaled Dot-Product Attention (SDPA)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1U2z2OUvhoY9QTMvfV3KXp1lOmJ5gT4BQ' width=300/>\n","\n","- Data $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the number data and $d$ is the data dimension\n","- Query and Key $Q, K \\in \\mathbb{R}^{n \\times d_K}$\n","- Value $V \\in \\mathbb{R}^{n \\times d_V}$\n","\n","<br>\n","\n","$$\n","\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{K}}}\\right) V \\in \\mathbb{R}^{n \\times d_{V}}\n","$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0A-mnVlWoGK","executionInfo":{"status":"ok","timestamp":1628817108761,"user_tz":-540,"elapsed":7,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"cec13328-7bfe-4ca8-a2e9-9509b9d73546"},"source":["class ScaledDotProductAttention(nn.Module):\n","    def forward(self, Q, K, V, mask=None):\n","        d_K = K.size()[-1] # key dimension\n","        scores = Q.matmul(K.transpose(-2, -1)) / np.sqrt(d_K) # FILL IN HERE\n","        if mask is not None:\n","            scores = scores.masked_fill(mask==0, -1e9)\n","        attention = F.softmax(scores, dim=1)\n","        out = attention.matmul(V)\n","        return out, attention\n","\n","# Demo run of scaled dot product attention\n","# scaled dot product attention 의 목적은 Q 벡터에 대한 인코더를 찾는 것이다.\n","SPDA = ScaledDotProductAttention()\n","# K 와 V 의 dim 이 달라도 된다.\n","n_batch, d_K, d_V = 3, 128, 256 # d_K(=d_Q) does not necessarily be equal to d_V\n","# K 와 V 의 갯수는 같아야 하지만 Q 의 갯수는 달라도 된다.\n","n_Q, n_K, n_V = 30, 50, 50 # n_K and n_V should be the same (같이 decoder 로 넘어가야 하기 때문)\n","Q = torch.rand(n_batch, n_Q, d_K)\n","K = torch.rand(n_batch, n_K, d_K)\n","V = torch.rand(n_batch, n_V, d_V)\n","out, attention = SPDA.forward(Q, K, V, mask=None)\n","\n","def sh(x): return str(x.shape)[11:-1]\n","print('SPDA: Q%s K%s V%s => out%s attention%s' % (sh(Q), sh(K), sh(V), sh(out), sh(attention)))\n","\n","# It supports 'multi-headed' attention\n","n_batch, n_head, d_K, d_V = 3, 5, 128, 256\n","n_Q,n_K,n_V = 30,50,50\n","Q = torch.rand(n_batch,n_head,n_Q,d_K)\n","K = torch.rand(n_batch,n_head,n_K,d_K)\n","V = torch.rand(n_batch,n_head,n_V,d_V)\n","# out: [n_batch x n_head x n_Q x d_V]\n","# attention: [n_batch x n_head x n_Q x n_K] \n","def sh(x): return str(x.shape)[11:-1] \n","print (\"(Multi-Headed) SDPA: Q%s K%s V%s => out%s attention%s\"%\n","       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["SPDA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n","(Multi-Headed) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QbPs0tFjWB_R"},"source":["<br>\n","\n","## 8.3 Multi-Headed Attention (MHA)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16rzLuujTDQJ5RtLGD7_MnCbN5Q6ihKNP' width=300/>\n","\n","$$\n","\\text { head }_{i}=\\text { Attention }\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n","$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPrBGJq5WykR","executionInfo":{"status":"ok","timestamp":1628818759472,"user_tz":-540,"elapsed":524,"user":{"displayName":"김성한","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4suhwOAnN5grX1rwPh5jj9nqx3rC6svCtsnJJLQ=s64","userId":"13992033533003163921"}},"outputId":"bba4ad9b-cd46-4801-899f-7641de663ef7"},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, d_feat=128, n_head=5, actv=F.relu, USE_BIAS=True, dropout_p=0.1, device=None):\n","        \"\"\"\n","        :param d_feat: feature dimension\n","        :param n_head: number of heads\n","        :param actv: activation after each linear layer\n","        :param USE_BIAS: whether to use bias\n","        :param dropout_p: dropout rate / dropout 이 attention weight 에 들어간다.(논문에는 안 들어가 있음)\n","        :device: which device to use (e.g., cuda:0)\n","        \"\"\"\n","        super(MultiHeadedAttention, self).__init__()\n","        \n","        if (d_feat % n_head) != 0: # input dimension 이 head 의 갯수로 나눠질 수 있어야 한다.\n","            raise ValueError(\"d_feat (%d) should be divisible by n_head(%d)\" % (d_feat, n_head))\n","        \n","        self.d_feat = d_feat\n","        self.n_head = n_head\n","        self.d_head = self.d_feat // self.n_head\n","        self.actv = actv\n","        self.USE_BIAS = USE_BIAS\n","        self.dropout_p = dropout_p # prob. of zeroed\n","\n","        self.lin_Q = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS) # 입력이 들어왔을 때 Query vector 를 얻어내는 네트워크\n","        self.lin_K = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS) # 입력이 들어왔을 때 Key vector 를 얻어내는 네트워크\n","        self.lin_V = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS) # 입력이 들어왔을 때 Value vector 를 얻어내는 네트워크\n","        self.lin_O = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n","\n","        self.dropout = nn.Dropout(p=self.dropout_p)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        \"\"\"\n","        :param Q: [n_batch, n_Q, d_feat]\n","        :param K: [n_batch, n_K, d_feat]\n","        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \n","        :param mask: \n","        \"\"\"\n","        n_batch = Q.shape[0]\n","        Q_feat = self.lin_Q(Q)\n","        K_feat = self.lin_Q(K)\n","        V_feat = self.lin_Q(V)\n","        # Q_feat: [n_batch, n_Q, d_feat]\n","        # K_feat: [n_batch, n_K, d_feat]\n","        # V_feat: [n_batch, n_V, d_feat]\n","\n","        # Multi-head split of Q, K, and V (d_feat = n_head * d_head)\n","        # head 의 갯수대로 Q, K, V 를 나눈다.\n","        # d_feat -> n_head & d_head\n","        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        # Q_split: [n_batch, n_head, n_Q, d_head]\n","        # K_split: [n_batch, n_head, n_K, d_head]\n","        # V_split: [n_batch, n_head, n_V, d_head]\n","\n","        # Multi-headed Attention\n","        d_K = K.size()[-1] # key dimension\n","        scores = torch.matmul(Q_split, K_split.permute(0,1,3,2)) / np.sqrt(d_K) # FILL IN HERE\n","        if mask is not None:\n","            scores = scoroes.masked_fill(mask==0, -1e9)\n","        attention = torch.softmax(scores, dim=-1)\n","        x_raw = torch.matmul(self.dropout(attention), V_split) # dropout is NOT mentioned in the paper\n","        # attention: [n_batch, n_head, n_Q, n_K]\n","        # x_raw: [n_batch, n_head, n_Q, d_head]\n","\n","        # Reshape x\n","        # Linear layer 를 통과할 때 batched multiplication 을 하기 위해\n","        x_rsh1 = x_raw.permute(0, 2, 1, 3).contiguous()\n","        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n","        x_rsh2 = x_rsh1.view(n_batch, -1, self.d_feat)\n","        # x_rsh2: [n_batch, n_Q, d_feat]\n","\n","        # Linear\n","        x = self.lin_O(x_rsh2)\n","        # x: [n_batch, n_Q, d_feat]\n","        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n","               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n","               'scores':scores,'attention':attention,\n","               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\n","        return out\n","\n","# Self-Attention Layer\n","n_batch = 128\n","n_src = 32\n","d_feat = 200\n","n_head = 5\n","src = torch.rand(n_batch, n_src, d_feat)\n","self_attention = MultiHeadedAttention(\n","    d_feat=d_feat, n_head=n_head, actv=F.relu, USE_BIAS=True, dropout_p=0.1, device=device\n",")\n","out = self_attention.forward(src, src, src, mask=None)\n","\n","Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\n","Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\n","scores,attention = out['scores'],out['attention']\n","x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\n","\n","# Print out shapes\n","def sh(_x): return str(_x.shape)[11:-1] \n","print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\n","print ()\n","print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\n","print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\n","print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\n","print ()\n","print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\n","print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\n","print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\n","print ()\n","print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\n","print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\n","print ()\n","print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\n","print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\n","print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\n","print ()\n","print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Input src:\t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Q_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","K_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","V_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Q_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","K_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","V_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","\n","scores:   \t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n","attention:\t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n","\n","x_raw:    \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","x_rsh1:   \t[128, 32, 5, 40]  \t= [n_batch, n_src, n_head, d_head]\n","x_rsh2:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Output x: \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"351EL1GFKUb-"},"source":[""],"execution_count":null,"outputs":[]}]}