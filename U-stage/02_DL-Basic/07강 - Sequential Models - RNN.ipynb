{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07강 - Sequential Models - RNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPvZs725THB32fpqJ5OTJsO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4qZlBhHe3dJ2"},"source":["# 7. Sequential Models - RNN"]},{"cell_type":"markdown","metadata":{"id":"iZ7ZZUOa3_Ep"},"source":["## 7.1 Sequential Model\n","\n","- Sequential 데이터\n","  - 말, 음성, 비디오 등\n","- Sequential 데이터를 처리하는 데 있어서 가장 큰 어려움은 뭘까?\n","  - 얻고 싶은 것은 하나의 라벨(정보)일 때가 많다.\n","  - Sequential 데이터는 길이가 언제 끝날 지 모른다.\n","  - 즉, 내가 받아들여야 할 입력의 차원을 알 수 없다.\n","  - 그러므로 Fully connected layer 나 Convolution layer 를 사용할 수 없다."]},{"cell_type":"markdown","metadata":{"id":"qfiPIP_reHQY"},"source":["- Sequential Model\n","  - 입력이 여러 개 들어왔을 때 다음 번 입력에 대해 예측하는 것\n","  - 이전 데이터를 이용하여 다음 데이터를 예측\n","  - ex) Language Model"]},{"cell_type":"markdown","metadata":{"id":"sH_l5QTm4FTn"},"source":["<br>\n","\n","### 7.1.1 Naive sequence model\n","\n","- The number of inputs varies.\n","- 내가 고려해야 하는 과거의 정보량이 점점 늘어난다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1nzQuhnk3xj2u-UQMRRL2t8henoE5oeAA' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"6bVqOk_C4LGh"},"source":["<br>\n","\n","### 7.1.2 Autoregressive model\n","\n","- Fix the past timespan\n","- 입력의 길이를 알 수 없는 문제를 가장 간단히 해결할 수 있는 방법은 정해진 길이 만큼만 과거 정보를 보는 것이다.\n","- 이렇게 하면 훨씬 계산이 쉬워진다.\n","- 이런 것은 Autoregressive model 이라고 한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1UuDDUX6cwUMuqQqd7OwlhzyOEtSUFFFz' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"FYo0-raB4ToJ"},"source":["<br>\n","\n","### 7.1.3 Markov model (first-order autoregressive model)\n","\n","- 내가 가정하기에 현재는 과거에만 의존적(dependent)이다.\n","  - 과거 : 바로 전 과거\n","- Markov Property 는 과거의 많은 정보를 버릴 수 밖에 없다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=15cvKRt-tk4OKMUPkhsl1kc4aQgHMz41Z' width=700/>\n","\n","- Easy to express the joint distribution\n","- first-order autoregressive model 의 가장 큰 장점은 joint distribution 으로 표현하는 것이 쉬워진다.\n","  - generative model 에서 굉장히 많이 활용되는 개념이다."]},{"cell_type":"markdown","metadata":{"id":"hDJTsA8-4ez0"},"source":["<br>\n","\n","### 7.1.4 Latent autoregressive model\n","\n","- 이전의 모델들의 가장 큰 단점은 과거의 많은 정보들을 고려해야 하는 데 이 고려를 할 수 없다는 점이다.\n","- Latent autoregressive model 은 중간에 hidden state 가 들어 있다.\n","- 이 hidden state 가 과거의 정보를 요약하고 있다고 보는 것이다.\n","  - $h$ : summary of the past\n","- 다음 번 time step 은 이 hidden state 하나에만 dependent 하다.\n","  - output 입장에서 봤을 땐 하나의 과거 정보에만 dependent 하지만 이 하나의 과거와 과거 이전의 정보를 summarize 한 정보이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SYSB1rvqWhiIQvZ0bRMaBSVyX8TE_W8S' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"WCfmABbC4l3L"},"source":["<br>\n","\n","## 7.2 Recurrent Neural Network, RNN"]},{"cell_type":"markdown","metadata":{"id":"0px44kwB4pri"},"source":["### 7.2.1 RNN Basic\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1gogBrLcw2_nOMl7ygYgO_12adS6ddWiN' width=150/>\n","\n","- 자기 자신으로 돌아오는 구조가 하나 있다."]},{"cell_type":"markdown","metadata":{"id":"jF41D9Hh4scw"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1bCiYnhVO8iInhAGWu-tgf9OXpwfGJXZ5' width=600/>\n","\n","- RNN 을 보통 시간 순으로 풀어낸다고 말을 한다.\n","- Recurrent 구조가 있는 구조를 시간 순으로 풀게 되면 입력이 굉장히 많은 fully connected layer 로 표현될 수 있다."]},{"cell_type":"markdown","metadata":{"id":"_OvscJHh4tDF"},"source":["<br>\n","\n","### 7.2.2 Short-term dependencies\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rP00hOw8ihwxhFnkqu2d9bBqP7i56Grz' width=600/>\n","\n","- RNN은 Short-term dependencies 가 높다.\n","  - 가까운 과거에 있는 정보는 현재 고려가 잘 된다.\n"]},{"cell_type":"markdown","metadata":{"id":"K636HTSb41ns"},"source":["<br>\n","\n","### 7.2.3 **Long**-term dependencies\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=14VjndCzt4lzW5yiBwpLXtjByQoPPaNcG' width=600/>\n","\n","- RNN **Long**-term dependencies 가 낮다는 단점이 있다.\n","  - 과거에 얻어진 정보들이 다 취합(summarize)되서 미래에서 이를 고려해야 하는 데 RNN 은 하나의 fixed rule 로 이 정보들을 계속 취합하기 때문에 먼 과거에 있던 정보가 끝까지 살아남기가 힘들다.\n","  - 가까운 과거에 있는 정보는 현재 고려가 잘 되지만 먼 과거의 정보를 고려하기 힘들다."]},{"cell_type":"markdown","metadata":{"id":"5IQPb5me46J7"},"source":["<br>\n","\n","### 7.2.4 RNN 학습이 어려운 이유\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ZjhR4AhNUKSA6BMji_PqWtYIK_E-zXYT' width=600/>\n","\n","- 네트워크를 풀게 되면 네트워크의 width 가 매우 커진다.\n","- 위 수식과 같이 $h$ 에는 중첩되는 구조가 들어가는 것을 볼 수 있다.\n","- $h_0$ 가 $h_4$ 까지 가기 위해서는 똑같은 weight 를 곱하고 non-linear 를 통과해야 한다.\n","- non-linear function 이 **sigmoid** 라고 생각해보면 $h_0$ 에서 왔던 정보가 0에 가까워져서 의미가 없어진다.\n","  - vanishing gradient\n","- non-linear function 이 **ReLU** 인 경우 weight 가 양수인 경우 W 를 n번을 곱하게 되어 그 값이 매우 커져서 $h_0$ 의 값이 $h_4$ 에 매우 크게 반영된다.\n","  - exploding gradient"]},{"cell_type":"markdown","metadata":{"id":"AqotisyQ48RU"},"source":["<br>\n","\n","## 7.3 Long Short Term Memory, LSTM"]},{"cell_type":"markdown","metadata":{"id":"vbLuJ3Hx9Kb_"},"source":["### 7.3.1 Vanilla RNN 의 구조\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=15waKiptahyMTdPDtwMYOIs6HEpBiw4za' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"uLAR_wny5C90"},"source":["<br>\n","\n","### 7.3.2 LSTM 의 구조\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=13hAsiLhPprlkUZ7HZx4bPJVHU2FZ-ntJ' width=700/>"]},{"cell_type":"markdown","metadata":{"id":"OX04-jXb5DdL"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1gkVH0cbdoi0hONempUt1h3qWZ04vH_Qj' width=600/>\n","\n","- $X_t$\n","  - 입력 (input)\n","  - ex) 단어의 임베딩 벡터\n","- $h_t$\n","  - 출력 (output)\n","  - output 을 hidden state 라고 부른다.\n","- Previous cell state\n","  - cell 내부에서 위쪽 라인\n","  - 내부에서만 흘러가고 cell 밖으로 나가지 않는다.\n","  - 지금($t$)까지 흘러들어온 $t+1$개(0~t)의 정보를 다 취합해서 summarize 한 정보가 된다.\n","- Previous hidden state\n","  - 이전 cell 의 hidden state\n","- $\\sigma$\n","  - sigmoid"]},{"cell_type":"markdown","metadata":{"id":"O33IZ05uL7OF"},"source":["- LSTM 은 3개의 gate 로 이루어져 있다.\n","  - Forget gate\n","  - Input gate\n","  - Output gate"]},{"cell_type":"markdown","metadata":{"id":"DTPABarJ5FDj"},"source":["<br>\n","\n","### 7.3.3 Core idea\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1d3DiGApc1HMWNNr8uZ_K__sFC4vbvrqe' width=600/>\n","\n","- 중간에 흘러가는 cell state (왼쪽 그림)\n","  - timestep t 까지 들어온 정보를 summarize\n","  - 컨베이어 벨트처럼 볼 수 있다.\n","  - 컨베이어에서 어떤 걸 빼고 더할 지가 gate 에 해당한다."]},{"cell_type":"markdown","metadata":{"id":"pfS5Dq505ICz"},"source":["<br>\n","\n","### 7.3.4 LSTM's gate (3) & Update cell"]},{"cell_type":"markdown","metadata":{"id":"m4lAs3L-9WEY"},"source":["#### 7.3.4.1 Forget Gate\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1qDgX1qD7iG0fNjEWijlwNpU4AZgt717C' width=400/>\n","\n","$\n","\\qquad\n","f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)\n","$\n","\n","<br>\n","\n","- Decide which information to **throw** away\n","- 어떤 정보를 버릴 지를 결정한다.\n","- Forget gate 의 입력\n","  - $x_t$ : 현재의 입력\n","  - $h_{t-1}$ : 이전 cell 의 hidden state\n","- Forget gate 의 출력\n","  - $f_t$\n","  - sigmoid 를 통과하기 때문에 항상 0~1 사이의 값을 갖는다.\n","  - $f_t$ 는 이전의 cell state 에서 나온 정보 중 살릴 것과 버릴 것을 정하게 된다.\n","  - 이전의 출력값인 previous hidden state 와 현재의 입력을 가지고 weight 를 곱하고 activation 을 통과시켜 이전의 cell state 에서 넘어온 정보 중에서 버릴 것을 정한다."]},{"cell_type":"markdown","metadata":{"id":"6KZgjZtG5MVb"},"source":["<br>\n","\n","#### 7.3.4.2 Input Gate\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=10POKdEHm9SHeW94zXQ0ssfTgZHfy3np3' width=400/>\n","\n","$\n","\\qquad\n","\\begin{aligned}\n","i_{t} &=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\\n","\\tilde{C}_{t} &=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right)\n","\\end{aligned}\n","$\n","\n","<br>\n","\n","- Decide which information to **store** in the cell state\n","- 들어온 정보 중에 어떤 정보를 올릴 지를 결정한다."]},{"cell_type":"markdown","metadata":{"id":"Y3uErmnA5Z4p"},"source":["<br>\n","\n","#### 7.3.4.3 Update cell\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1FLA2bvHUP-AhmVrfM4YqBK9nwzsLZjHf' width=400/>\n","\n","$\n","\\qquad\n","\\begin{aligned}\n","i_{t} &=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\\n","C_{t} &=f_{t} * C_{t-1}+i_{t} * \\tilde{C}_{t}\n","\\end{aligned}\n","$\n","\n","<br>\n","\n","- Update the cell state.\n","- Forget Gate 와 Input Gate 에서 올라온 값과 이전 cell state 을 결합하여 새로운 cell state 계산"]},{"cell_type":"markdown","metadata":{"id":"zUSXyugG50I6"},"source":["<br>\n","\n","#### 7.3.4.4 Output Gate\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1qukJb8BOSak_YUNVfOfujbq-o-hndaxA' width=400/>\n","\n","$\n","\\qquad\n","\\begin{aligned}\n","o_{t} &=\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right) \\\\\n","h_{t} &=o_{t} * \\tanh \\left(C_{t}\\right)\n","\\end{aligned}\n","$\n","\n","<br>\n","\n","- Make output using the updated cell state.\n","- 생성된 값을 얼만큼 출력할 지를 결정한다."]},{"cell_type":"markdown","metadata":{"id":"yzUBvWjt5864"},"source":["<br>\n","\n","### 7.3.5 To summarize\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1y3O0AgNkQ2c3gdn-8UNsB6pSOjB8D1fn' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"61WGzDse6A2r"},"source":["<br>\n","\n","## 7.4 Gated Recurrent Unit, GRU\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1uGJHCQWnShZt3qy8_VVWINL4Wa46LMw3' width=500/>\n","\n","$\n","\\qquad\n","\\begin{aligned}\n","z_{t} &=\\sigma\\left(W_{z} \\cdot\\left[h_{t-1}, x_{t}\\right]\\right) \\\\\n","r_{t} &=\\sigma\\left(W_{r} \\cdot\\left[h_{t-1}, x_{t}\\right]\\right) \\\\\n","\\tilde{h}_{t} &=\\tanh \\left(W \\cdot\\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\\n","h_{t} &=\\left(1-z_{t}\\right) * h_{t-1}+z_{t} * \\tilde{h}_{t}\n","\\end{aligned}\n","$\n","\n","<br>\n","\n","- Simpler architecture with two gates (**reset gate** and **update gate**).\n","- gate 가 2개만 존재한다.\n","  - reset gate\n","    - LSTM 의 forget gate 와 유사한 기능을 수행\n","  - update gate\n","- No **cell state**, just **hidden state**.\n","- GRU 는 LSTM 과 달리 hidden state 가 곧 output 이고 동시에 다음 cell 로 들어간다.\n","  - cell state 가 없음\n","  - 이로 인해 output gate 가 필요 없음"]},{"cell_type":"markdown","metadata":{"id":"hTpoTr5W6JMo"},"source":["- 똑같은 task 에 대해 LSTM 보다 GRU 를 사용할 때 성능이 더 올라가는 것을 볼 수 있다.\n","  - LSTM 에 비해 GRU 의 파라미터가 적음"]},{"cell_type":"code","metadata":{"id":"frwxWG4xPvbV"},"source":[""],"execution_count":null,"outputs":[]}]}