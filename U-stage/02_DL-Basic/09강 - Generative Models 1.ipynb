{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09ê°• - Generative Models 1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNfJFOK0gJrLtLV9W2TNBmj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tOitH6XaNPo-"},"source":["# 9. Generative Models 1"]},{"cell_type":"markdown","metadata":{"id":"1Ni-f0mKNUE9"},"source":["## 9.1 Introduction\n","\n","> What does it mean to learn a **generative model**?"]},{"cell_type":"markdown","metadata":{"id":"NYjEvOOgsBBW"},"source":["<br>\n","\n","## 9.2 Learning a Generative Model\n","\n","- Suppose we are given images of dogs.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_kgakInGQBBg6IPNOfbZuO4Sp4BiBd3R' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"aiUkAooxsNUI"},"source":["<br>\n","\n","### 9.2.1 learn $p(x)$\n","\n","- We want to learn a probability distribution $p(x)$ such that\n","  - Generation\n","  - Density estimation\n","  - Unsupervisesd representation learning"]},{"cell_type":"markdown","metadata":{"id":"Y-kYWrw8shV9"},"source":["<br>\n","\n","#### 9.2.1.1 Generation\n","\n","- If we sample $x_{\\text {new }} \\sim p(x)$, $x_{\\text {new }}$ should like a dog.\n","- **sampling**"]},{"cell_type":"markdown","metadata":{"id":"x-MFjELMsvrV"},"source":["<br>\n","\n","#### 9.2.1.2 Density estimation\n","\n","- $p(x)$ should be high if $x$ looks like a dog, and low otherwise.\n","- **anomaly detection**\n","- Also known as, **explicit** models"]},{"cell_type":"markdown","metadata":{"id":"UxtX8D1-tCMd"},"source":["<br>\n","\n","#### 9.2.1.3 Unsupervisesd representation learning\n","\n","- We should be able to learn what these images have in common, e.g., ears, tail, etc\n","- **feature learning**"]},{"cell_type":"markdown","metadata":{"id":"ixWu16-ctSXO"},"source":["<br>\n","\n","### 9.2.2 **represent** $p(x)$\n","\n","- Then, how can we **represent $p(x)$?"]},{"cell_type":"markdown","metadata":{"id":"vRLT5z36tkOt"},"source":["<br>\n","\n","## 9.3 Basic Discrete Distributions"]},{"cell_type":"markdown","metadata":{"id":"Bv39GIfEtp3V"},"source":["### 9.3.1 Bernoulli distribution\n","\n","- (biased) coin flip\n","- $D=\\{$ Heads, Tails $\\}$\n","- Specify $P(X=$ Heads $)=p$. Then $P(X=$ Tails $)=1-p$\n","- Write: $X \\sim \\operatorname{Ber}(p)$"]},{"cell_type":"markdown","metadata":{"id":"RkeSza9juIu2"},"source":["<br>\n","\n","### 9.3.2 Categorical distribution\n","\n","- (biased) m-sided dice\n","- $D = \\{ 1, \\dots, m \\}$\n","- Specify $P(Y = i) = p_i$, such that $\\sum_{i=1}^m p_i = 1$\n","- Write: $Y \\sim Cat(p_1, \\dots, p_m)$"]},{"cell_type":"markdown","metadata":{"id":"-wJtf4JAu5_x"},"source":["<br>\n","\n","## 9.4 Example"]},{"cell_type":"markdown","metadata":{"id":"_-5WyZA1vdrp"},"source":["### 9.4.1 RGB image\n","\n","- Modeling an RGB joint distribution (of a single pixel)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1RfoFXWwkV-hSx7Byc6KEk5fhYHbkVz2r' width=300/>\n","\n","- $(r, g, b) \\sim p(R, G, B)$\n","- Number  of cases?\n","  - 256 x 256 x 256\n","- How many parameters do we need to specify?\n","  - 256 x 256 x 256 - 1"]},{"cell_type":"markdown","metadata":{"id":"oWx6lTc8vIPm"},"source":["<br>\n","\n","### 9.4.2 binary image\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=13q5GXS8WRCNUNB5YK09PcxFJ0muyPlAy' width=300/>\n","\n","- Suppose we have $X_{1}, \\ldots, X_{n}$ of $n$ binary pixels (a binary image).\n","- How many possible states?\n","  - $2 \\times 2 \\times \\dots \\times 2 = 2^n$\n","- Sampling from $p(x_1, \\dots, x_n)$ generates and image.\n","- How many parameters to specify $p(x_1, \\dots, x_n)$?\n","  - $2^n - 1$"]},{"cell_type":"markdown","metadata":{"id":"BgDWBhW1v3cB"},"source":["<br>\n","\n","## 9.5 Structure Through Independence\n","\n","- What if $X_1, dots, X_n$ are independent, then\n","  - $p\\left(x_{1}, \\ldots, x_{n}\\right)=p\\left(x_{1}\\right) p\\left(x_{2}\\right) \\cdots p\\left(x_{n}\\right)$\n","- How many possible states?\n","  - $2^n$\n","- How many parameters to specify $p(x_1, \\dots, x_n)$?\n","  - $n$\n","\n","<br>\n","\n","- $2^n$ entries can be described by just $n$ numbers!\n","- But this **independence** assumption is too strong to model useful distributions."]},{"cell_type":"markdown","metadata":{"id":"lOgqjs_Rw8Yn"},"source":["<br>\n","\n","## 9.6 Conditional Independence\n","\n","- Three important rules\n","  - Chain rule\n","  - Bayes' rule\n","  - Conditional independence"]},{"cell_type":"markdown","metadata":{"id":"GcQzdVF_xHtl"},"source":["<br>\n","\n","### 9.6.1 Chain rule\n","\n","$$p\\left(x_{1}, \\ldots, x_{n}\\right)=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) p\\left(x_{3} \\mid x_{1}, x_{2}\\right) \\cdots p\\left(x_{n} \\mid x_{1}, \\cdots, x_{n-1}\\right)$$"]},{"cell_type":"markdown","metadata":{"id":"hdfMLljxxOOm"},"source":["<br>\n","\n","### 9.6.2 Bayes' rule\n","\n","$$\n","p(x \\mid y)=\\frac{p(x, y)}{p(y)}=\\frac{p(y \\mid x) p(x)}{p(y)}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"ZdcrB2w5xUVz"},"source":["<br>\n","\n","### 9.6.3 Conditional independence\n","\n","$$\n","\\text { If } x \\perp y \\mid z \\text { , then } p(x \\mid y, z)=p(x \\mid z)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"kRjuQD4Axaqv"},"source":["<br>\n","\n","### 9.6.4 Using the chain rule\n","\n","- Using the chain rule,\n","  - $p\\left(x_{1}, \\ldots, x_{n}\\right)=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) p\\left(x_{3} \\mid x_{1}, x_{2}\\right) \\cdots p\\left(x_{n} \\mid x_{1}, \\cdots, x_{n-1}\\right)$\n","- How many parameters?\n","  - $p(x_1)$: 1 parameter\n","  - $p(x_2 | x_1)$: 2 parameters\n","    - one per $p(x_2|x_1 = 0)$ and one per $p(x_2|x_1 = 1)$\n","  - $p(x_3 | x_1, x_2)$: 4 parameters\n","  - Hence, $1 + 2 + 2^2 + \\cdots + 2^{n-1} = 2^n - 1$, which is the same as before.\n","- Why?"]},{"cell_type":"markdown","metadata":{"id":"1Xe9tDCixqFG"},"source":["<br>\n","\n","### 9.6.5 Markov assumption\n","\n","- Now, suppose $X_{i+1} \\perp X_{1}, \\ldots, X_{i-1} \\mid X_{i}$ (Markov assumption), then\n","  - $p\\left(x_{1}, \\ldots, x_{n}\\right)=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) p\\left(x_{3} \\mid x_{2}\\right) \\cdots p\\left(x_{n} \\mid x_{n-1}\\right)$\n","- How many parameters?\n","  - $2n - 1$\n","- Hence, by leveraging the Markov assumption, we get exponential reduction on the number of parameters.\n","\n","<br>\n","\n","- **Auto-regressive models** leverage this conditional independency."]},{"cell_type":"markdown","metadata":{"id":"UE9ikNwbyqkX"},"source":["<br>\n","\n","## 9.7 Auto-regressive Model\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1JaCmxJYv_rEv1dTOjKoi1QXhA9woJkdx' width=600/>\n","\n","- Suppose we have $28 \\times 28$ binary pixelss.\n","- Out goal is to learn $p(x)=p\\left(x_{1}, \\ldots, x_{784}\\right)$ over $x \\in\\{0,1\\}^{784}$.\n","- How can we parametrize $p(x)$?\n","  - Let's use the **chain rule** to factor the joint distribution.\n","  - $p\\left(x_{1: 784}\\right)=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) p\\left(x_{3} \\mid x_{1: 2}\\right) \\cdots$\n","  - This is called an **autoregressive model**.\n","  - Note that we need **an ordering** of all random variables."]},{"cell_type":"markdown","metadata":{"id":"aSMtS4Bj2Nxw"},"source":["<br>\n","\n","## 9.8 NADE: Neural Autoregressive Density Estimator\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1XDQtG3RqWQRf_3QX82LbAcvHH7aryaMh' width=800/>\n","\n","- The probability distribution of $i$-th pixel is\n","  - $p\\left(x_{i} \\mid x_{1: i-1}\\right)=\\sigma\\left(\\alpha_{i} \\mathbf{h}_{i}+b_{i}\\right)$ where $\\mathbf{h}_{i}=\\sigma\\left(W_{<i} x_{1: i-1}+\\mathbf{c}\\right)$"]},{"cell_type":"markdown","metadata":{"id":"v0zFXfqD2hHW"},"source":["- **NADE** is an **explicit** model that can compute the **density** of the given inputs.\n","- How can we compute the **density** of the given image?\n","  - Suppose we have a binary image with 784 binary pixels, $\\{x_1, x_2, \\dots, x_{784}\\}$\n","  - Then, the joint probability is computed by\n","    - $p\\left(x_{1}, \\ldots, x_{784}\\right)=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) \\cdots p\\left(x_{784} \\mid x_{1: 783}\\right)$\n","    - where each conditional probability $p\\left(x_{i} \\mid x_{1: i-1}\\right)$ is computed independently.\n","- In case of modeling continuous random variables, **a mixture of Gaussian** can be used."]},{"cell_type":"markdown","metadata":{"id":"4RXj7rTh3QT3"},"source":["<br>\n","\n","## 9.9 Pixel RNN\n","\n","- We can also use **RNNs** to define an auto-regressive model.\n","- For example, for an $n \\times n$ RGB images,\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1xAszapTNxUesQ7FPyiWMuV4dZR-ZI0gn' width=600/>\n","\n","<br>\n","\n","- There are two model architectures in Pixel RNN based on the **ordering** of chain\n","  - Row LSTM\n","  -  Diagonal BiLSTM\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1USBlyWv3tiQn1TIouTayi73SBswNuost' width=600/>"]},{"cell_type":"code","metadata":{"id":"6yp0X7RD3qYW"},"source":[""],"execution_count":null,"outputs":[]}]}