{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08강 - Sequential Models - Transformer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP+Dhp++BylcY65Fdrt86LM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"C0vJ9AH_-0nd"},"source":["# 8. Sequential Models - Transformer"]},{"cell_type":"markdown","metadata":{"id":"NDK9cmYQ-31x"},"source":["## 8.1 Sequential Model\n","\n","- What makes sequential modeling a hard problem to handle?\n","  - Trimed : 잘린\n","  - Omitted : 생략된\n","  - Permuted : 재배치된\n","- 문장은 길이가 달라질 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1iFo16lZd8MiNmoqwYANotkPtz73FiVDn' width=600/>\n"]},{"cell_type":"markdown","metadata":{"id":"KsmQZgbH_Y7N"},"source":["<br>\n","\n","## 8.2 Transformer\n","\n","> Attentioni is All You Need, NIPS, 2017\n","\n","- [paper](https://arxiv.org/abs/1706.03762)"]},{"cell_type":"markdown","metadata":{"id":"xjHkR7xs_fIw"},"source":["<br>\n","\n","### 8.2.1 Transformer architecture\n","\n","- **Transformer** is the first sequence transduction(변환) model based entirely on attention.\n","- Transformer 는 RNN 과 같은 재귀적인 구조가 없고 attention 구조를 활용한 시퀀스 변환 모델이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rsXed2MgB6iaj4Z8S6hKNkTXceFfRxqT' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"TtYPV5bb_rUJ"},"source":["<br>\n","\n","- From a bird's-eye view, this is what the **Transformer** does for machine translation tasks.\n","- 기계어 번역(NMT) 문제에 Transformer 가 어떻게 적용되었는 지 살펴본다.\n","  - 이 방법론은 sequential 한 데이터를 처리하고 이 데이터를 encoding 하는 방법이기 때문에 NMT 문제에만 적용되지는 않는다.\n","  - Transformer 의 활용 분야는 다음과 같다.\n","    - 이미지 분류, ViT\n","    - 이미지 detection\n","    - DALL-E : 문장 -> 이미지 (GPT-3 기반)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=14rqMbFhtuMYxcqVaZtiP85suPJILE40C' width=700/>\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"6uXBEqJoAO0Q"},"source":["<br>\n","\n","- If we glide down a little bit, this is what the **Transformer** does.\n","- Transformer 가 하고자 하는 것은 어떤 불어 문장이 주어지면 영어 문장으로 바꾸는 것이다.\n","  - 이러한 프로세스를 sequence to sequence 라고 한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1nDWGUH7Sy9k6DkgWeSoW5Nric4RdVyme' width=500/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"RUhA_7drAnoU"},"source":["<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1eqiSd3dS6xfG3e08uQiJ7Y52R5dII-VK' width=700/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))\n","\n","<br>\n","\n","- 입력은 3개의 단어로 되어 있고 출력은 4개의 단어로 되어 있다.\n","  - 입력 sequence 와 출력 sequence 의 단어 숫자는 다를 수 있다.\n","  - 또한 입력 sequence 와 출력 sequence 의 도메인(불어, 영어)도 다를 수 있다.\n","  - 모델은 하나의 모델이다.\n","- Transformer 의 encoder 부분은 입력되는 단어의 갯수에 상관없이 한 번에 인코딩할 수 있다.\n","  - encoder 부분의 self-attention 이라는 구조는 한 번에 N개의 단어를 처리할 수 있다.\n","- decoder 에서 문장을 generation 할 때는 autoregressive 한다.\n","\n","<br>\n","\n","- Six identical (but not shared) **encoders** and **decoders** are stacked.\n","- 각각 동일한 구조를 갖지만 네트워크 파라미터가 다르게 학습되는 encoder 와 decoder 가 쌓여 있는 구조로 되어 있다.\n","\n","<br>\n","\n","- What is the information being sent from **encoders** to **decoders**?\n","- 먼저 N개의 단어가 어떻게 encoder 에서 한 번에 처리되는 지를 이해해야 한다.\n","- 또한 encoder 와 decoder 사이에 어떤 정보를 주고 받는 지를 이해해야 한다.\n","- 마지막으로 decoder 가 어떻게 generation 을 할 수 있는 지를 이해해야 한다.\n","  - 이 부분은 이번 강의에서 많이 다루지 않는다."]},{"cell_type":"markdown","metadata":{"id":"FIOCJpAvA2oI"},"source":["<br>\n","\n","### 8.2.2 Self-Attention\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1J_830KjYEcyrr8GqkOKtPvII7m4tII9i' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))\n","\n","- N개의 단어(임베딩 벡터)가 한 번에 들어간다.\n","- 하나의 encoder 는 self-attention 구조와 FFNN 구조를 각각 거치게 된다.\n","- encoder 의 N개의 출력값이 그 다음 encoder 의 입력으로 들어간다.\n","\n","<br>\n","\n","- The **Self-Attention** in both encoder and decoder is the cornerstone(초석) of Transformer.\n","- Self-Attention 이 Transformer 가 좋은 성능을 나타내게된 핵심 구조이다."]},{"cell_type":"markdown","metadata":{"id":"VJOTadV9BJAn"},"source":["<br>\n","\n","### 8.2.3 Self-Attention & FFNN\n","\n","- First, We represent each word with some embedding vectors.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SwDECfRymwico6ywzyPyj1glYUg28u48' width=600/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))\n","\n","- 3개의 불어 단어가 들어가고 각 단어 마다 특정한 벡터로 표현하게 된다."]},{"cell_type":"markdown","metadata":{"id":"UlUgxmp1BYKQ"},"source":["<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1eX_uL96oT5fWrqDuhe7yBTc0IFXRSPpL' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))\n","\n","<br>\n","\n","- Then, Transformer encodes each word to feature vectors with **Self-Attention**.\n","- Self-Attention 은 3개의 단어 벡터가 주어지면 이를 3개의 벡터로 바꿔주는 역할을 한다.\n","  - N개가 들어오면 N개가 나온다.\n","- Self-Attention 은 하나의 벡터($z_1$)를 만들 때 단순히 $x_1$ 만 사용하는 것이 아니라 $x_2$, $x_3$ 의 정보를 같이 활용한다.\n","\n","<br>\n","\n","- Self-Attention's paths have **dependencies**.\n","- 그렇기 때문에 Self-Attention 는 dependency 가 존재한다.\n","\n","<br>\n","\n","- Feed-forward paths are word independent, and parallelized.\n","- FFNN 은 dependency 가 존재하지 않는다.\n","  - 각각의 $z_1$, $z_2$, $z_3$ 가 동일한 FFNN 을 통과하여 출력이 된다."]},{"cell_type":"markdown","metadata":{"id":"WM_V_iHuBfhQ"},"source":["<br>\n","\n","### 8.2.4 Self-Attention Process"]},{"cell_type":"markdown","metadata":{"id":"n-O9wNflH8rR"},"source":["#### 8.2.4.1 Input words\n","\n","- Suppose we are encoding two words:\n","  - **Thinking** and **Machines**.\n","- 좀 더 문제를 단순하게 만들기 위해서 **\"Thinking\"** 와 **\"Machines\"** 라는 2개의 단어만 주어졌다고 가정하자.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1D_uBQI_rTXaKgG2x21xvHbTQoT9CzAd5' width=700/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"aW89-fstB-LI"},"source":["<br>\n","\n","#### 8.2.4.2 Relationship between words\n","\n","- **Self-Attention** at a high level\n","  - \"The animal didn't cross the street becuase **it** was too tired.\"\n","    - What is **\"it\"** referring to?\n","\n","<br>\n","\n","- 다음과 같은 문장이 있다고 하자.\n","  - \"The animal didn't cross the street becuase **it** was too tired.\"\n","- 여기서 **\"it\"** 은 문장에 등장하는 어떤 단어에 dependent 할까?\n","- 즉, 하나의 문장에 있는 단어를 설명할 때는 단어 그 자체로만 이해하는 것이 아닌 문장 속에서 다른 단어들과 어떤 interaction 이 있는 지를 알아야 한다.\n","\n","<br>\n","\n","- Transformer 가 \"it\" 이라는 단어를 encoding 할 때 다른 단어들과의 관계성을 알아서 학습하게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1YkP81NjixXA_Q0ytU7iNAASmQF6MzZWS' width=400/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"_DAtWap9CUKv"},"source":["<br>\n","\n","#### 8.2.4.3 Query, Key, Value vector\n","\n","- Self-attention 구조는 3가지 벡터(Query, Key, Value)를 만들어 낸다.\n","  - 3개의 NN 이 있다고 보면 된다.\n","- **Query, Key, Value** vectors are computed per each word(=embedding).\n","- 각 단어 마다 q, k, v 벡터를 갖게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1skYkey7CpwUVvS2z3A9U7TwgUSxwcll7' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"QLPAc380Cdf3"},"source":["<br>\n","\n","- Suppose we are encoding the first word: **'Thinking'** given 'Thinking' and 'Machines'.\n","- Suppose that we want to **encode** the first word **'Thinking'** given two words.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ErHb3eb6YM8r3adzpzsgZ-9x9oyacGPD' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"ZM8JmvK_CvQ3"},"source":["<br>\n","\n","#### 8.2.4.4 Score\n","\n","- Then, we need to compute the **score** per each word.\n","- 먼저 단어에 대한 score 를 생성한다.\n","  - 내가 인코딩 하고자 하는 단어의 query 벡터($q_1$)와 나머지 모든 단어의 key 벡터들($k_1, k_2$)을 구한다.\n","  - 이 둘을 각각 내적을 한다. ($q_1 \\cdot k_1$ 와 $q_1 \\cdot k_2$)\n","  - 다시 말하면 이 두 벡터가 얼마나 align 이 되어 있는 지를 보는 것이다.\n","  - i번째 단어가 자신을 포함한 다른 단어 N 개와 얼마나 유사도를 갖는 지를 보는 것이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1DDEMprWB0xdYJq0nqTQq5SXrn514GkYz' width=600/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"jUY0CCAHC-i_"},"source":["<br>\n","\n","- When encoding the first word, the corresponding(해당하는) **query** vector is used.\n","- 이렇게 내적을 한 score 는 i 번째 단어와 나머지 단어들 사이의 얼마나 interaction 을 해야 하는 지를 알아서 학습하게 하는 것이다.\n","  - 이 것이 바로 attention 이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1y5vAHVyfv8Jni7zxp8mgfHReVTjq-1N4' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"OrtONDvfDPbo"},"source":["<br>\n","\n","- We compute scores of all given words where the corresponding **key vectors** are used. ($k_1$)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_G_v34TG-QGbLDxMNVMZJoUE29BQ5PYs' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"jZD1_G2EDYOP"},"source":["<br>\n","\n","- We compute scores of all given words where the corresponding key vectors are used. ($k_2$)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1YmwMYBZGAxOI99T-NlCoqPcxCzBmwqrB' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"x8dGnepmDgIX"},"source":["<br>\n","\n","- We compute the **scores** of each word with respect to **'Thinking'**.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Gdk10mj-xBImXG46PWvAwiHNM1r2VH61' width=600/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"cIvzFd8LDp3I"},"source":["<br>\n","\n","#### 8.2.4.5 Normalize & Softmax\n","\n","- Then, we compute the **attention weights** by scaling followoed by softmax.\n","- 계산된 score 들을 normalize 해준다.\n","  - 먼저 score 를 $\\sqrt{d_k}$ 로 나눠준다.\n","    - $d_k$: Key vector 의 dimension\n","    - Key vector 의 dimension 은 Query vector 의 dimension 와 동일하다.\n","  - 이 과정은 socre 가 너무 크지 않도록 조정해주는 것이다.\n","- 그런 다음 score 에 softmax 를 취해한다.\n","- 그 결과 특정 단어와 자기 자신을 포함한 다른 단어들 간의 attention 에 대한 interaction 값인 **attention weights** 을 얻을 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1a_QHLlzfOsdbNSm6_ZtjCH20jKKHDfNi' width=600/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"8qpcVbVJDv54"},"source":["<br>\n","\n","#### 8.2.4.6 Self-attention output\n","\n","- The final encoding is done by the weighted sum of the **value** vectors.\n","- 각각의 단어에서 나오는 value vector 에 대해 위에서 구한 softmax 를 취한 값을 곱하여 다 합하면 Self-attention 의 output 인 단어에 대한 encoding vector z 를 얻게 된다.\n","  - 결국 query vector 와 key vector 를 내적하여 score 를 구하고, normalize 및 softmax 를 취하여 나온 attention 이 value vector 의 weight 가 되는 것이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1QGTt2uuR1W-RN9viAG3aQoTQnpaO-9i2' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"s-GtXwejLpsB"},"source":["<br>\n","\n","#### 8.2.4.7 Vector's dimension\n","\n","- Query vector 와 Key vector 의 차원은 같아야 한다.\n","  - 내적을 해야 하기 때문\n","- 하지만 value vector 는 차원이 달라도 된다.\n","  - weight sum 만 하면 되기 때문\n","- 단어에 대한 encoding vector 의 차원은 value vector 와 동일하다.\n","  - 이는 multi-head attention 에서는 달라진다."]},{"cell_type":"markdown","metadata":{"id":"BxdgQ6qmD1Pn"},"source":["<br>\n","\n","#### 8.2.4.8 Calculating with Matrix\n","\n","- Calculating Q, K, and V from X in a matrix form.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1PorJT1rmsOzbQHxzsepiI0YAkRz5pHus' width=700/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"2GmnS0IwEEK6"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_oYGOAYRLGi_2JWRfgN8tnDsGr4fwiAP' width=700/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"ZzNSAKWYEGc_"},"source":["<br>\n","\n","### 8.2.5 Multi-headed attention (MHA)\n","\n","- Multi-headed attention 은 앞에서 살펴본 self-attention 을 여러 번 한 것이다.\n","  - Q, K, V 벡터를 N개 만든다.\n","- Multi-headed attention (MHA) allows Transformer to focus on different positions.\n","- N개의 다른 attention 을 수행하게 되면 N개의 encoding vector 를 얻을 수 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1JeZT4z8p6hCivdYiaeQp4zUjLceRps9s' width=600/>\n","\n","- (출처: [http://jalammar.\n","github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"BWu_ADZJEVXX"},"source":["<br>\n","\n","- 논문에서는 8개의 head 가 사용됐다.\n","- If eight heads are used, we end up getting eight different sets of encoded vectors (**attention heads**).\n","- 하나의 임베딩된 입력 벡터가 있을 때 8개의 encoding 된 벡터가 나오게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1FwQNYbJWt_471lFMX4pVsMbsOgO-v4OH' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))\n","\n","<br>\n","\n","- But, how do we feed eight sets of attention head to the next layer?\n","- 다음 번 encoder 로 넘어가기 위해 입력과 출력의 차원을 맞춰줘야 한다."]},{"cell_type":"markdown","metadata":{"id":"_eLXyjbFEd7f"},"source":["<br>\n","\n","- 8개의 head 에서 나온 임베딩 벡터들을 concatenate 하고 크기가 (attention의 출력 dim x 8, 입력 벡터의 dim)인 가중치 행렬을 행렬곱하여 입력과 출력의 차원을 맞춰준다.\n","- We simply pass them through additional (learnable) linear map.\n","  - Concatenate all the attention heads\n","  - Multiply with a weight matrix $W^O$ that was trained jointly with the model\n","  - The result would be the $Z$ matrix that captures information from all the attention heads. We can send this forward to the FFNN.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SqgJ0bXTi9k1M0B6vditXeLS7OqMzD7k' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"fYzF1pz-EpzP"},"source":["<br>\n","\n","### 8.2.6 Attention Summary\n","\n","1. This is our input sentence*.\n","  - \"Thinking Machines.\"\n","2. We embed each word*.\n","3. Split into 8 heads. We multiply $X$ or $R$ with weight matrices.\n","4. Calculate attention using the resulting $Q/K/V$ matrices.\n","5. Concatenate the resulting $Z$ matrices, then multiply with weight matrix $W^O$ to produce the output of the layer.\n","\n","<br>\n","\n","- In all encoders other than #0, we don't need embedding.\n","- We start directly with the output of the encoder right below thtis one.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1UtazAK2iTz7mbDYURWbiuGlkaleE_Zhl' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"EnxIOrZbEr1i"},"source":["<br>\n","\n","### 8.2.7 Positional Encoding\n","\n","- [참고자료](https://inmoonlight.github.io/2020/01/26/Positional-Encoding/)\n","\n","- 입력에 특정 값을 더해준다. (bias)\n","\n","<br>\n","\n","- Why do we need positional encoding?\n","  - Positional encodings are **added** to the original embedding.\n","- 왜 positional encoding 이 필요할까?\n","- 사실 입력된 N개의 단어에 대한 sequential 한 정보가 포함되어 있지 않다.\n","  - order independent\n","- 그로 인해 positional encoding 이 필요하게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Chgai1v-4Fng4ixaLoderAK_Q5vDBxp5' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"zc0OfSIoE1vg"},"source":["<br>\n","\n","- This is the case for 4-dimensional encoding.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1C6YnuHiLwZRiyBmRqipadCbVK6tt55_U' width=600/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"826D9AdAE6X_"},"source":["<br>\n","\n","- This is the case for 512-dimensional encoding.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1cudJsqzAFfwG7hjOH03aBgld1d0CzR2u' width=700/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"JckNJMVHE_nX"},"source":["<br>\n","\n","- Recent (July, 2020) update on positional encoding.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1RlGUljE8wIj7PHPscK98VlO7W-z_tCiV' width=700/>\n","\n","(출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"nVmFhhfOUuO1"},"source":["<br>\n","\n","### 8.2.8 Add & Normalize layer\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=16uO3Bgh8gGJBBCaxM2RWmKKX2KEAR4Fe' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"gERD5mClFEqX"},"source":["<br>\n","\n","### 8.2.9 Decoder Process\n","\n","- Now, let's take a look at the **decoder** side.\n","- What is the information being sent from **encoders** to **decoders**?\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1uAKGXm55nJhWEHdG31hq72sgBdYp7Q1l' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"pOs0oxIjFPn4"},"source":["<br>\n","\n","- **Transformer** transfers **key (K)** and **value (V)** of the topmost encoder to the decoder.\n","- Transformer 는 decoder 로 Key 와 Value 를 보낸다.\n","  - input 에 있는 단어들을 출력하고자 하는 단어들에 대해 attention map 을 만들려면 key 와 value vector 가 필요하다. (self-attention 을 만드는 과정을 거꾸로 한다고 생각)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1oPRwMgkJO8G2mgQaZIed7Ectaw_r3eb8' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"GQfoyL1KFjsQ"},"source":["<br>\n","\n","- The output sequence is generated in an autoregressive manner.\n","- 디코더에 들어가는 단어들의 query vector 와 encoder 에서 넘어온 key, value 벡터를 가지고 최종값이 나오게 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Vtua5t4durb7-_Cj_7Ekm3TAs_h9Tjiy' width=800/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"38yWR8kcFpeh"},"source":["<br>\n","\n","- In the **decoder**, the **self-attention layer** is only allowed to attend to earlier positions in the output sequence which is done by masking future positions before the softmax step.\n","- 학습할 때는 입력과 정답인 출력을 알고 있다.\n","- 정답을 다 알고 있으면 의미가 없으므로 중간에 masking 을 해준다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_n6h7of8qescLf2GYAnfdwc_b3M-ExrT' width=800/>\n","\n","(출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"__PP-95bF1Cp"},"source":["<br>\n","\n","- The **\"Encoder-Decoder Attention\"** layer works just like multi-headed self-attention, except it creates its **Queries** matrix from **the layer below it**, and takes thte **Keys** and **Values** from the encoder stack.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1RmMeKD246nqaITS2JPg6QjMCzKAd72tX' width=800/>\n","\n","(출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"-JAYsvtyGI1X"},"source":["<br>\n","\n","### 8.2.10 Final layer\n","\n","- The final layer converts the stack of decoder outputs to the distribution over words.\n","- 단어드의 분포를 만들어서 그 중 단어 하나를 샘플링하는 식으로 동작한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1ZUdsB6UIViNCkmfNFODwoU_JEEWTKTdi' width=600/>\n","\n","- (출처: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"]},{"cell_type":"markdown","metadata":{"id":"v03ZYUd3GOs3"},"source":["<br>\n","\n","## 8.3 Vision Transformer (ViT)\n","\n","- [paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1P9wEe77PaeNU_i2r6buvo6YRrDUqypA-' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"bZH2DMmIGah3"},"source":["<br>\n","\n","## 8.4 DALL-E\n","\n","- [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)\n","\n","<br>\n","\n","- An armchair in the shape of an avocado.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1c3sqzNsx-jUf-AEyXrspidR6xHJizAsy' width=400/>\n","\n"]},{"cell_type":"code","metadata":{"id":"-_6dlWhyGp7o"},"source":[""],"execution_count":null,"outputs":[]}]}