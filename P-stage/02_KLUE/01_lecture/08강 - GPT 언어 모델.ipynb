{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08강 - GPT 언어 모델.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPUskzyM2N+HLWTjng0plGH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pi5izsguhs2b"},"source":["# 8. GPT 언어 모델"]},{"cell_type":"markdown","metadata":{"id":"rPFR84EAhxU_"},"source":["## 강의 소개\n","\n","GPT는 Generation Pre-trained Transformer의 약자로 OpenAI에서 공개한 모델입니다. 😄\n","\n","BERT와 가장 큰 차이점은 GPT는 단일 방향으로 주어진 단어 다음에 가장 올 확률이 높은 단어를 예측하는 모델입니다.🤣\n","\n","Generation 분야에 강점을 가지고 있는 모델입니다."]},{"cell_type":"markdown","metadata":{"id":"D84nCnN6h2k3"},"source":["<br>\n","\n","## Reference\n","\n","- GPT\n","  - [1. Paper : Language Understanding](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) \n","- GPT-2\n","  - [1. Paper : Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n","- GPT-3\n","  - [1. Paper : Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n","- 언어 모델의 학습법\n","  - [1. Week 19 - 언어 모델을 가지고 트럼프 봇 만들기?!](https://jiho-ml.com/weekly-nlp-19/)"]},{"cell_type":"markdown","metadata":{"id":"26Byw-fJiB0_"},"source":["<br>\n","\n","## 8.1 GPT 모델 소개"]},{"cell_type":"markdown","metadata":{"id":"RjWofcmXiIJp"},"source":["### 8.1.1 GPT-1\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WPyBfDG21XF5TGavzXJdx7a-dC-kxM33' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"STHSGrmKiJgy"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WXXy3rXqsn0DSNfX7dS0mo0uqWDMSwaf' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"vBpvMgEBiVW1"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WXmAQFCXp4mO_xLA62xB4lwqLepW6PMn' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"uUiifU9uiV8h"},"source":["- [자연어 문장 -> 분류] 성능이 아주 좋은 디코더인 GPT\n","- 덕분에 적은 양의 데이터에서도 높은 분류 성능을 나타냄\n","- 다양한 자연어 task에서 SOTA 달성\n","- Pre-train 언어 모델의 새 지평을 열었음 -> BERT로 발전의 밑거름\n","\n","<br>\n","\n","- 하지만 여전히, 지도 학습을 필요로 하며, labeled data가 필수임\n","- 특정 task를 위해 fine-tuning 된 모델은 다른 task에서 사용 불가능\n","\n","<br>\n","\n","- \"언어\"의 특성 상, 지도학습의 목적 함수는 비지도 학습의 목적함수와 같다!\n","  - fine-tuning에서 사용되는 label 자체도 하나의 언어이다.\n","  - fine-tuning? 필요 없다!!!"]},{"cell_type":"markdown","metadata":{"id":"CFOlA9ewibG_"},"source":["<br>\n","\n","- 엄청 큰 데이터셋을 사용하면 자연어 task를 자연스럽게 학습\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WYxmAKBwVX7kanpjhpgAwNSgtH1ZgHz-' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"3cPuY6FeiyR0"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WlN7oouihzj4FhGoF2Ri-a3ecwTbh4AO' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"h00zibSCi3uJ"},"source":["<br>\n","\n","### 8.1.2 GPT-2\n","\n","- 인간은 새로운 task 학습을 위해 수많은 데이터를 필요로 하지 않는다.\n","- pre-train model -> fine-tuning으로 한 모델이 하나의 task만 수행 가능한 건 바보같은 일이다!\n","- 그래서 제안한 방법이 zero-shot, one-shot, few-shot learning 이다\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WnqfzPv2fhM3XXJrkoSovoJCpO9IQMnl' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"PuoB35KSjFri"},"source":["- GPT-2는 GPT-1에 비해 더 큰 사이즈의 데이터를 사용했다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Wob22tLdy22ZZKQfzpR3p2Hvxd2_2x45' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"4kYt_mS1jMxr"},"source":["- GPT-2의 디코더 구조는 GPT-1의 디코더 구조와 조금 다르게 구성되어 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WsPT1FSo3gE5wpNYGLHXgbx2H94dZiuj' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"eUad--1CjUab"},"source":["<br>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1WtGOFBq0h1UBhgFJuMEcQcqbPDCPJ1rv' width=800/>\n","\n","- GPT-2를 사용했을 때 다음 단어 예측 방식에서는 SOTA 성능\n","- 기계독해, 요약, 번역 등의 자연어 task에서는 일반 신경망 수준\n","- 하지만 Zero, One, Few-shot learning의 새 지평을 제시"]},{"cell_type":"markdown","metadata":{"id":"fmqxs6M7jjgx"},"source":["<br>\n","\n","### 8.1.3 GPT-3\n","\n","- GPT-3에서는 GPT-2의 하이퍼파라미터의 100배 정도되는 것을 사용\n","- 학습 데이터도 더 많이 사용\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Wwd-VBoRcfbYmejFWQotvPk6ylvlQSOK' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"23GuywxUjoLK"},"source":["<br>\n","\n","- GPT-3 역시 트랜스포머의 디코더 구조를 사용\n","- GPT-2의 트랜스포머 디코더와는 약간의 차이가 있다.\n","  - 구조를 살짝 바꿈\n","  - initialization을 수정\n","- 전체적인 구조는 트랜스포머의 디코더로 동일하다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Wzcw7ur9DY1DLEjgRBgZ6RoJoLuNjzpR' width=800/>\n","\n","- Modified initialization\n","- Pre-normalization, and reversible tokenization\n","- Alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"]},{"cell_type":"markdown","metadata":{"id":"Ccoox_blj5sI"},"source":["- 뉴스 기사 생성 실험\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1X2exjO4FWtELgL4njIhmygDJm3JTHO7L' width=800/>\n","\n","- Control: 실제 사람이 쓴 뉴스 기사"]},{"cell_type":"markdown","metadata":{"id":"Pyja7IWmj95a"},"source":["- GPT-3를 이용한 zero-shot, one-shot, few-shot learning에 대한 수학 연산과 관련된 task의 실험 진행\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1X3XiY11bpGoEvyBX4X9qJ01UM8wn5rZ1' width=700/>"]},{"cell_type":"markdown","metadata":{"id":"XF0AXMiskDvZ"},"source":["- 2개의 숫자를 더하는 것에 대해서는 100%에 가까운 accuracy가 나타났다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1X9V0rPkvCdx0ZmS_zEKa4vahwDgSSeji' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"anQGxVkhkHR5"},"source":["- Open Domain QnA 실험 진행\n","- 기계 독해의 경우 정답에 관련된 context가 input으로 같이 주어진다.\n","- 하지만 Open Domain QnA의 경우 context가 주어지지 않고 질문을 하게 된다.\n","- 이러한 task는 모델이 수행하기엔 매우 어려운 task이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1XAP-dkvKocN3K0P79Qc0QS9KqmnaF_kv' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"T6NzifwskMkC"},"source":["<br>\n","\n","## 8.2 GPT의 응용"]},{"cell_type":"markdown","metadata":{"id":"sdQt6VygkS7B"},"source":["### 8.2.1 GPT-3 예시: 상식 Q&A\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1XC3YEbuVWzLDgqMI6CS2SI2qlQzhcmcH' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"R_6P68k6kXXZ"},"source":["<br>\n","\n","### 8.2.2 GPT-3 예시: 텍스트 데이터 파싱\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1XF-IMDvTyTuFjmfcjmqK0bRu9DWShpqk' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"RQ21ZnN9kgUb"},"source":["<br>\n","\n","### 8.2.3 GPT-3 예시: 의학\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1XTIq5ZtZcWEur_xP9mD1Tl354mT7yqQc' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"9aZFVNcjknNq"},"source":["<br>\n","\n","### 8.2.4 Awesome GPT-3\n","\n","- 70개 가량 예제 수록\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1XU8-gk54Zp72RbVLRur-ATuZROvuIHqF' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"KQ79ZZDdk0LZ"},"source":["<br>\n","\n","## 8.3 남은 과제\n","\n","- 다음 단어 혹은 masked 단어 예측하는 언어 모델 학습 방식으로 정말 다 해결될까?\n","- Weight update가 없다는 것은 모델에 새로운 지식 학습이 없다는 것\n","- 시기에 따라 달라지는 문제에도 대응 불가\n","- 갈수록 모델 사이즈만 키우면 되는 것인가? -> 다른 연구 방향 필요\n","- 멀티 모달 정보가 필요 -> GPT는 글로만 세상을 배움"]},{"cell_type":"code","metadata":{"id":"qhVt6dwUlKgk"},"source":[""],"execution_count":null,"outputs":[]}]}