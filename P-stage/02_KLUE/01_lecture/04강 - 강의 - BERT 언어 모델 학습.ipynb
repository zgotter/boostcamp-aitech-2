{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04강 - 강의 - BERT 언어 모델 학습.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyObS52nnwQcp+Jmk/xr7FZS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1hh8m6NgiAUr"},"source":["# 4. BERT 언어 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"LL9aoaLViFC6"},"source":["## 강의 소개\n","\n","이번에는 3강에서 소개한 BERT를 직접 학습하는 강의입니다.\n","\n","다양한 언어 모델들을 활용하고 공유할 수 있는 Huggingface Hub에 대해 소개하고, 직접 본인의 모델을 공유하는 실습을 진행합니다.🤓"]},{"cell_type":"markdown","metadata":{"id":"0HnoE101iPvG"},"source":["<br>\n","\n","## Reference\n","\n","- [LM training from scratch](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw)\n","- 나만의 BERT Wordpiece Vocab 만들기\n","    - [Wordpiece Vocab 만들기](https://monologg.kr/2020/04/27/wordpiece-vocab/)\n","    - [Wordpiece Tokenizer 만들기](https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0)\n","- [Extracting training data from large language model](https://www.youtube.com/watch?v=NGoDUEz3tZg)\n","- [BERT 추가 설명](https://jiho-ml.com/weekly-nlp-28/)"]},{"cell_type":"markdown","metadata":{"id":"WtjIr9rhiUim"},"source":["<br>\n","\n","## 4.1 BERT 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"a0RL3Adcidcm"},"source":["### 4.1.1 BERT 학습의 단계\n","\n","1. Tokenizer 만들기\n","2. 데이터셋 확보\n","3. Next sentence prediction (NSP)\n","4. Masking"]},{"cell_type":"markdown","metadata":{"id":"D36Dss33ilP8"},"source":["<br>\n","\n","### 4.1.2 새로 학습해야 하는 이유\n","\n","- 도메인 특화 task의 경우, 도메인 특화된 학습 데이터만 사용하는 것이 성능이 더 좋다는 연구 결과가 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1TLBRGnAWyWqafvIC9Hq773LjtQmPAuiN' width=900/>\n","\n","- 오른쪽 표를 보면 BERT의 Corpus는 Wiki와 Books로 되어 있다.\n","- 위 논문에서 소개한 PubMedBERT의 경우 생리학 관련 최대 저널 아카이빙 사이트인 PubMed의 데이터를 사용하여 모델을 pre-trained 시켰다."]},{"cell_type":"markdown","metadata":{"id":"tcoM8attive9"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1TLT2YkW15Kl7hty26NDW3Lf5m4jnevj3' width=800/>\n","\n","- 각 모델들의 성능을 비교해보자.\n","- 위의 표에서 주어진 task들은 생리학에 관련된 자연어 task를 의미한다.\n","  - BC5-chem: 화학 관련 개체명 인식기\n","- 이 모든 task들에 대해 기존 BERT 모델을 fine-tuning하는 것보다 새로 학습한 PubMedBERT 모델을 사용하는 것이 더 좋은 성능을 나타냈다."]},{"cell_type":"markdown","metadata":{"id":"FEkJXjtii1m1"},"source":["<br>\n","\n","### 4.1.3 학습을 위한 데이터 만들기\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1TRmTlBEbM3Izo1T5pJCsPpXjjavBsjZr' width=800/>\n","\n","- 모은 데이터를 데이터셋의 형태로 만들어줘야 한다.\n","- `Dataset`\n","  - 모델이 학습하기 위한 재료 만들기\n","  - 모델의 입력으로 들어갈 수 있는 형태로 변환\n","- `DataLoader`\n","  - 모델에 데이터를 입력하는 방법 정의\n","\n","<br>\n","\n","- 모델에 들어가는 입력의 형태\n","  - `input_ids`\n","  - token type (segA, segB 구분)\n","  - positional encoding"]},{"cell_type":"markdown","metadata":{"id":"Eu4Jn51mi74k"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1TT5_8bNWFKhLtan6egTjzDMi56OPIIQM' width=800/>\n","\n","- 위와 같은 방법으로 구축한 `Dataset`을 통해 모델에 입력을 할 땐 [MASK]로 토큰을 바꿔주는 작업을 수행해야 한다."]},{"cell_type":"code","metadata":{"id":"uuff1P-gjYpz"},"source":[""],"execution_count":null,"outputs":[]}]}