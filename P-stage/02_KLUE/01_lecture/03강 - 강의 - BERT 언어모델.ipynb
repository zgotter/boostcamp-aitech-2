{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03강 - 강의 - BERT 언어모델.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOrZH3In9ve0JD3ZBN+mwgF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CPWEJslBUrMi"},"source":["# 3. BERT 언어모델"]},{"cell_type":"markdown","metadata":{"id":"7b_cNkatVDd-"},"source":["## 강의 소개\n","\n","BERT는 Bidirectional Encoder Representations from Transformers의 약자로 구글이 공개한 인공지능 언어 모델입니다.🤩\n","\n","BERT는 주어진 Mask에 대하여 양방향으로 가장 적합한 단어를 예측하는 사전 언어 모델입니다.\n","\n","이번 강의에는 BERT의 내부 구조에 대해 간략하게 알아보고, BERT를 활용하여 해결할 수 있는 다양한 자연어 처리 Task에 대하여 알아봅니다.🤩"]},{"cell_type":"markdown","metadata":{"id":"pcdkk4GYVcHa"},"source":["## 3.1 BERT 모델 소개"]},{"cell_type":"markdown","metadata":{"id":"5zOojdswWLwd"},"source":["### 3.1.1 Seq2Seq, Attention + Seq2Seq, Transformer\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1S0xepF-wyUXA2UZ4OXCY9oMRathU4QWb' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"kwB5fd_9VzyM"},"source":["<br>\n","\n","### 3.1.2 이미지 Autoencoder vs BERT\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1S3ChwnqKLZXQts-qw4BTkjXK-u4NP2HP' width=800/>\n","\n","- 이미지 Autoencoder\n","  - encoder와 decoder로 구성\n","  - encoder: 입력된 이미지를 압축된 데이터(compressed data)로 표현\n","  - decoder: 원본 이미지로 복원하는 것이 목표\n","  - autoencoder에서 compressed된 벡터를 해당 이미지에 대한 벡터값으로 사용할 수 있다.\n","- BERT\n","  - masked된 이미지를 입력으로 받아 원본 이미지를 출력할 수 있게끔 학습됨"]},{"cell_type":"markdown","metadata":{"id":"4VUy6SeqV0jE"},"source":["<br>\n","\n","### 3.1.3 GPT-1, BERT, GPT-2\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1S8_Onq30bDOHbprKn2jSFYZYzQ-MGVao' width=800/>\n","\n","- GPT-1\n","  - Transformer의 디코더만 사용\n","- BERT\n","  - masked된 자연어를 원본 자연어로 복원하는 과정에서 학습\n","- GPT-2\n","  - 원본 자연어를 특정한 시퀀스를 기준으로 자름\n","  - 모델은 이 잘린 다음 시퀀스를 예측하도록 학습됨"]},{"cell_type":"markdown","metadata":{"id":"rgqmrT9JV5eE"},"source":["<br>\n","\n","### 3.1.4 모델 구조도\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1S96mbk-4N22K4VzNn_RPLRa4QwZ7OHFa' width=800/>\n","\n","- Input layer: sentence 2개를 입력으로 받음\n","- BERT는 transformer 12개의 layer로 구성되어 있다."]},{"cell_type":"markdown","metadata":{"id":"s_P9wBkwWJiU"},"source":["<br>\n","\n","### 3.1.5 학습 코퍼스 데이터\n","\n","- BooksCorpus (800M words)\n","- English Wikipeida (2,500M words without lists, tables and headers)\n","- 30,000 token vocabulary"]},{"cell_type":"markdown","metadata":{"id":"v3WzSeebWpjc"},"source":["<br>\n","\n","### 3.1.6 데이터의 tokenizing\n","\n","- WordPiece tokenizing\n","- He likes playing -> He likes play ##ing\n","- 입력 문장을 tokenizing하고, 그 token들로 \"token sequence\"를 만들어 학습에 사용\n","- 2개의 token sequence가 학습에 사용\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SBKwWNHigxNCDzlna3o2SNLTZn6E0PHZ' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"OphZsgV7W_uw"},"source":["<br>\n","\n","### 3.1.7 Masked Language Model\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SE1t2av2nRlV04Fk8vJJxQSUahVyQ4-L' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"pq9ELFdmXIDs"},"source":["<br>\n","\n","### 3.1.8 Datasets\n","\n","- GLUE datasets\n","  - MNLI: Multi-Genre Natural Language Inference\n","    - 두 문장의 관계 분류를 위한 데이터셋\n","  - QQP: Quora Question Pairs\n","    - 두 질문이 의미상 같은지 다른지 분류를 위한 데이터셋\n","  - QNLI: Question Natural Language Inference\n","    - 질의응답 데이터셋\n","  - SST-2: The Stanford Sentiment Treebank\n","    - 영화 리뷰 문장에 관한 감성 분석을 위한 데이터셋\n","  - CoLA: The Corpus of Linguistic Acceptability\n","    - 문법적으로 맞는 문장인지 틀린 문장인지 분류를 위한 데이터셋\n","  - STS-B: The Semantic Textual Similarity Benchmark\n","    - 뉴스 헤드라인과 사람이 만든 paraphrasing 문장이 의미상 같은 문장인지 비교를 위한 데이터셋\n","  - MRPC: Microsoft Research Paraphrase Corpus\n","    - 뉴스의 내용과 사람이 만든 문장이 의미상 같은 문장인지 비교를 위한 데이터셋\n","  - RTE: Recognizing Textual Entailment\n","    - MNLI와 유사하나 상대적으로 훨씬 적은 학습 데이터셋\n","  - WNLI: Winograd NLI\n","    - 문장 분류 데이터셋\n","- SQuAD v1.1\n","  - 질의응답 데이터셋\n","- CoNLL 2003 Named Entity Recognition datasets\n","  - 개체명 분류 데이터셋\n","- SWAG: Situations With Adversarial Generations\n","  - 현재 문장 다음에 이어질 자연스러운 문장을 선택하기 위한 데이터셋"]},{"cell_type":"markdown","metadata":{"id":"2ATwNq47ZZN-"},"source":["<br>\n","\n","### 3.1.9 Fine-tuning\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SI7ABNF3s6HWQCnggqvngTsIhUIROneK' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"bLcNHGTqZhF9"},"source":["<br>\n","\n","## 3.2 BERT 모델의 응용"]},{"cell_type":"markdown","metadata":{"id":"73rpg9fPZsVF"},"source":["### 3.2.1 감성 분석\n","\n","- 네이버 영화 리뷰 코퍼스 (https://github.com/e9t/nsmc)로 감성 분석\n","- 학습: 150,000 문장 / 평가: 50,000 문장 (긍정: 1, 부정: 0)\n","\n","<br>\n","\n","- 입력된 문장의 긍부정 분류\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SIu1vgwpFlCODrEX_j-h2T4aVXLHakFU' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"bEQOk8HmaSrk"},"source":["<br>\n","\n","### 3.2.2 관계 추출\n","\n","- KAIST가 구축한 Silver data 사용 (1명의 전문가가 annotation)\n","- 학습: 985,806 문장 / 평가: 100,001 문장\n","- 총 81개 label (관계 없음 포함)\n","\n","<br>\n","\n","- Entity: 관계 추출의 대상이 되는 존재\n","- 주어진 문장에서 subject와 object가 갖는 관계를 예측하는 문제\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SNM7hqgsKjl1E97gb9zE7kWsMLH0c3nC' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"2diX7mizaxNE"},"source":["<br>\n","\n","### 3.2.3 의미 비교\n","\n","- 디지털 동반자 패러프레이징 질의 문장 데이터를 이용하여 질문-질문 데이터 생성 및 학습\n","- 학습: 3,401 문장 쌍 (유사 x: 1,700개, 유사 o: 1,701개)\n","- 평가: 1,001 문장 쌍 (유사 x: 500개, 유사 o: 501개)\n","\n","<br>\n","\n","- 두 문장이 의미적으로 갖은지 아닌지를 분류하는 문제\n","- 두 문장이 너무 다른 문장으로 전처리되어서 성능이 높게 나온 측면이 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SQXQVBHi684WT9UGd-agFIeqma6Nn2Ct' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"EoGvTpV4bHBL"},"source":["<br>\n","\n","### 3.2.4 개체명 분석\n","\n","- ETRI 개체명 인식 데이터를 활용하여 학습 및 평가 진행 (정보통신단체표준 TTA.KO-10.0852)\n","- 학습: 95,787 문장 / 평가: 10,503 문장\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Sa-t5bODMla39Jk4Vi2ToDAioRow4XHU' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"uYsFuzS9bdb-"},"source":["<br>\n","\n","### 3.2.5 기계 독해\n","\n","- LG CNS가 공개한 한국어 QA 데이터셋, KorQuAD (https://koorquad.github.io/)\n","- Wikipeida article에 대해 10,645 건의 문단과 66,181개의 질의응답\n","- Traning set 60,407개, Dev set: 5,774개\n","\n","<br>\n","\n","- WordPiece Tokenizer의 한계 때문에 어절 단위 tokenizer보다 음절 단위 tokenizer가 더 좋은 성능을 보였다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SaI12-Anw-pd4Jh7FUgsaPwlTxPQiOVg' width=900/>"]},{"cell_type":"markdown","metadata":{"id":"3PbM8SJicAhQ"},"source":["<br>\n","\n","## 3.3 한국어 BERT 모델"]},{"cell_type":"markdown","metadata":{"id":"FoO2kwrucC7c"},"source":["### 3.3.1 ETRI KoBERT의 tokenizing\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Sksv6rJK62q2jBZcR8yS-1J6Wp0e77fR' width=900/>\n","\n","- 먼저 형태소 단위로 분리를 한 후 이를 바탕으로 WordPiece Tokenizer를 사용했다.\n","- 형태소의 역할 : 의미를 갖는 최소 단위로 분리한다.\n","- 한국어에 맞게 모델을 구축했고 구글의 모델보다 좋은 성능을 나타냈다."]},{"cell_type":"markdown","metadata":{"id":"tnFvO0H1cGqb"},"source":["<br>\n","\n","### 3.3.2 한국어 tokenizing에 따른 성능 비교\n","\n","- [https://arxiv.org/abs/2010.02534](https://arxiv.org/abs/2010.02534)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Sltt6Rr1-MEMeYKvLwdQ_htkvT3Y76jc' width=900/>\n","\n","- 형태소 분석을 수행한 후 tokenizer를 사용한 Morpheme-aware Subword 가 가장 성능이 좋았다."]},{"cell_type":"markdown","metadata":{"id":"egiEU4u_cZx8"},"source":["<br>\n","\n","### 3.3.3 Advanced BERT model\n","\n","- KBQA에서 가장 중요한 entity 정보가 기존 BERT에서는 무시\n","- Entity linking을 통한 주요 entity 추출 및 entity tag 부착\n","- Entity embedding layer의 추가\n","- 형태소 분석을 통해 NNP와 entity 우선 chunkinig masking\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SvAUjGlnn7OlHDbLlvCoABinHwuUrBTE' width=800/>\n","\n","- BERT에는 Entity를 명시할 수 있는 구조가 존재하지 않는다.\n","- 원본 문장에서 Entity 태그인 `[ENT]`를 부착한 후 Entity Embedding layer를 추가했다.\n","- 이렇게 하여 성능 향상을 얻을 수 있었다."]},{"cell_type":"markdown","metadata":{"id":"C_Fyk4VKcxLh"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1SwwcRYl2nLJW7V0RJGdTM8jw-WkciXsr' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"A-DXwhCCc-yF"},"source":["<br>\n","\n","- 학습 데이터: 2019년 06월 20일 Wiki dump (약 4,700만 어절)\n","- Batch: 128\n","- Sequence length: 512\n","- Training steps: 300,000 (대략 10 epochs)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1T-FsdjmBg1nf_jQ_8c01MZE_GVexSgXo' width=500/>"]},{"cell_type":"code","metadata":{"id":"BGkiorjUdKek"},"source":[""],"execution_count":null,"outputs":[]}]}