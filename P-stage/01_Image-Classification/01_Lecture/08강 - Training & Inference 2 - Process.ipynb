{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08강 - Training & Inference 2 - Process.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPsoghsyuxREYoVLHb/DTUU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JRNpkQomP-Bz"},"source":["# 8. Training & Inference 2 - Process\n","\n","> 이제 정말 본격적으로 학습과 추론 프로세스를 시작합니다.\n","\n","> 앞서 정의한 데이터셋, 모델, Loss, Optimizer, Metric을 가지고 실제 파이토치가 모델의 파라미터를 업데이트 하는 과정과 방식을 조금 디테일하게 접근해보겠습니다. 각각의 요소들이 하는 일과, 그로 인해 발생할 수 있는 side effect들이 모델을 업데이트하는 과정을 하나하나 보도록 하겠습니다.\n","\n","> 그리고, 모델을 추론하고 제출하는 것도 간략히 다루겠습니다.\n","\n","> 마지막으로, Pytorch Lightning이 어떤 것인지 간략히 다루겠습니다."]},{"cell_type":"markdown","metadata":{"id":"THfFUKKYQDzE"},"source":["<br>\n","\n","## 8.1 Overview\n","\n","- 학습과 추론 프로세스의 과정을 이해 하는 것이 목표입니다.\n","\n","<br>\n","\n","- 학습 과정\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1lC63Bexlc-IKvxmF0JlrXFmoxayHEQSH' width=500/>\n","\n","- 출처: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","\n","<br>\n","\n","- 평가 과정\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1xhESp7aXqdfLjlYoLOVqz7LfYs5f7JPd' width=500/>\n","\n","- 출처: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"HnlhFaKmQSzM"},"source":["<br>\n","\n","\n","## 8.2 Training Process\n","\n","- Training 준비\n","- Training 프로세스 이해\n","- More: Gradient Accumulation"]},{"cell_type":"markdown","metadata":{"id":"IxUbv6p9Qtlk"},"source":["<br>\n","\n","### 8.2.1 Training 준비\n","\n","- 학습 한번 하기 위해 지금 까지 만든 결과물\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1JbAQSS6mU8U-wngUVcf0VKKObsMH5QJG' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"cPluNeSBQyUf"},"source":["<br>\n","\n","### 8.2.2 Training 프로세스의 이해"]},{"cell_type":"markdown","metadata":{"id":"iBOWVKFTQ_5c"},"source":["#### 8.2.2.1 `model.train()`\n","\n","- 모델을 train 상태로 바꾼다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1dRfw5dSI_vA_GO4pIg2M4G7xLtbgzBKZ' width=800/>\n","\n","- 출처: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"]},{"cell_type":"markdown","metadata":{"id":"3tiDDxzKRC0f"},"source":["<br>\n","\n","#### 8.2.2.2 `optimizer.zero_grad()`\n","\n","- optimizer 에 이전 배치 때의 grad 가 남아 있는 것을 0으로 초기화해준다.\n","  - 엄밀하게 말하면 loss 가 gradient 를 갖고 있는 것이다.\n","- optimizer 는 모델의 파라미터들을 가지고 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1NGf5s31x7eECPDedUbHE0swOYbVdWQgr' width=600/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1g6bk9pedrAMo-XDd4KD5Ng6LxpletHKm' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"hkt0Ky2NRMio"},"source":["<br>\n","\n","#### 8.2.2.3 `loss = criterion(outputs, labels)`\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Ci_B8yJvB9bscDcRp8NOrYLF0YXrpsGu' width=700/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1QMk95L01JlcgjstKZT0TIVQtIBYmwcp3' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"yGfEBAoDRbQQ"},"source":["<br>\n","\n","#### 8.2.2.4 loss 를 마지막으로 chain 생성\n","\n","- loss 또한 결국은 `nn.Module` 을 상속받은 것으로 `forward` 함수를 갖고 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1dbK6o7mR2wTwwHrBW05xRV_QFTyMj4Pw' width=700/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1PPtkboujf5-LfQsoPij6vYRTpHYAi27f' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"DIiAS1J3Rxqg"},"source":["<br>\n","\n","#### 8.2.2.5 loss 의 `grad_fn` chain -> `loss.backward()`\n","\n","- 디버깅 과정을 통해 파라미터 확인\n","- `grad_fn` 에서 연결점 발생\n","  - 모델의 파라미터까지 연결된다.\n","- `loss.backward()` 를 했을 때 연결된 파라미터들에게 gradient 를 업데이트하게 된다.\n","- loss 의 역할은 gradient 를 업데이트하는 것이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1qh0nKasUVchL9YrHDazpHFbOzSPlqOMw' width=500/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1d2hvxRbFl7KH2tVkVZ4-qXeBZlmNQgcK' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"ITaY4Fw9R_2b"},"source":["<br>\n","\n","#### 8.2.2.6 `optimizer.step()`\n","\n","- `loss.backward()` 를 수행했기 때문에 모델의 각 파라미터의 grad 는 업데이트 되어 있다.\n","- 이 때 `optimizer.step()` 을 하면 optimizer 가 가지고 있는 파라미터의 업데이트된 grad 를 바탕으로 실제 데이터에 적용 시킨다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1eOcmGZwCydwiAqrueeuS2qPhp2uqXcSW' width=500/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1RupwR4UZ3exXeMBoXlG_9GzmJf8rvUxY' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"E7JTL_PnSN-3"},"source":["<br>\n","\n","### 8.2.3 More: Gradient Accumulation\n","\n","- 지금까지의 모든 과정을 이해했다면 이를 응용하는 것도 가능하다."]},{"cell_type":"markdown","metadata":{"id":"ZR9YHqF71j5w"},"source":["- 기존의 training process\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1EipNannV7mgj8zeVN0-ncuPSOVZhIUdy' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"-tKfU7pE1p5F"},"source":["- `NUM_ACCUM = 2` 로 지정하므로서 배치가 2번 돌 때마다 누적된 gradient 를 한 번에 적용하는 것이 <font color='yellow'>Gradient Accumulation</font> 이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=177-xOYAAjg0xbtLdiH087XvqXn3JMjk-' width=500/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f9kDa6bKSd33"},"source":["<br>\n","\n","## 8.3 Inference Process\n","\n","- Inference 프로세스 이해\n","- Validation\n","- Checkpoint\n","- 최종 Output, Submission 형태로 변환"]},{"cell_type":"markdown","metadata":{"id":"Fvn69DDSSren"},"source":["<br>\n","\n","### 8.3.1 Inference 프로세스 이해"]},{"cell_type":"markdown","metadata":{"id":"zAw_uIqKSzqA"},"source":["#### 8.3.1.1 `model.eval()`\n","\n","- model 을 eval 상태로 변경\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1uGLMNMJEyIl0Q0qOn9FTzKSuzfKU3fqC' width=800/>\n","\n","- 출처: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"]},{"cell_type":"markdown","metadata":{"id":"AB4e1YRjS80O"},"source":["<br>\n","\n","#### 8.3.1.2 `with torch.no_grad():`\n","\n","- inference 과정에서 gradient 가 업데이트가 안되도록 설정\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1_I1Ny0vOQ6-vtk05o8e66t7l9HSMKSXs' width=500/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1GVAWfouxIJfbk5llUbModPdTX7PCjIvB' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"mZ0UDNGjTAh1"},"source":["<br>\n","\n","### 8.3.2 Validation 확인\n","\n","- 추론 과정에 Validation 셋이 들어가면 그게 검증이다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1UL1f6P-0PtQDmtxIqOX1o5wp8ley0-N9' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"GDVovrGcTMqZ"},"source":["<br>\n","\n","### 8.3.3 Checkpoint\n","\n","- 그냥 직접 짜면 된다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1reKh-uXhIzqlqeROgQl-ulh01jw8V8cZ' width=700/>"]},{"cell_type":"markdown","metadata":{"id":"uzmJuJvPTvgf"},"source":["<br>\n","\n","### 8.3.4 최종 Output, Submission 형태로 변환\n","\n","- 최종 Submissioni 스펙을 확인 후 변환하여 제출\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1JgyxnkGrEwyprY1N6nYN8astAiI9_CKf' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"tFo3WzciT6uh"},"source":["<br>\n","\n","## Apppendix: Pytorch Lightning\n","\n","- 실무에서는 생산성이 중요하다.\n","- 지금까지의 모든 프로세스를 하나의 클래스로 정의한다.\n","- `fit()` 으로 전체 과정을 실행시킨다.\n","  - Keras 코드를 보는 듯...\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=15Rk8xviskbXlqJGCzqYV5Ql22k_68qWC' width=500/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rISeeMHxIANkxw2okp30IEJCN6u6uhop' width=900/>\n","\n","- 출처: https://www.pytorchlightning.ai/"]},{"cell_type":"markdown","metadata":{"id":"NTbI0ETaUGE4"},"source":["<br>\n","\n","- 그래도 공부는 Pytorch로 했으면 해요.\n","- 충분한 이해가 바탕이 되지 않은 상태에서는 오히려 독이 될 수 있습니다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1Dk5RM2INx4Ftc-3d_DBguU_mzJCrN_ok' width=500/>\n","\n","- 코드로부터 머신러닝 프로세스를 배울 수 있고 자유롭게 응용할 수 있음"]},{"cell_type":"code","metadata":{"id":"FNWocvLYUbdw"},"source":[""],"execution_count":null,"outputs":[]}]}