{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07강 - Linking MRC and Retrieval.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOGnkXH2ZqACXjT3U/VuwZL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"klHJdPzs2GjU"},"source":["# 7. Linking MRC and Retrieval"]},{"cell_type":"markdown","metadata":{"id":"eDMeEhgz2V7C"},"source":["## 강의소개\n","\n","- 6강에서는 기계독해와 문서 검색을 연결해 Open-domain question answering (ODQA)를 푸는 방법에 대해 배워보겠습니다.\n","- 먼저 이번 기계독해 강의의 대회에서도 풀어야 할 ODQA가 무엇인지에 대해 알아보겠습니다.\n","- ODQA를 풀기 위한 접근법 중 하나는 1~3강에서 배운 기계독해 (reader) 방법과 4~6강에서 배운 문서검색 (retriever)을 붙이는 방법입니다.\n","- 이번 강의에서는 이런 retriever-reader approach에 대해 배워보고 실습해보겠습니다."]},{"cell_type":"markdown","metadata":{"id":"Kv_igI7B2h4i"},"source":["<br>\n","\n","## Further Reading\n","\n","- [Reading Wikipedia to Answer Open-domain Questions](https://arxiv.org/abs/1704.00051)\n","- [A survey on Machine Reading Comprehension](https://arxiv.org/abs/2006.11880)\n","- [ACL 2020 ODQA tutorial](https://slideslive.com/38931668/t8-opendomain-question-answering)"]},{"cell_type":"markdown","metadata":{"id":"w8YaKNTD2l9b"},"source":["<br>\n","\n","## 7.1 Introduction to Open-domain Question Answering (ODQA)"]},{"cell_type":"markdown","metadata":{"id":"3MTOh-vx2xzb"},"source":["### 7.1.1 Linking MRC and Retrieval: ODQA"]},{"cell_type":"markdown","metadata":{"id":"CXFLv4Go3GWC"},"source":["#### 7.1.1.1 MRC\n","\n","- <mark>지문이 주어진 상황</mark>에서 질의응답\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rCCAVGpzKhsbm0sX5ORb-EbR0b74q7LZ' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"GAXfyR_928F5"},"source":["<br>\n","\n","#### 7.1.1.2 ODQA\n","\n","- <mark>지문이 따로 주어지지 않음</mark>\n","- 방대한 World Knowledge에 기반해서 질의응답\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rHMqaGcyqzaaHi6PJbGGZPJfnhIRqMDe' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"RE_p7jfB3RQ4"},"source":["<br>\n","\n","#### 7.1.1.3 Ex. Modern search engines\n","\n","- 연관 문서 뿐만 아니라 질문의 답을 같이 제공\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rJd-wKUZUIUWGdIbiJ9xw7zmCwN0-3Jc' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"JIMDPmeK3bXg"},"source":["<br>\n","\n","### 7.1.2 History of ODQA\n","\n","- ODQA는 생각보다 꽤 오래된 도메인이다.\n","- Text retrieval conference (TREC) - QA Tracks (1999-2007)\n","  - 연관문서만 반환하는 information retrieval(IR)에서 더 나아가서, short answer with support 형태가 목표\n","- 3가지 파트로 구성되어 있음\n","  - Question processing\n","  - Passage retrieval\n","  - Answer processing\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rT7hFWBqoXawloG-X85tyeM2ZXa0wr5Y' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"EijFzdp738X2"},"source":["<br>\n","\n","#### 7.1.2.1 Question processing\n","\n","- 어떻게 하면 질문을 잘 이해할 수 있을까?\n","- 그 당시엔 딥러닝이 발전하지 못했기 때문에 질문으로부터 키워드를 선택해서 answer type을 선택하는 것이 유일한 방법이였다.\n","- Query formulation\n","  - 질문으로부터 키워드를 선택\n","  - Answer type selection\n","  - ex) `LOCATION: country`\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rVWMSBeWiZJTEzaZLLUKjtQ5ZIQ2NKwi' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"Ow8n_80m431G"},"source":["<br>\n","\n","#### 7.1.2.2 Passage retrieval\n","\n","- 현재의 방법론과 유사하다.\n","- 기존의 IR 방법을 활용해서 연관된 document를 뽑고, passage 단위로 자른 후 선별\n","- Named entity / Passage 내 question 단어의 개수 등과 같은 hand-crafted features 활용\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rZsN-ydIrlKsY9SBwmN2ptspn9xf7ZJa' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"LnzyIJcL5k14"},"source":["<br>\n","\n","#### 7.1.2.3 Answer processing\n","\n","- Hand-crafted features와 heuristic을 활용한 classifier\n","- 주어진 question과 선별된 passage들 내에서 답을 선택\n","- 현재의 방법론과는 차이가 있다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1r_pdPBT19_Q2btcnMUNRLvKvR7LnwuT9' width=400/>"]},{"cell_type":"markdown","metadata":{"id":"_tW7dMZE6WwG"},"source":["<br>\n","\n","#### 7.1.2.4 IBM Watson (2011)\n","\n","- The DeepQA PRoject\n","- Jeopardy! (TV quiz show) 우승\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rbXQMiAfXbbHqFMqOMW8hIW3YUWIYUDQ' width=400/>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rc07oP4q1UHrAGpK2bk-UgkLbc0qu81c' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"DSrJdRZS6qM2"},"source":["<br>\n","\n","### 7.1.3 Recent ODQA Research\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rg5GblFoS0Yb46CHax6CXGOHpbV4QmAf' width=600/>\n","\n","- 2017년을 기점으로 많은 모델들이 나왔고 지속적인 성능 향상이 있었다."]},{"cell_type":"markdown","metadata":{"id":"G8nyAoGD6361"},"source":["<br>\n","\n","## 7.2 Retriever-Reader Approach"]},{"cell_type":"markdown","metadata":{"id":"MYt8CTxW7A7z"},"source":["### 7.2.1 Retriever-Reader 접근 방식\n","\n","- Retriever\n","  - 데이터베이스에서 관련 있는 문서를 검색(search)함\n","- Reader\n","  - 검색된 문서에서 질문에 해당하는 답을 찾아냄\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rkKwARMFGNftSg_7CH9duG_JCAC-Gbkz' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"KD7o9Qeb7Ril"},"source":["<br>\n","\n","### 7.2.2 Retriever-Reader 입출력\n","\n","- Retriever\n","  - 입력\n","    - 문서 set (Document corpus)\n","    - 질문 (query)\n","  - 출력\n","    - 관련성 높은 문서 (document)\n","- Reader\n","  - 입력\n","    - Retrieved된 문서 (document)\n","    - 질문 (query)\n","  - 출력\n","    - 답변 (answer)"]},{"cell_type":"markdown","metadata":{"id":"kXIX0clE7t4A"},"source":["<br>\n","\n","### 7.2.3 학습 단계\n","\n","- Retriever\n","  - TF-IDF, BM25 -> 학습 없음\n","  - Dense -> 학습 있음\n","- Reader\n","  - SQuAD와 같은 MRC 데이터셋으로 학습\n","  - 학습 데이터를 추가하기 위해 Distant supervision 활용"]},{"cell_type":"markdown","metadata":{"id":"LcT9lzrr8Qhj"},"source":["<br>\n","\n","### 7.2.4 Distant supervision\n","\n","- 질문-답변만 있고 <mark>지문이 없는</mark> 데이터셋(ex. CuratedTREC, WebQuestions, WikiMovies)에서 MRC 학습 데이터 만들기\n","- Supporting document가 필요함\n","\n","<br>\n","\n","프로세스\n","\n","1. 위키피디아에서 Retriever를 이용해 관련성 높은 문서를 검색\n","2. 너무 짧거나 긴 문서, 질문의 고유명사를 포함하지 않는 등 부적합한 문서 제거\n","3. answer가 exact match로 들어있지 않은 문서 제거\n","4. 남은 문서 중에 질문과 (사용 단어 기준) 연관성이 가장 높은 단락을  supporting evidence로 사용함"]},{"cell_type":"markdown","metadata":{"id":"yyFfhKXa9tPk"},"source":["각 데이터셋 별 distant supervision을 적용한 예시\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rsW8QjA4Izdd-Pdfdvy0YGsrUTFb-0Q1' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"24ZLSJ1y94cm"},"source":["<br>\n","\n","### 7.2.5 Inference\n","\n","- Retriever가 질문과 가장 관련성 높은 5개 문서 출력\n","- Reader는 5개 문서를 읽고 답변 예측\n","- Reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용함"]},{"cell_type":"markdown","metadata":{"id":"BwQpybhn-N02"},"source":["<br>\n","\n","## 7.3 Issues & Recent Approaches"]},{"cell_type":"markdown","metadata":{"id":"le4_bbvk-TFc"},"source":["### 7.3.1 Different granularities(세분화) of text at indexing time\n","\n","- Passage 라는 단위가 위키피디아에선 엄밀하게 정의되어 있지 않다.\n","- 위키피디아에서 각 Passage의 단위를 문서, 단락, 또는 문장으로 정의할지 정해야 함\n","  -  Article: 5.08 million\n","  -  Paragraph: 29.5 million\n","  -  Sentence: 75.9 million\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rt7flDzXGaBsoGPxeIMC1YqPyZTIVYj_' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"ffqNc29l-qKe"},"source":["<br>\n","\n","- Retriever 단계에서 몇 개(top-k)의 문서를 넘길 지 정해야 함\n","- Granularity에 따라 k가 다를 수 밖에 없음\n","  - article -> k=5\n","  - paragraph -> k=29\n","  - sentence -> k=78\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rxzEuBZV3ateJeTJ3jDN1HH1k6W1Amxs' width=600/>"]},{"cell_type":"markdown","metadata":{"id":"vHHcdrRM_Agn"},"source":["<br>\n","\n","### 7.3.2 Single-passage training vs Multi-passage training"]},{"cell_type":"markdown","metadata":{"id":"JCcwSPgg_N-c"},"source":["#### 7.3.2.1 Single-passage\n","\n","- 현재 우리는 k 개의 passages 들을 reader이 각각 확인하고 특정 answer \n","span에 대한 예측 점수를 나타냄\n","- 그리고 이 중 가장 높은 점수를 가진 answer span 을\n","고르도록 함\n","- <mark>이 경우 각 retrieved passages 들에 대한 직접적인 비교라고 볼 수 없음</mark>\n","- <mark>따로 reader 모델이 보는 게 아니라 전체를 한번에 보면 어떨까?</mark>"]},{"cell_type":"markdown","metadata":{"id":"jwz_OGzY_Uf8"},"source":["<br>\n","\n","#### 7.3.2.2 Multi-passage\n","\n","- retrieved passages 전체를 하나의 passage 로 취급하고, reader 모델이 그\n","안에서 answer span 하나를 찾도록 함\n","- Cons\n","  - 문서가 너무 길어지므로 GPU에 더 많은 메모리를 할당해야함 & 처리해야하는 연산량이 많아짐"]},{"cell_type":"markdown","metadata":{"id":"CvWldErPAX8E"},"source":["<br>\n","\n","### 7.3.3 Importance of each passage\n","\n","- Retriever 모델에서 추출된 top-k passage들의 retrieval score를 reader 모델에 전달\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1rzK26aJ9KojXwnqcK7OyIGQUV7lySgpS' width=800/>"]},{"cell_type":"code","metadata":{"id":"FUKoqFcfAfGm"},"source":[""],"execution_count":null,"outputs":[]}]}