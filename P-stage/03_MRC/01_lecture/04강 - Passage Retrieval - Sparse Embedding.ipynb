{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04강 - Passage Retrieval - Sparse Embedding.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1l0UQa6c-x1PtLNgD8YzY5ohe2TEcB7O1","authorship_tag":"ABX9TyPlDrePUcH7PFFH9ZE18/OL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7nWmcnM088GZ"},"source":["# 4. Passage Retrieval - Sparse Embedding"]},{"cell_type":"markdown","metadata":{"id":"IXcD0wQk9F62"},"source":["## 강의소개\n","\n","- 4강에서는 단어기반 문서 검색에 대해 배워보겠습니다.\n","- 먼저 문서 검색 (Passage retrieval)이란 어떤 문제인지에 대해 알아본 후, 문서 검색을 하는 방법에 대해 알아보겠습니다.\n","- 문서 검색을 하기 위해서는 문서를 embedding의 형태로 변환해 줘야 하는데, 이를 passage embedding 이라고 합니다.\n","- 이번 강의에서는 passage embedding이 무엇인지 알아보는 동시에, 단어 기반으로 만들어진 passage embedding인 sparse embedding, 그 중에서도 자주 쓰이는 TF-IDF에 대해 알아볼 예정입니다."]},{"cell_type":"markdown","metadata":{"id":"aDngS8ge9WxU"},"source":["<br>\n","\n","## Further Reading\n","\n","- [Pyserini BM25 MSmarco documnet retrieval 코드](https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-doc.md)\n","- [Sklearn feature extractor](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) ⇒ text feature extractor 부분 참고"]},{"cell_type":"markdown","metadata":{"id":"fan1KVDbXLuj"},"source":["<br>\n","\n","## Reference\n","\n","- [CMU Probabilistic graphical model Lecture 7 slide](https://www.cs.cmu.edu/~epxing/Class/10708-20/lectures.html:)\n","- [Research paper classifcation systems based on TF-IDF and LDA schemes, Kim et al, 2019](https://en.wikipedia.org/wiki/Discovery_of_the_neutron)"]},{"cell_type":"markdown","metadata":{"id":"jddew91S9jyu"},"source":["<br>\n","\n","## 4.1 Introduction to Passage Retrieval\n","\n","- Passage Retrieval\n","- Passage Retrieval with MRC\n","- Overview of Passage Retrieval"]},{"cell_type":"markdown","metadata":{"id":"eRw88TikAQpp"},"source":["<br>\n","\n","### 4.1.1 Passage Retrieveal\n","\n","- 질문(query)에 맞는 문서(passage)를 찾는 것\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1lmgfxRH3JNw8bvIvBpcfd8BiN24ST5w-' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"Qgr2J3yjAfQI"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1m55_jhHwgsTFE_j5X-9k4qy0M9JAdi1f' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"2PQcxPtGDdUo"},"source":["<br>\n","\n","### 4.1.2 Passage Retrieval with MRC\n","\n","- Open-domain Question Answering\n","  - 대규모의 문서 중에서 질문에 대한 답을 찾기\n","- Passage Retrieval과 MRC를 이어서 2-Stage로 만들 수 있음\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mC3F9FJDqUesOeu2ElG5Xou2NmQcV-LE' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"clYaliWnIzNS"},"source":["<br>\n","\n","### 4.1.3 Overview of Passage Retrieval\n","\n","- Query와 Passage를 임베딩한 뒤 유사도로 랭킹을 매기고, 유사도가 가장 높은 Passage를 선택함\n","\n","<br>\n","\n","- 질문(Query)이 들어왔을 때 벡터 공간에 임베딩한다.\n","- 같은 벡터 공간에 문서(Passage)들을 임베딩한다.\n","  - Passage 임베딩은 질문이 들어올 때마다 하는 것이 아니라 미리 해둠으로서 효율성을 도모한다.\n","- Query에 대한 임베딩과 Passage에 대한 임베딩 사이의 유사도를 계산한다.\n","\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mEHBVaEcQXPQ7LzMcVjWiI4YiyxH3_fM' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"duXcz6J0LYZs"},"source":["<br>\n","\n","## 4.2 Passage Embedding and Sparse Embedding\n","\n","- Passage Embedding\n","- Passage Embedding Space\n","- Sparase Embedding"]},{"cell_type":"markdown","metadata":{"id":"qww5T6a0LnnD"},"source":["<br>\n","\n","### 4.2.1 Passage Embedding Space\n","\n","- Passage Embedding의 벡터 공간\n","- 벡터화된 Passage를 이용하여 Passage 간 유사도 등을 알고리즘으로 계산할 수 있음\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mEn0b4EjcyF9GQUJ9GFmU1OFXtwLmiBb' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"uypeq_fbL1BK"},"source":["<br>\n","\n","### 4.2.2 Sparse Embedding 소개\n","\n","- Sparse\n","  - 희소\n","  - 0이 아닌 숫자가 적게 있는 경우를 말함\n","  - Dense의 반대 개념"]},{"cell_type":"markdown","metadata":{"id":"tjxqx7K4MBvn"},"source":["<br>\n","\n","#### 4.2.2.1 Bag-of-Words (BoW)\n","\n","- 문서가 주어졌을 때 해당 문서를 embedding space로 mapping하기 위해서 문서에 존재하는 각 단어들의 존재여부를 1 또는 0으로 표시하여 길이가 긴 벡터로 표현한다.\n","- 벡터의 길이 == vocab size\n","- 벡터의 각 dimension은 하나의 단어를 의미한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mIwNem9U9xVQjEzSBmms7aXPu3VliqDW' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"LnqHiosuMFxB"},"source":["<br>\n","\n","#### 4.2.2.2 구성 방법\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mQU-87izyRKcrk-OROvYitbKNVl7xKcq' width=800/>\n","\n","1. BoW를 구성하는 방법 - (1)\n","  - n-gram을 통해 BoW를 구성할 수 있다.\n","  - unigram (1-gram)\n","    - 하나의 단어만을 보는 방법\n","    - 벡터의 크기가 단어의 갯수와 동일해진다.\n","    - `It was the best of times` -> `It`, `was`, `the`, `best`, `of`, `times`\n","  - bigram (2-gram)\n","    - 인접한 2개의 단어를 함께 보는 방법\n","    - 벡터의 크기는 단어의 갯수의 제곱이다.\n","    - unigram보다 advanced한 방법\n","    - `It was the best of times` -> `It was`, `was the`, `the best`, `best of`, `of times`\n","2. Term Value를 결정하는 방법 - (2)\n","  - Term이 document에 등장하는 지 (binary - 0, 1)\n","  - Term이 몇 번 등장하는 지 (term frequency)\n","  - TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"lw_12xaUM466"},"source":["<br>\n","\n","### 4.2.3 Sparse Embedding 특징"]},{"cell_type":"markdown","metadata":{"id":"CxbVfwsPNEuS"},"source":["**1. Dimension of embedding vector = number of terms**\n","\n","- 등장하는 단어가 많아질수록 임베딩 차원이 증가\n","- N-gram의 n이 커질수록 임베딩 차원이 증가\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mWrCy1UpPjOCRX1FVyS5yAiHJpOVrqEN' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"nNICte-6NSOD"},"source":["**2. Term overlap을 정확하게 잡아 내야 할 때 유용**\n","\n","- 검색에 활용을 할 때는 Query에 있는 단어가 Passage에 실제로 있는 지 없는 지를 파악하는 데 유용하게 사용할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"ZmQ0uWSMNbLp"},"source":["**3. 반면, 의미(semantic)가 비슷하지만 다른 단어인 경우 비교가 불가**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mazm_zokeFJhC4NRufGqgQbSkqsfruok' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"nnqg1MM6NiV6"},"source":["<br>\n","\n","## 4.3 TF-IDF\n","\n","- TF-IDF 소개\n","- Term Frequency (TF)\n","- Inverse Document Frequency (IDF)\n","- Combine TF & IDF\n","- TF-IDF 계산하기\n","- BM25란?"]},{"cell_type":"markdown","metadata":{"id":"MlNCsiXLNwpA"},"source":["<br>\n","\n","### 4.3.1 TF-IDF(Term Frequency - Inverse Document Frequency) 소개\n","\n","- Term Frequency (TF)\n","  - 단어의 등장빈도\n","- Inverse Document Frequency (IDF)\n","  - 단어가 제공하는 정보의 양\n","  - 단어가 얼만큼 덜 등장하는지에 대한 정보\n","\n","<br>\n","\n","- ex) `It was the best of times`\n","  - `It`, `was`, `the`, `of`\n","    - 자주 등장하지만 제공하는 정보량이 적음\n","  - `best`, `times`\n","    - 자주 등장하지 않는 단어들은 좀 더 많은 정보를 제공\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mkXPNef975FZVrIIKGB83htal_0h4bkn' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"16Aayga2OJts"},"source":["<br>\n","\n","### 4.3.2 Term Frequency (TF)\n","\n","- 해당 문서 내 단어의 등장 빈도\n","\n","<br>\n","\n","- Term Frequency를 구하는 방법\n","  1. Raw count\n","    - 특정 문서에서 해당 단어가 등장하는 횟수를 센다.\n","  2. Adjusted for doc length (TF)\n","    - raw count / num words\n","    - raw count를 전체 단어의 갯수로 나눠준다.\n","    - TF를 비율로 나타내고 모두 합했을 때 1이 되게 만들어준다.\n","  3. Other variants\n","    - binary, log normalization, etc.\n","    - 다른 여러가지 방법들이 존재한다.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mk_RcRXNrxnIbtMEu5DAJkfJREhnR11R' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"YGmhttPySXk1"},"source":["<br>\n","\n","### 4.3.3 Inverse Document Frequency (IDF)\n","\n","- 단어가 제공하는 정보의 양\n","\n","$$\n","I D F(t)=\\log \\frac{N}{D F(t)}\n","$$\n","\n","- <mark>$N$</mark>: 총 document의 개수\n","- <mark>Document Frequency (DF)</mark>: Term $t$가 등장한 document의 개수\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mpiBpkDi6_8GeoHEywcoF33VqOrgpkbU' width=500/>\n","\n","- 자주 등장하는 단어의 경우 DF의 값이 클 것이다.\n","- 모든 문서에 등장하는 단어의 경우 IDF = 0 이 된다.\n","  - 모든 문서에서 해당 단어가 등장했으므로 DF가 N이 된다.\n","- 자주 등장하지 않는 단어의 경우 DF의 값이 작으므로 IDF값이 커진다.\n","\n","<br>\n","\n","- IDF의 특징은 단어에 의해 값이 결정되고 문서에 대해서는 무관하다는 점이다.\n","  - TF는 t와 d에 대한 함수이지만, IDF는 t에 대한 함수이다."]},{"cell_type":"markdown","metadata":{"id":"yPqgvIstS4Vm"},"source":["<br>\n","\n","### 4.3.4 Combine TF & IDF\n","\n","- TF-IDF($t,d$)\n","  - TF-IDF for term $t$ in document $d$\n","  - TF-IDF는 TF와 IDF를 곱한 값이다.\n","\n","$$\n","T F(t, d) \\times I D F(t)\n","$$\n","\n","<br>\n","\n","- Low TF-IDF\n","  - `a`, `the` 등의 관사\n","  - TF는 높을 수 있지만, IDF가 0에 가까울 것\n","  - 거의 모든 document에 등장 $\\Rightarrow \\mathrm{N} \\approx \\mathrm{DF}(t) \\Rightarrow \\log (\\mathrm{N} / \\mathrm{DF}) \\approx 0$\n","- High TF-IDF\n","  - 자주 등장하지 않는 고유 명사\n","  - ex) 사람 이름, 지명 등\n","  - IDF가 커지면서 전체적인 TF-IDF 값이 증가"]},{"cell_type":"markdown","metadata":{"id":"01Swjzm4TJXR"},"source":["<br>\n","\n","### 4.3.5 TF-IDF 계산하기"]},{"cell_type":"markdown","metadata":{"id":"fWC2OMoJT4Kb"},"source":["#### 4.3.5.1 실험 데이터\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mqh7OmttXdlWLTVSd9z3JRHYK_uE8DT9' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"ZvqOgUpXT6cS"},"source":["<br>\n","\n","#### 4.3.5.2 토크나이저\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1mv6F7DaNr-HvrCoFBr4sJYmhDuE7tZgS' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"cZ1zAfzHUCKb"},"source":["<br>\n","\n","#### 4.3.5.3 TF 계산\n","\n","- Term Frequency (TF): 단어의 등장 빈도\n","- TF($t,d$): TF for term $t$ and document $d$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1n4Nr4xEu-yi61tX4YbRCL-dZLr-tjGVq' width=800/>\n","\n","- 계산 결과에 0 또는 1밖에 없는 이유\n","  - 예시에선 각 문서마다 특정 단어가 1번 또는 0번 나왔기 때문"]},{"cell_type":"markdown","metadata":{"id":"9lJB12buUfW1"},"source":["<br>\n","\n","#### 4.3.5.4 IDF 계산\n","\n","- Inverse Document Frequency(IDF): 단어가 제공하는 정보의 양\n","\n","$$\n","I D F(t)=\\log \\frac{N}{D F(t)}\n","$$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1n4mivvTEuIycKq7106LeOZtpMeQbbTMi' width=800/>\n","\n","- 자주 출현한 단어들은 IDF값이 낮은 것을 확인할 수 있다.\n","- IDF값은 <mark>문서에 상관없이</mark> 항상 일정한 값을 가짐을 확인할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"YwOeJYB1Ut7e"},"source":["<br>\n","\n","#### 4.3.5.5 TF-IDF 계산\n","\n","$$\n","T F(t, d) \\times I D F(t)\n","$$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1n5oH0FUf-Iqkv2ialJ4cYECo7cAVDB2c' width=800/>\n","\n","- 각 문서가 가지고 있는 고유한 단어가 높은 TF-IDF 값을 가진 것을 확인할 수 있다.\n","- 자주 출현한 단어들은 TF-IDF 값이 낮은 것을 확인할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"pJaCaZHeVKny"},"source":["<br>\n","\n","### 4.3.6 TF-IDF를 이용해 유사도 구해보기"]},{"cell_type":"markdown","metadata":{"id":"XeqG_tZuWTWU"},"source":["#### 4.3.6.1 목표\n","\n","- 계산한 문서 TF-IDF를 가지고 질의 TF-IDF를 계산한 후 가장 관련있는 문서를 찾기"]},{"cell_type":"markdown","metadata":{"id":"VwW6Z9lbVrzq"},"source":["<br>\n","\n","#### 4.3.6.2 질문\n","\n","- \"주연은 BTS의 누구를 가장 잘생겼다고 생각한다?\"\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1n8XC78rLWH-HGqfve_AMnlGyYxsfx0VF' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"A1SPKMR0VhTz"},"source":["<br>\n","\n","#### 4.3.6.3 유사도 계산 과정\n","\n","- 계산한 TF-IDF를 가지고 사용자가 물어본 질의에 대해 가장 관련있는 문서를 찾아보자.\n","- TF-IDF의 경우 코사인 유사도를 이용하여 유사도를 계산한다.\n","\n","<br>\n","\n","1. 사용자가 입력한 질의를 토큰화\n","2. 기존에 단어 사전에 없는 토큰들은 제외\n","3. 질의를 하나의 문서로 생각하고, 이에 대한 TF-IDF 계산\n","4. 질의 TF-IDF 값과 각 문서별 TF-IDF 값을 곱하여 유사도 점수 계산\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1nBBxrcrm65T-sOkQY0b82efTY77xBILK' width=800/>\n","5. 가장 높은 점수를 가지는 문서 선택"]},{"cell_type":"markdown","metadata":{"id":"3XXoKCOyWbcV"},"source":["<br>\n","\n","#### 4.3.6.4 유사도가 높은 문서 추출\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img src='https://drive.google.com/uc?id=1nDFgkvBSzM1Y8Gb4MxxqtNU5RmcmJ9br' width=800/>"]},{"cell_type":"markdown","metadata":{"id":"O18M8D6LWoIL"},"source":["<br>\n","\n","### 4.3.7 BM25란?\n","\n","- TF-IDF의 개념을 바탕으로, 문서의 길이까지 고려하여 점수를 매김\n","- TF 값에 한계를 지정해두어 일정한 범위를 유지하도록 함\n","- 평균적인 문서의 길이보다 더 작은 문서에서 단어가 매칭된 경우 그 문서에 대해 가중치를 부여\n","- 실제 검색엔진, 추천 시스템 등에서 아직까지도 많이 사용되는 알고리즘\n","\n","$$\n","\\operatorname{Score}(D, Q)=\\sum_{t e r m \\in Q} I D F(\\text { term }) \\cdot \\frac{\\text { TFIDF }(\\text { term, } D) \\cdot\\left(k_{1}+1\\right)}{\\operatorname{TFIDF}(\\text { term, } D)+k_{1} \\cdot\\left(1-b+b \\cdot \\frac{|D|}{\\text { avgdl }}\\right)}\n","$$\n","\n","- [참조자료: Pyserini BM25 MSMarco Document Retrieval 예시](https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-doc.md)"]},{"cell_type":"code","metadata":{"id":"Hpg_D7pTcus4"},"source":[""],"execution_count":null,"outputs":[]}]}